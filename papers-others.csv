Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Publication Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier,Include Title,Include abstract,Exclude,accept contents,rq answers,,
Real Time Bangladeshi Sign Language Detection using Faster R-CNN,O. B. Hoque; M. I. Jubair; M. S. Islam; A. -F. Akash; A. S. Paulson,"Department of Computer Science and Engineering, Ahsanullah University of Science and Technology, Dhaka, Bangladesh; Department of Computer Science and Engineering, Ahsanullah University of Science and Technology, Dhaka, Bangladesh; Department of Computer Science and Engineering, Ahsanullah University of Science and Technology, Dhaka, Bangladesh; Department of Computer Science and Engineering, Ahsanullah University of Science and Technology, Dhaka, Bangladesh; Department of Computer Science and Engineering, Ahsanullah University of Science and Technology, Dhaka, Bangladesh",2018 International Conference on Innovation in Engineering and Technology (ICIET),07-Mar-19,2018,,,1,6,"Bangladeshi Sign Language (BdSL) is a commonly used medium of communication for the hearing-impaired people in Bangladesh. Developing a real time system to detect these signs from images is a great challenge. In this paper, we present a technique to detect BdSL from images that performs in real time. Our method uses Convolutional Neural Network based object detection technique to detect the presence of signs in the image region and to recognize its class. For this purpose, we adopted Faster Region-based Convolutional Network approach and developed a dataset - BdSLImset - to train our system. Previous research works in detecting BdSL generally depend on external devices while most of the other vision-based techniques do not perform efficiently in real time. Our approach, however, is free from such limitations and the experimental results demonstrate that the proposed method successfully identifies and recognizes Bangladeshi signs in real time.",,978-1-5386-5229-9,10.1109/CIET.2018.8660780,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660780,Bangladeshi Sign Language;Convolutional Neural Network;Faster R-CNN,Assistive technology;Gesture recognition;Real-time systems;Training;Object detection;Convolutional neural networks;Lighting,computer vision;convolutional neural nets;handicapped aids;object detection;sign language recognition,Faster r-CNN;BdSL;hearing-impaired people;image region;convolutional neural network;object detection technique;faster region-based convolutional network;Bangladeshi sign language detection,,21,,16,IEEE,07-Mar-19,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""Images of different
BdSL signs from our BdSLImset dataset were trained by Faster
R-CNN based model to solve the problem of sign language
recognition."" pg.4.",,1
An Efficient Binarized Neural Network for Recognizing Two Hands Indian Sign Language Gestures in Real-time Environment,M. Jaiswal; V. Sharma; A. Sharma; S. Saini; R. Tomar,"Dept. of Electronics and Communication Engineering, The LNM Institute of Information Technology, Jaipur, India; Dept. of Electronics and Communication Engineering, The LNM Institute of Information Technology, Jaipur, India; Dept. of Electronics and Communication Engineering, The LNM Institute of Information Technology, Jaipur, India; Dept. of Electronics and Communication Engineering, The LNM Institute of Information Technology, Jaipur, India; Dept. of Electronics and Communication Engineering, The LNM Institute of Information Technology, Jaipur, India",2020 IEEE 17th India Council International Conference (INDICON),05-Feb-21,2020,,,1,6,"This paper proposes an efficient architecture based on Binarized Neural Network (BNN) for recognizing two-hand Indian Sign Language (ISL). Some of the existing works apply deep Convolutional Neural Networks (CNN) and machine learning (ML) models to solve the challenging task of ISL recognition in a real-time environment. Therefore, the proposed BNN architecture having binary weights and activations, with bitwise operations used to reduce computational complexity during training and also, the method based on skin color segmentation and Otsu thresholding is applied to extract hand region during sign recognition. Extensive experiments are carried out on the ISL database having English alphabets and words static gestures show that the proposed work achieves a higher recognition rate of 98.8% in real-time as compared to other works. Moreover, a comparative analysis of the proposed BNN is performed over CNN in terms of memory consumption, speed, and training accuracy.",2325-9418,978-1-7281-6916-3,10.1109/INDICON49873.2020.9342454,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9342454,Real time recognition;Deep Learning;Sign language;Binarized Neural Networks;model compression,Training;Assistive technology;Neural networks;Gesture recognition;Real-time systems;Skin;Task analysis,computational complexity;feature extraction;gesture recognition;image colour analysis;image segmentation;learning (artificial intelligence);neural nets,words static gestures;CNN;efficient binarized neural network;hands indian sign language gestures;real-time environment;two-hand Indian Sign Language;ISL recognition;BNN architecture;binary weights;bitwise operations;skin color segmentation;hand region;sign recognition;ISL database,,6,,22,IEEE,05-Feb-21,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""In this paper, a novel architecture of BNN is proposed for ISL recognition in a cluttered background. Using this architecture, and skin color hand segmentation technique, the experiments conducted on two hands ISL database under various illuminations shows that the realtime system correctly predicts the English alphabets and words"" .pg.5.",,2
Real-Time Recognition of Bangla Sign Language Characters: A Computer Vision Based Approach Using Convolutional Neural Network,M. Tanvir; M. S. Alam; D. K. Saha; S. A. Hasib; S. Islam,"Dept. of MTE, RUET, Rajshahi, Bangladesh; Dept. of MTE, RUET, Rajshahi, Bangladesh; Dept. of MTE, RUET, Rajshahi, Bangladesh; Dept. of MTE, RUET, Rajshahi, Bangladesh; Dept. of MTE, RUET, Rajshahi, Bangladesh",2021 3rd International Conference on Electrical & Electronic Engineering (ICEEE),01-Mar-22,2021,,,177,180,"Sign Language is the elementary communication media for Deaf & Mute (D&M) people. On the other hand, it seems too tenacious for the general people to understand this language. In order to tear out this communication barrier, a real-time automated translator is essential. Through this research, a computer vision-based approach has been developed for the recognition of Bangla Sign Language (BdSL) characters. In this work, a deep learning-based recognition model has been developed. Adaptive thresholding has been integrated with 2D Convolutional Neural Network (CNN) to construct this model. Proposed model has been trained to build this real-time automated translator through our own created dataset (dataset containing 3600 different images for 36 distinct characters). The proposed model has been trained and tested with 2880 (80%) training images and 720 (20%) testing images respectively. Thirty-six unique characters of Bangla Sign Language can be recognized through this model with significant accuracy. The model delivers validation accuracy of 99.72% and validation loss of 0.73%. A significant result has been achieved for the recognition and translation of Bangla Sign Language characters with this dataset over other existing Bangla Sign Language Recognition model.",,978-1-6654-8281-3,10.1109/ICEEE54059.2021.9718800,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9718800,Sign Language;Computer Vision;Deep Learning;CNN;Bangla Sign Language,Training;Adaptation models;Computer vision;Computational modeling;Gesture recognition;Assistive technologies;Real-time systems,computer vision;handicapped aids;learning (artificial intelligence);natural language processing;neural nets;sign language recognition,time Recognition;Bangla Sign Language characters;Convolutional Neural Network;elementary communication media;Deaf & Mute people;general people;communication barrier;real-time automated translator;computer vision-based approach;deep learning-based recognition model;36 distinct characters;2880 training images;thirty-six unique characters;existing Bangla Sign Language Recognition model,,1,,12,IEEE,01-Mar-22,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""CNN architecture has been applied to build the recognition model of this paper."" pg.2.",,3
Bangla Sign Digits: A Dataset For Real Time Hand Gesture Recognition,D. Tasmere; B. Ahmed; M. M. Hasan,"Department of Computer Science & Engineering, Rajshahi University of Engineering & Technology, Rajshahi, Bangladesh; Department of Computer Science & Engineering, Rajshahi University of Engineering & Technology, Rajshahi, Bangladesh; Department of Computer Science & Engineering, Rajshahi University of Engineering & Technology, Rajshahi, Bangladesh",2020 11th International Conference on Electrical and Computer Engineering (ICECE),05-Apr-21,2020,,,186,189,"With current widespread computer vision study, research with a particular culture and aesthetic contexts lead to enhanced performance with improved user experience. Despite a large number of deaf and dumb people, Bangla Sign Language has been less-focused on the research area. This paper proposes the building of identifying Bangla Sign Language using hand gestures to reduce the isolation of the hearing-impairment world. For the last little advancement in deep learning, Convolution Neural Network (CNN) has been used to classify Bangla Sign Language with higher accuracy. Our methodology overcomes all constraints from existing research of Bangla hand gesture recognition. In this paper, we presented a novel real-time method for Bangla sign digits (0-9) that focuses on three sections: image acquisition, preprocessing, and lastly, recognition of Bangla sign digits. For training, we have created a dataset of 1674 images from different people from various conditions. The proposed CNN model architecture classifies all Bangla sign digits with an overall accuracy of 97.63% on real-time video that ensures a potential gesture model that contributed significantly in the domain of Bangla sign number recognition compared to previous researches.",,978-1-6654-2254-3,10.1109/ICECE51571.2020.9393070,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9393070,A dataset for Bangla Sign digits;Bangla Sign Digits Recognition;Deep Convolution Neural Network;Real Time Recognition.,Training;Target recognition;Convolution;Assistive technology;Gesture recognition;Real-time systems;User experience,computer vision;gesture recognition;handicapped aids;learning (artificial intelligence);neural nets;object detection;sign language recognition,Bangla Sign digits;time hand gesture recognition;current widespread computer vision study;Bangla Sign Language;hand gestures;Bangla hand gesture recognition;Bangla sign digits;Bangla sign number recognition,,2,,16,IEEE,05-Apr-21,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""Our proposed system detects hand area from a real-time video extracts hand features after preprocessing, and trains a deep CNN model to recognize correct gestures."" pg.2.",,4
Dynamic Hand Gesture Recognition System for Kurdish Sign Language Using Two Lines of Features,M. R. Mahmood; A. Mohsin Abdulazeez; Z. Orman,"Department of Computer Science, University of Zakho, Zakho, Kurdistan Region, Iraq; Duhok Polytechnic University, Duhok, Kurdistan Region, Iraq; Department of Computer Engineering, University of Istanbul, Istanbul, Turkey",2018 International Conference on Advanced Science and Engineering (ICOASE),29-Nov-18,2018,,,42,47,"Hand gesture recognition forms a great difficulty for computer vision especially in dynamics. Sign language has been significant and an interesting application field of dynamic hand gesture recognition system. The recognition of human hands formed an- extremely complicated mission. The solution for such a difficulty requires a robust hand tracking method which depends on an effective feature and classifier. This paper presents a novel, fast and simple method for dynamic hand gesture recognition based on two lines (hundred) of features extracted from two rows of a Real-Time video. Feature selections have been used for hand shape representation to recognize the dynamic word for Kurdish Sign Language. The features extracted in real time from pre-processed hand object were represented through the optimization values of binary captured frame. Finally, an Artificial Neural Network classifier is used to recognize the performed hand gestures by 80% for training and 20% for testing with success 98%.",,978-1-5386-6696-8,10.1109/ICOASE.2018.8548840,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8548840,hand gesture;sign language;Kurdish sign language;feature extraction;artificial neural networks,Feature extraction;Gesture recognition;Assistive technology;Training;Testing;Real-time systems;Shape,computer vision;feature extraction;feature selection;neural nets;optimisation;pattern classification;sign language recognition,hand shape representation;dynamic word;pre-processed hand object;dynamic hand gesture recognition system;hand gesture recognition forms;human hands;robust hand tracking method;real-time video;feature selection;optimization values;computer vision;Kurdish sign language;artificial neural network classifier,,6,,20,IEEE,29-Nov-18,,,IEEE,IEEE Conferences,Yes ,Yes,No,No,"""The features of hand image (dynamic hand image) are stored in training dataset vector then are used to match them with features of the testing image by ANN."" pg.3.",,
Real-Time American Sign Language Recognition Using Skin Segmentation and Image Category Classification with Convolutional Neural Network and Deep Learning,S. Shahriar; A. Siddiquee; T. Islam; A. Ghosh; R. Chakraborty; A. I. Khan; C. Shahnaz; S. A. Fattah,"Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh",TENCON 2018 - 2018 IEEE Region 10 Conference,24-Feb-19,2018,,,1168,1171,A real-time sign language translator is an important milestone in facilitating communication between the deaf community and the general public. We hereby present the development and implementation of an American Sign Language (ASL) fingerspelling translator based on skin segmentation and machine learning algorithms. We present an automatic human skin segmentation algorithm based on color information. The YCbCr color space is employed because it is typically used in video coding and provides an effective use of chrominance information for modeling the human skin color. We model the skin-color distribution as a bivariate normal distribution in the CbCr plane. The performance of the algorithm is illustrated by simulations carried out on images depicting people of different ethnicity. Then Convolutional Neural Network (CNN) is used to extract features from the images and Deep Learning Method is used to train a classifier to recognize Sign Language.,2159-3450,978-1-5386-5457-6,10.1109/TENCON.2018.8650524,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8650524,sign language;skin segmentation;machine learning;YCbCr color space;gesture recognition,Image color analysis;Skin;Feature extraction;Gesture recognition;Assistive technology;Image segmentation;Colored noise,convolutional neural nets;feature extraction;gesture recognition;image classification;image colour analysis;image segmentation;learning (artificial intelligence);normal distribution;real-time systems;sign language recognition;skin,real-time American Sign Language recognition;Deep Learning Method;bivariate normal distribution;skin-color distribution;human skin color;chrominance information;YCbCr color space;color information;automatic human skin segmentation algorithm;machine learning algorithms;American Sign Language fingerspelling translator;general public;deaf community;real-time sign language translator;Convolutional Neural Network;image category classification,,16,,18,IEEE,24-Feb-19,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""We captured numerous images and extracted features from it using CNN Transfer Learning. These features were then again passed onto a Deep Learning Classifier to ensure proper classification of hand-signs."" pg.1.",,5
A CNN sign language recognition system with single & double-handed gestures,N. Buckley; L. Sherrett; E. Lindo Secco,"AI Laboratory, School of Mathematics, Computer Science and Engineering, Liverpool Hope University, Liverpool, UK; AI Laboratory, School of Mathematics, Computer Science and Engineering, Liverpool Hope University, Liverpool, UK; Robotics Laboratory, School of Mathematics, Computer Science and Engineering, Liverpool Hope University, Liverpool, UK","2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",09-Sep-21,2021,,,1250,1253,"This work aims at presenting a novel Computer Vision approach in the development of a real-time, web-camera based, British Sign Language recognition system. A literature review focused on current (1) state of sign language recognition systems and (2) techniques used is conducted. This review process is used as a foundation on which a Convolutional Neural Network (CNN) based system is designed and then implemented. A bespoke British Sign Language dataset -containing 11,875 images - is then performed to train and test the CNN which is used for the classification of human hand performed gestures. Finally, the CNN architecture recognized 19 static British Sign Language gestures, incorporating both single and double-handed gestures. During testing, the system achieved an average recognition accuracy of 89%.",0730-3157,978-1-6654-2463-9,10.1109/COMPSAC51774.2021.00173,Liverpool Hope University; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529449,sign language recognition;AI;CNN;human-machine interaction,Computer vision;Conferences;Bibliographies;Gesture recognition;Computer architecture;Assistive technologies;Software,computer vision;convolutional neural nets;feature extraction;gesture recognition;handicapped aids;image sensors;natural language processing;sign language recognition,CNN sign language recognition system;double-handed gestures;computer vision approach;web-camera;literature review;review process;convolutional neural network based system;human hand;CNN architecture;static British sign language gestures;bespoke British sign language dataset;British sign language recognition system,,5,,15,IEEE,09-Sep-21,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""The system will make use of a CNN for the feature extraction and classification process.""pg.2.",,6
Real time Hand Gesture Recognition using different algorithms based on American Sign Language,M. M. Islam; S. Siddiqua; J. Afnan,"Shahjalal University of Science and Technology, Sylhet, BD; Department of Electrical and Electronic Engineering, Shahjalal University of Science and Technology, Sylhet, Bangladesh; Department of Electrical and Electronic Engineering, Shahjalal University of Science and Technology, Sylhet, Bangladesh","2017 IEEE International Conference on Imaging, Vision & Pattern Recognition (icIVPR)",03-Apr-17,2017,,,1,6,"Human Computer Interaction (HCI) is a broad research field based on human interaction with computers or machines. Basically, Hand Gesture Recognition (HGR) is a subfield of HCI. Today, many researchers are working on different HGR applications like game controlling, robot control, smart home system, medical services etc. The purpose of this paper is to represent a real time HGR system based on American Sign Language (ASL) recognition with greater accuracy. This system acquires gesture images of ASL with black background from mobile video camera for feature extraction. In the processing phase, the system extracts five features such as fingertip finder, eccentricity, elongatedness, pixel segmentation and rotation. For feature extraction, a new algorithm is proposed which basically combines K curvature and convex hull algorithms. It can be called âK convex hullâ method which can detect fingertip with high accuracy. In our system, Artificial Neural Network (ANN) is used with feed forward, back propagation algorithm for training a network using 30 feature vectors to recognize 37 signs of American alphabets and numbers properly which is helpful for HCI system. The total gesture recognition rate of this system is 94.32% in real time environment.",,978-1-5090-6004-7,10.1109/ICIVPR.2017.7890854,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890854,American sign language;hand gesture recognition;fingertip finder algorithm;k curvature;convex hull;pixel segmentation;eccentricity;elongatedness;artificial neural network,Feature extraction;Gesture recognition;Assistive technology;Image segmentation;Shape;Real-time systems;Cameras,backpropagation;feature extraction;feedforward neural nets;human computer interaction;natural language processing;sign language recognition,hand gesture recognition;HGR;American sign language recognition;ASL recognition;human computer interaction;HCI;feature extraction;artificial neural network;ANN;feedforward algorithm;backpropagation algorithm,,44,,7,IEEE,03-Apr-17,,,IEEE,IEEE Conferences,Yes ,No,,,,,
Real-time Sign Language Recognition using Computer Vision,J. J. Raval; R. Gajjar,"Electronics and Communication Department, Institute of Technology, Nirma University, Ahmedabad, India; Electronics and Communication Department, Institute of Technology, Nirma University, Ahmedabad, India",2021 3rd International Conference on Signal Processing and Communication (ICPSC),15-Jun-21,2021,,,542,546,Speech impairment is a disability that affects an individual's ability to verbal communication. To overcome this issue sign language is used which is one of the most organised languages. There is definitely a need for a method or an application that can recognize sign language gestures so that communication is possible even if someone does not understand sign language. My paper is an effort towards filling the gap between differently-abled people like deaf and dumb and the other people. Image processing combined with machine learning helped in forming a real-time system. Image processing is used for pre-processing the images and extracting different hand from the background. These images obtained after extracting background were used for forming data that contained 24 alphabets of the English language. The Convolutional Neural Network proposed here is tested on both a custom-made dataset and also with real-time hand gestures performed by people of different skin tones. The accuracy obtained by the proposed algorithm is 83%.,,978-1-6654-2864-4,10.1109/ICSPC51351.2021.9451709,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9451709,Machine learning;Image processing;Convolutional Neural Network,Assistive technology;Signal processing algorithms;Gesture recognition;Signal processing;Feature extraction;Cameras;Real-time systems,computer vision;feature extraction;gesture recognition;handicapped aids;learning (artificial intelligence);neural nets;sign language recognition,verbal communication;issue sign language;organised languages;differently-abled people;image processing;real-time system;extracting different hand;English language;real-time hand gestures;time sign language recognition;computer vision;speech impairment;individual,,1,,14,IEEE,15-Jun-21,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""A Convolutional Neural Network model is designed for extracting features from the corresponding signs."" pg.5.",,7
Real-time computer vision-based Bengali Sign Language recognition,M. A. Rahaman; M. Jasim; M. H. Ali; M. Hasanuzzaman,"Department of Computer Science & Engineering, University of Dhaka, Dhaka, Bangladesh; Department of Computer Science & Engineering, University of Dhaka, Dhaka, Bangladesh; Department of Computer Science & Engineering, University of Dhaka, Dhaka, Bangladesh; Department of Computer Science & Engineering, University of Dhaka, Dhaka, Bangladesh",2014 17th International Conference on Computer and Information Technology (ICCIT),02-Apr-15,2014,,,192,197,"This paper presents a real-time computer vision-based Bengali Sign Language (BdSL) recognition system. The system detects the probable hand from the captured image. The system uses Haar-like feature-based cascaded classifiers to detect the hand in each frame. From the detected hand area, the system extracts the hand sign based on Hue and Saturation value corresponding to human skin color. After normalization the system converts the hand sign to binary image. Then the binary images are classified by comparing with pre-trained binary images of hand sign using K-Nearest Neighbors (KNN) Classifier. The system is able to recognize 6 Bengali Vowels and 30 Bengali Consonants. The system is trained using 3600 (36Ã10Ã10) training images where each of 10 signers performed 10 signs for each corresponding Bengali alphabet and the system is tested using another 3600 (36Ã10Ã10) images of 10 signers. The system is achieved recognition accuracy of 98.17% for Vowels and 94.75% for Consonants.",,978-1-4799-6288-4,10.1109/ICCITechn.2014.7073150,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7073150,Skin color segmentation;Hand detection;K-Nearest Neighbors (KNN) algorithm;Bengali Sign Language Recognition,Assistive technology;Feature extraction;Gesture recognition;Accuracy;Training;Image color analysis;Skin,computer vision;Haar transforms;image capture;image classification;image colour analysis;natural language processing;sign language recognition,real-time computer vision-based Bengali sign language recognition system;probable hand detection;image capture;Haar-like feature-based cascaded classifiers;hue and saturation value;human skin color;binary image classification;pretrained binary images;k-nearest neighbor classifier;KNN classifier;Bengali vowels;Bengali consonants;Bengali alphabet;recognition accuracy;BdSL recognition system,,47,,11,IEEE,02-Apr-15,,,IEEE,IEEE Conferences,No,No,No,No,,,
A Review on Real-Time Sign Language Recognition,D. Sau; S. Dhol; M. K; K. Jayavel,"Department of CSE, SRM Institute of Science and Technology, Chennai, India; Department of CSE, SRM Institute of Science and Technology, Chennai, India; Networking and Communications, SRM Institute of Science and Technology, Kattankuathur; Networking and Communications, SRM Institute of Science and Technology, Kattankuathur",2022 International Conference on Computer Communication and Informatics (ICCCI),31-Mar-22,2022,,,1,5,"Sign Language Recognition (SLR) or Sign Language translation is one of the famous areas of research. Sign Language Recognition (SLR) is aimed at creating two-way communication between people who use sign language as their basic communication with those who don't. Although these had been popular areas for computer vision, the application had been less due to resources that were required. Creating an efficient model becomes complicated and diverse. For this literature survey, the authors analysed and summarised several methodologies and models used to construct sign-language translators. On conducting a survey, those papers selected did conclude with excellent results respectively but did lack in certain areas. American Sign Language (ASL) has been taken as the parameter for differentiation because it consists of its own set of strengths and weaknesses.",2329-7190,978-1-6654-8035-2,10.1109/ICCCI54379.2022.9740868,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9740868,Sign Language Recognition (SLR);Convolutional Neural Network(CNN);Transfer Learning;ASL (American Sign Language);Human Computer Interaction (HCI),Human computer interaction;Computer vision;Analytical models;Computational modeling;Gesture recognition;Assistive technologies;Real-time systems,computer vision;gesture recognition;handicapped aids;language translation;sign language recognition,real-time sign language recognition;SLR;Sign Language translation;famous areas;sign-language translators;American Sign Language,,,,16,IEEE,31-Mar-22,,,IEEE,IEEE Conferences,Yes ,Yes,Yes,,,,
Real-Time Indian Sign Language Recognition System using YOLOv3 Model,N. Sarma; A. K. Talukdar; K. K. Sarma,"Department of Electronics and Communication Engineering, Gauhati University, Guwahati, Assam, India; Department of Electronics and Communication Engineering, Gauhati University, Guwahati, Assam, India; Department of Electronics and Communication Engineering, Gauhati University, Guwahati, Assam, India",2021 Sixth International Conference on Image Information Processing (ICIIP),10-Feb-22,2021,6,,445,449,"Sign Language is a language which helps deaf and mute people for communication with hearing people. The aim of Indian Sign Language recognition (ISLR) is to understand the meaning of signs of speech impaired or hearing impaired person in the Indian region to interact with the society. This paper proposes for ISLR system in real-time based on the YOLOv3 Model and used in conjunction with Darknet-53 convolutional neural network. The system has been tested in real-time with 16 different signs for images and 7 signs for videos. The proposed model was labeled the sign language datasets in YOLO format. The sign language images are captured by the webcam for static sign language recognition and videos are recorded for dynamic sign language recognition. We achieved the accuracies for static and dynamic signs as 95.7% and 93.1%, respectively. The experimental results show that the proposed system can recognize both static and dynamic ISL signs effectively in real time with reduced computational time.",2640-074X,978-1-6654-3361-7,10.1109/ICIIP53038.2021.9702611,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9702611,Real-Time;Indian Sign language;Deep Learning;YOLOv3,Image recognition;Webcams;Gesture recognition;Speech recognition;Auditory system;Information processing;Assistive technologies,convolutional neural nets;handicapped aids;sign language recognition,sign language datasets;sign language images;static sign language recognition;dynamic sign language recognition;static ISL signs;dynamic ISL signs;computational time;YOLOv3 Model;deaf people;mute people;hearing people;Indian region;ISLR system;Darknet-53 convolutional neural network;real-time Indian sign language recognition system;hearing impaired person;speech impaired person,,1,,19,IEEE,10-Feb-22,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""In this paper, we developed a robust real-time Indian sign language recognition system based on YOLOv3 model.""pg.4.",,8
Computer vision based framework for digit recognition by hand gesture analysis,O. De; P. Deb; S. Mukherjee; S. Nandy; T. Chakraborty; S. Saha,"Department of Computer Science Engineering, Institute of Engineering & Management, Kolkata, India; Department of Information Technology, Institute of Engineering & Management, Kolkata, India; Department of Information Technology, Institute of Engineering & Management, Kolkata, India; Department of Information Technology, Institute of Engineering & Management, Kolkata, India; Department of Computer Science Engineering, Institute of Engineering & Management, Kolkata, India; Department of Computer Science Engineering, Institute of Engineering & Management, Kolkata, India","2016 IEEE 7th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)",17-Nov-16,2016,,,1,5,"In recent years, hand gesture recognition framework as an effective sign language tool has been extensively explored by many researchers. This paper presents an idea of developing a framework using computer vision for hand gesture based sign language recognition from real-time video stream. The proposed system identifies hand-palm in video stream based on skin color and background subtraction scheme as opposed to the conventional techniques of using gloves or markers as interface and thereby makes an effort of exploring the possibility of a suitable computer vision framework for hand gesture recognition. We have also proposed an iterative polygonal shape approximation strategy in fusion with a special chain-coding scheme for shape-similarity matching. This proposed framework considers digits as important symbols of sign language and it successfully recognizes hand gestures corresponding to various numerical digits with acceptable accuracy.",,978-1-5090-0996-1,10.1109/IEMCON.2016.7746361,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7746361,Sign language;Computer Vision;Gesture Recognition,Gesture recognition;Skin;Shape;Image color analysis;Assistive technology;Streaming media;Computer vision,approximation theory;computer vision;image coding;image colour analysis;image matching;iterative methods;sign language recognition;video signal processing,computer vision;digit recognition;hand gesture analysis;hand gesture recognition;hand gesture based sign language recognition;real-time video stream;hand-palm;skin color;background subtraction scheme;iterative polygonal shape approximation strategy;special chain-coding scheme;shape-similarity matching,,3,,16,IEEE,17-Nov-16,,,IEEE,IEEE Conferences,Yes ,Yes,No,No,,,
Real time sign language recognition using the leap motion controller,D. Naglot; M. Kulkarni,"Department of CSE (IT), Vishwakarma Institute of Technology, Pune, India; Department of CSE (IT), Vishwakarma Institute of Technology, Pune, India",2016 International Conference on Inventive Computation Technologies (ICICT),26-Jan-17,2016,3,,1,5,"Hearing and speech impaired people use Sign Language to convey their message to normal people. Sign Language has evolved as one of the major areas of research and study in computer vision. Researchers in sign language recognition used different input devices such as data gloves, web camera, depth camera, color camera, Microsoft's Kinect sensor, etc. to capture hand signs. In this paper we display the importance of American Sign Language and proposed technique for classification and their efficient results. American Sign Language uses only one hand to display the gestures and thus makes it easy for interpretation and understanding. The signs are captured using new digital sensor called âLeap Motion Controllerâ. LMC is 3D non-contact motion sensor which can tracks and detects hands, fingers, bones and finger-like objects. Proposed system used Multi-Layer Perceptron (MLP) neural network with Back Propagation (BP) algorithm to build a classification model which takes feature set as input. Multi-Layer Perceptron (MLP) neural network used to recognize different signs. We have considered 26 different alphabets of American Sign Language. Multi-Layer Perceptron (MLP) is executed on a dataset of total 520 samples (consisting of 20 samples of each alphabet). Recognition rate of proposed system is 96.15%.",,978-1-5090-1285-5,10.1109/INVENTIVE.2016.7830097,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7830097,Leap Motion Controller (LMC);American Sign Language (ASL);Sign Language;Multi-Layer Perceptron (MLP),Assistive technology;Gesture recognition;Thumb;Neurons;Feature extraction;Artificial neural networks,feature extraction;handicapped aids;image classification;image motion analysis;image sensors;multilayer perceptrons;natural language processing;object detection;object tracking;real-time systems;sign language recognition,real time sign language recognition;leap motion controller;hearing impaired people;speech impaired people;computer vision;data gloves;Web camera;depth camera;color camera;Microsoft Kinect sensor;hand signs capture;American sign language;classification;gestures display;digital sensor;LMC;3D noncontact motion sensor;hand tracking;hand detection;multilayer perceptron neural network;MLP neural network;back propagation;BP algorithm;feature set,,28,,16,IEEE,26-Jan-17,,,IEEE,IEEE Conferences,Yes ,Yes,No,No,This paper presents method for American Sign Language recognition using newly introduced Leap Motion Controller.,,
Sign Language Recognition Based on Computer Vision,T. Li; Y. Yan; W. Du,"Zhengzhou University, Zhengzhou, China; Zhengzhou University, Zhengzhou, China; Zhengzhou University, Zhengzhou, China",2022 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),05-Aug-22,2022,,,927,931,"With the application of deep learning in the field of computer vision, target detection, recognition and target tracking technologies have been developed. Sign language recognition is currently a hot topic in the field of machine learning, and this project uses computer vision domain technologies to implement sign language recognition to solve the problems of deaf people in daily communication. In this project, two algorithm models, CNN+LSTM network structure and YOLOv5 target detection, are studied, and the functions of sign language recognition are implemented by the two algorithms respectively, and their implementation effects are compared, and the advantages and disadvantages of the two algorithms in sign language recognition are derived. Among them, YOLOv5 completes the task of sign language recognition by detecting hand movements, and its fast detection speed is more in line with the needs of life scenes and satisfies the demand for real-time sign language translation, which is more promising for application.",,978-1-6654-9991-0,10.1109/ICAICA54878.2022.9844497,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9844497,Computer vision;sign language recognition;neural networks;YOLOv5,Computer vision;Target tracking;Machine learning algorithms;Target recognition;Computational modeling;Gesture recognition;Object detection,computer vision;feature extraction;gesture recognition;handicapped aids;image motion analysis;learning (artificial intelligence);object detection;recurrent neural nets;sign language recognition;target tracking,sign language recognition;target tracking technologies;computer vision domain technologies;real-time sign language translation,,1,,8,IEEE,05-Aug-22,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""This project is based on existing technologies in the field of computer vision for sign language recognition, one is a hybrid network model of CNN+LSTM, and the other is a target detection model of YOLOv5.""pg.5.",,9
American Sign Language Static Gesture Recognition using Deep Learning and Computer Vision,S. N. Reddy Karna; J. S. Kode; S. Nadipalli; S. Yadav,"Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru Amrita Vishwa Vidyapeetham, India; Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru Amrita Vishwa Vidyapeetham, India; Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru Amrita Vishwa Vidyapeetham, India; Department of Electrical and Electronics Engineering, Amrita School of Engineering, Bengaluru Amrita Vishwa Vidyapeetham, India",2021 2nd International Conference on Smart Electronics and Communication (ICOSEC),12-Nov-21,2021,,,1432,1437,"Specially-abled people (speech and hearing impaired) rely on hand-gestures for communication on a daily basis. Majority of the people arenât aware of the universally accepted hand-gestures alphabet, making communication difficult between the two groups of people. In an attempt to fill this void, this research work proposes a real time hand-gesture based recognition system based on the American Sign Language (ASL) dataset and capturing data through a BGR webcam and processing it using Computer Vision (OpenCV). The 29 static gestures (the alphabet) from the official, standard ASL dataset were trained with the help of Vision Transformer Model (ViT). The model showed an accuracy rate of 99.99% after being trained with 87,000 RGB samples for 1 epoch (2719 batches of 32 images each).",,978-1-6654-3368-6,10.1109/ICOSEC51865.2021.9591845,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591845,Gesture classification;American Sign Language;Vision Transformer;Convolutional Neural Networks;OpenCV,Deep learning;Computer vision;Webcams;Computational modeling;Gesture recognition;Auditory system;Assistive technologies,computer vision;gesture recognition;handicapped aids;robot vision,standard ASL dataset;Vision Transformer Model;American Sign Language static gesture recognition;deep learning;Computer Vision;specially-abled people;hearing;universally accepted hand-gestures alphabet;time hand-gesture based recognition system;capturing data;29 static gestures,,,,22,IEEE,12-Nov-21,,,IEEE,IEEE Conferences,Yes ,Yes,No,No,The paper introduces the idea of seamlessly incorporating state-of-the-art Transformer models into the field of CV by mimicking performances similar to CNNs architectures in the image classification domain while using CNN as little as possible in their architecture. We have used this pretrained model for ASL dataset in an attempt to create a robust model and decrease computational resources for training.(pg-3),,
Implementation of Real Time Static Hand Gestures Recognition for Sign Language,S. Gobhinath; S. Sophia,"Department of ECE, Sri Krishna College of Engineering and Technology, Coimbatore, TamilNadu, India; Department of ECE, Sri Krishna College of Engineering and Technology, Coimbatore, TamilNadu, India",2021 7th International Conference on Advanced Computing and Communication Systems (ICACCS),03-Jun-21,2021,1,,837,840,"The paper propose that uses the computer vision of hand recognition. The camera records live video streams, where the image is taken with the help of the interface. The system is train for every type of hand gesture (one, two, three, four, and five) at least once. After that a test action is given and the system tries to detect it. A proposed system in which hand gestures are detected using image processing. The system detects the number of fingers. The system finds separate fingers above the palm. The system first detects skin color in an image using a filter. The image goes through various steps of image editing to give the right amount of fingers. The system detects the nearest point from the decision point [1]. The system deletes the image according to the centroid point. After that many steps are applied to adjust the image to the effective image so that the fingers are exposed. The system finally detects the number of fingers and displays the calculation to the user. Research has been done on many algorithms that can better distinguish hand gestures. It was found that the diagonal sum algorithm provided the highest accuracy measure. In the progression phase, an automated algorithm removes the background for each training action. The image is then converted into a binary image and taken statistics for all the separated elements of the image. This amount helps us to distinguish and distinguish different hand gestures.",2575-7288,978-1-6654-0521-8,10.1109/ICACCS51430.2021.9441941,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9441941,Hand Gesture;Computer Vision;Color Image Segmentation and Feature Extraction,Training;Image segmentation;Neural networks;Training data;Gesture recognition;Streaming media;Skin,computer vision;handicapped aids;image colour analysis;image filtering;sign language recognition;statistical analysis;video cameras;video streaming,automated algorithm;binary image;diagonal sum algorithm;skin color;live video streams;sign language;computer vision;real time static hand gesture recognition;image editing;image processing;hand recognition,,1,,8,IEEE,03-Jun-21,,,IEEE,IEEE Conferences,Yes ,Yes,No,No,"This paper is hand acquisition and recognition of four different modes: Row vector algorithm, Edging and row vector passing algorithm, Mean and standard deviation of the two-dimensional image and Diagonal sum algorithm.(pg-4)",,
Real-Time Myanmar Sign Language Recognition Using Deep Learning,S. M. Htet; S. T. Aung; B. Aye,"Department of Computer Science, Higher Education Centre, Pyin Oo Lwin, Myanmar; Department of Computer Science, Higher Education Centre, Pyin Oo Lwin, Myanmar; Department of Computer Science, Higher Education Centre, Pyin Oo Lwin, Myanmar","2022 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)",09-Jun-22,2022,,,847,853,"Language is the main communication tool between human beings. People who have suffered from deaf and dumb use sign language for communication. To communicate with deaf and dumb people, we need to know sign language well. Researchers are seeking to explicit the meaning of sign language and to develop communication among deaf people and normal people. The significance of sign language recognition is hand detection, sign classification, and language translation. The main focus of this work is to develop a vision-based sign language recognition system, to transfer a tiny YOLO object detection model, a CNN-based classification model, and to develop a graph-based recognition model. Experimental results show that our proposed system can recognize Myanmar sign language not only with an accuracy of over 98% but also in real-time. In this paper, we have used 29 different hand signs for the proposed model in MATLAB 2020a.",,978-1-6654-8369-8,10.1109/ICIEAM54945.2022.9787266,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9787266,image classification;convolutional neural network;YOLO object detection algorithm;recognition,Training;Image color analysis;Gesture recognition;Object detection;Assistive technologies;Mathematical models;Real-time systems,computer vision;deep learning (artificial intelligence);handicapped aids;image classification;object detection;sign language recognition,hand signs;main communication tool;deaf people;dumb people;sign classification;language translation;vision-based sign language recognition system;graph-based recognition model;YOLO object detection model;CNN-based classification model;real-time Myanmar sign language recognition,,1,,16,USGov,09-Jun-22,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""In our proposed system, the Myanmar sign language recognition system is applied in three steps. Firstly, make detection of hand by using our proposed hand detection model which is transferred from the YOLOv2 object detection model. And we classify the detected hand sign by using the proposed Convolutional Neural Network (CNN) model and finally, we recognize the meaning of sign language by using our proposed graph model.""pg.1.",,10
Vision based Hand Gesture Recognition using Dynamic Time Warping for Indian Sign Language,W. Ahmed; K. Chanda; S. Mitra,"Advanced Signal Processing Group, Centre for Development of Advanced Computing, Kolkata, India; Advanced Signal Processing Group, Centre for Development of Advanced Computing, Kolkata, India; Advanced Signal Processing Group, Centre for Development of Advanced Computing, Kolkata, India",2016 International Conference on Information Science (ICIS),09-Feb-17,2016,,,120,125,"This paper presents an algorithm of Hand Gesture Recognition by using Dynamic Time Warping methodology. The system consists of three modules: real time detection of face region and two hand regions, tracking the hands trajectory both in terms of direction among consecutive frames as well as distance from the centre of the frame and gesture recognition based on analyzing variations in the hand locations along with the centre of the face. The proposed technique of ours overcomes not only the limitations of a glove based approach but also most of the vision based approach concerning different illumination conditions, background complexity and distance from camera which is up to 2 meters. Also by using Dynamic Time Warping Algorithm which finds the optimal alignment between the stored database features and query features, improvement in recognition accuracy is observed compared to conventional methods. Experimental results show that the accuracy is 90% in recognizing 24 gestures based on Indian Sign Language.",,978-1-5090-1987-8,10.1109/INFOSCI.2016.7845312,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845312,Human Computer Interaction;Hand gesture recognition;skin color segmentation;hand tracking;Dynamic Time Warping,Face;Gesture recognition;Skin;Image color analysis;Assistive technology;Heuristic algorithms;Tracking,computer vision;sign language recognition,vision based hand gesture recognition;Indian sign language;dynamic time warping methodology;real time detection;face region;hand locations;illumination conditions;background complexity;dynamic time warping algorithm;stored database features;query features,,31,,10,IEEE,09-Feb-17,,,IEEE,IEEE Conferences,Yes ,Yes,No,No,"In this paper, a technique along with application of Dynamic Time Warping methodology is used to perform similarity measurement efficiently.",,
Time Series Neural Networks for Real Time Sign Language Translation,S. S Kumar; T. Wangyal; V. Saboo; R. Srinath,"Computer Science and Engineering Department, PES Institute of Technology, Bangalore, India; Computer Science and Engineering Department, PES Institute of Technology, Bangalore, India; Computer Science and Engineering Department, PES Institute of Technology, Bangalore, India; Computer Science and Engineering Department, PES Institute of Technology, Bangalore, India",2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA),17-Jan-19,2018,,,243,248,"Sign language is the primary mode of communication for the hearing and speech impaired and there is a need for systems to translate sign languages to spoken languages. Prior research has been focused on providing glove based solutions which are intrusive and expensive. We propose a sign language translation system based solely on visual cues and deep learning for accurate translation. Our system applies Computer Vision and Neural Machine Translation for American Sign Language (ASL) gloss recognition and translation respectively. In this paper, we show that an end to end neural network system is not only capable of recognition of individual ASL glosses but also translation of continuous sign language videos into complete English sentences, making it an effective and practical tool for sign language communication.",,978-1-5386-6805-4,10.1109/ICMLA.2018.00043,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8614068,"Sign Language Translation, Computer Vision, Long Short Term Memory Networks, Neural Machine Translation, Attention Neural Networks, Deep Neural Networks, American Sign Language",Assistive technology;Gesture recognition;Videos;Recurrent neural networks;Time series analysis;Shape,computer vision;handicapped aids;language translation;learning (artificial intelligence);natural language processing;neural nets;real-time systems;sign language recognition;time series;video signal processing,time series neural networks;Neural Machine Translation;American Sign Language gloss recognition;hearing impaired;speech impaired;end to end neural network system;real time sign language translation system;ASL gloss recognition;computer vision;sign language communication;continuous sign language videos,,7,,23,IEEE,17-Jan-19,,,IEEE,IEEE Conferences,Yes ,Yes,No,No,"Our system is an ensemble of multiple neural networks, where each network is trained to optimize over separate stages of translation",,
Sign and Machine Language Recognition for Physically Impaired Individuals,S. Jothimani; S. Shruthi; E. D. Tharzanya; S. Hemalatha,"Department of Electronics and Communication Engineering, M. Kumarasamy College of Engineering, Karur, Tamil Nadu; Department of Electronics and Communication Engineering, M. Kumarasamy College of Engineering, Karur, Tamil Nadu; Department of Electronics and Communication Engineering, M. Kumarasamy College of Engineering, Karur, Tamil Nadu; Department of Electronics and Communication Engineering, M. Kumarasamy College of Engineering, Karur, Tamil Nadu",2022 3rd International Conference on Electronics and Sustainable Communication Systems (ICESC),19-Sep-22,2022,,,1483,1488,"Sign language is a type of communication used by people who are deaf or hard of hearing. Disabled People use sign language gestures as a non-verbal communication tool to express their feelings and thoughts to other people. It's tough to converse with those who are deaf or hard of hearing. Deaf and mute persons communicate via hand gesture sign language, which makes it difficult for non-deaf and mute people to understand their language. As a result, technologies that recognise numerous signs and communicate the data to regular people are needed. However, because these ordinary people have a hard time understanding their expressions, experienced sign language experts are required for medical and legal appointments, as well as educational and training sessions. There has been an upsurge in demand for these services in recent years. Other types of services, such as video remote human interpreter using a high-speed Internet connection, have been established, providing an easy-to-use sign language interpreter service that may be utilised and benefited, but with significant limits. In order to address this issue, the artificial intelligence technology can be implemented to analyze userâs hand with finger detection. A novel system has been suggested to design the vision-based system in real time environments. Then the Convolutional Neural Network (CNN) algorithm is used to classify the sign language and provide the label about recognized sign with voice alert.",,978-1-6654-7971-4,10.1109/ICESC54411.2022.9885433,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9885433,Sign Language;Deep learning;Impairment people;Convolutional neural network;Region of Interest,Training;Image recognition;Law;Gesture recognition;Speech recognition;Auditory system;Assistive technologies,gesture recognition;handicapped aids;human computer interaction;Internet;natural language processing;neural nets;sign language recognition,hearing;Disabled People;sign language gestures;nonverbal communication tool;deaf;hand gesture sign language;mute people;recognise numerous signs;regular people;ordinary people;experienced sign language experts;recognized sign,,,,24,IEEE,19-Sep-22,,,IEEE,IEEE Conferences,Yes ,Yes,No,No,,,
Low Resolution Hand Gestures Recognition of Bengali Sign Alphabet by Using a Convolutional Neural Network,A. Y. Srizon; M. A. Hossainy; M. R. Haquez,"Department of Computer Science & Engineering, Rajshahi University of Engineering & Technology, Rajshahi, Bangladesh; Department of Computer Science & Engineering, Rajshahi University of Engineering & Technology, Rajshahi, Bangladesh; Department of Computer Science & Engineering, Rajshahi University of Engineering & Technology, Rajshahi, Bangladesh",2021 24th International Conference on Computer and Information Technology (ICCIT),28-Jan-22,2021,,,1,6,"Sign language is an essential tool for the deaf and the hard of hearing community of approximately 1.33 billion people. Due to this fact, researches have been conducted for decades for near-accurate recognition of sign characters. Sensor-based approaches and vision-based approaches have been adapted so far for tackling this dilemma. Sensor-based approaches can obtain high performance but it is costly and demands physical contact to sensors. On the other hand, vision-based approaches are not costly, need no contact but have not yet been able to produce a high accuracy like sensor-based approaches. The dilemma of sign characters recognition gets more problematic for Bengali sign language as not many datasets regarding Bengali sign language are available. Moreover, not many significant contributions can be found in this domain like other popular languages such as English, Turkish, Japanese, and Indian sign language. Furthermore, one of the most popular Bengali sign language datasets, Ishara-Lipi, consists of a few low-resolution samples. This study is focused on recognizing the low-resolution hand gestures of Bengali sign language. In this research, a convolutional neural network has been proposed which is suitable for the recognition of low-resolution sign gestures. Experimental results showed that the proposed approach achieved 99.08%, 99.38%, and 99.07% overall accuracy for digits, characters, and both digits and characters of the Ishara-Lipi dataset respectively.",,978-1-6654-9435-9,10.1109/ICCIT54785.2021.9689895,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9689895,Convolutional Neural Networks;Augmentation;Ishara-Lipi Dataset;Bengali Sign Language Recognition;Adam,Performance evaluation;Computational modeling;Gesture recognition;Assistive technologies;Real-time systems;Mobile handsets;Sensors,feature extraction;gesture recognition;handicapped aids;natural language processing;neural nets;sign language recognition,convolutional neural network;sensor-based approaches;vision-based approaches;sign characters recognition;popular languages;Indian sign language;popular Bengali sign language datasets;low-resolution samples;low-resolution hand gestures;low-resolution sign gestures;low resolution hand gestures recognition;Bengali sign alphabet,,,,27,IEEE,28-Jan-22,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""Therefore, in this study, we are proposing a deep convolutional neural network that is capable of recognizing low-resolution hand gestures images.""pg.2.",,11
Thai Sign Language Recognition: an Application of Deep Neural Network,A. Chaikaew; K. Somkuan; T. Yuyen,"Computer Science, SCIT, Chiang Rai Rajabhat University, Chiang Rai, Thailand; Information Technology, SCIT, Chiang Rai Rajabhat University, Chiang Rai, Thailand; Computer Science, SCIT, Chiang Rai Rajabhat University, Chiang Rai, Thailand","2021 Joint International Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunication Engineering",11-May-21,2021,,,128,131,"Thai Sign Language (TSL) is the national sign language for Thai deaf people or hearing impaired in Thailand. These people with disabilities can use sign language to communicate with people with disabilities but face obstacles in communicating daily with ordinary people. Technology should play a key role in helping disadvantaged people achieve a better quality of life. This research aims to find ways to create Thai sign language recognition applications and be developed for real-time sign language translation in the next step. We purpose a simple approach with a MediaPipe framework that helps to extract the hand landmark from video on the preprocessing step and use that landmark to build the model for recognition hand gestures with various Recurrent neural networks (RNN). The result showed that the model builds with LSTM, BLSTM and GRU has an accuracy greater than 90 percent. This approach can produce an accurate close to the traditional approach.",,978-1-6654-1569-9,10.1109/ECTIDAMTNCON51128.2021.9425711,Research and Development; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425711,Computer Vision;Machine Learning;Neural Network;Thai Sign Language (TSL),Recurrent neural networks;Assistive technology;Face recognition;Digital art;Gesture recognition;Auditory system;Streaming media,handicapped aids;linguistics;real-time systems;recurrent neural nets;sign language recognition,recognition hand gestures;recurrent neural networks;deep neural network;national sign language;Thai deaf people;ordinary people;disadvantaged people;real-time sign language translation;Thai sign language recognition;RNN;TSL;LSTM;BLSTM;GRU;MediaPipe framework,,13,,12,IEEE,11-May-21,,,IEEE,IEEE Conferences,Yes ,Yes,No,No,"In this paper, we presented Thai Sign Languages recognition that builds from the MediaPipe framework by extracting key points from hand landmarks.",,
Visual Descriptors Based Real Time Hand Gesture Recognition,S. S. Kakkoth; S. Gharge,"Department of EXTC, V.E.S Institute of Technology, Mumbai, India; Department of EXTC, V.E.S Institute of Technology, Mumbai, India",2018 International Conference On Advances in Communication and Computing Technology (ICACCT),11-Nov-18,2018,,,361,367,"Vision based gesture recognition technologies based on structural shape descriptors and contour analysis methods have been described in this paper. Gesture have been the basic mode of communication since ancient times, until vocal communication was developed. Skin color detection technique for hand segmentation based on YCbCr methods has proved effective enough in real environment. Haar based classifier for face detection is used to remove one of the largest contour besides hand. Median filtering along with morphological operations applied here alleviates the effects of noise to a great extend without losing the boundary information in the image. The binary image formed after this only consists of two largest skin colored contours, mostly one or two hand contour image. Over this resultant contour, structural analysis based on contour shape along with convex hull and convexity defect formation together with geometrical analysis based on angle between convexity defect point and that of hand centroid helps in determining the number of fingertips. Based on number of finger-tips, the gestures can be classified into various gestures. Various applications in real time hand gesture recognition include American Sign Language (ASL) recognition, human computer interaction (HCI), robotics, real time traffic signal control and remotely controlling devices such as TVs, air conditioners, fan using hand gestures.",,978-1-5386-0926-2,10.1109/ICACCT.2018.8529663,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8529663,Hand gesture recognition;RGB camera;contour;convex hull;color space;ASL;image moments,Image color analysis;Face;Skin;Gesture recognition;Image segmentation;Real-time systems;Cameras,computer vision;feature extraction;gesture recognition;image colour analysis;image filtering;image segmentation;median filters,vision based gesture recognition technologies;structural shape descriptors;contour analysis methods;vocal communication;skin color detection technique;hand segmentation;YCbCr methods;haar based classifier;largest skin colored contours;hand contour image;structural analysis;contour shape;convex hull;convexity defect formation;hand centroid;real time traffic signal control;hand gestures;American sign language recognition;visual descriptors based real time hand gesture recognition,,2,,16,IEEE,11-Nov-18,,,IEEE,IEEE Conferences,Yes ,No,,,,,
Sign Language Recognition and Translation Method based on VTN,W. Qin; X. Mei; Y. Chen; Q. Zhang; Y. Yao; S. Hu,"College of Electrical Engineering and Control Science, Nanjing Tech University, Nanjing, China; College of Electrical Engineering and Control Science, Nanjing Tech University, Nanjing, China; College of Electrical Engineering and Control Science, Nanjing Tech University, Nanjing, China; College of Electrical Engineering and Control Science, Nanjing Tech University, Nanjing, China; College of Electrical Engineering and Control Science, Nanjing Tech University, Nanjing, China; Department of Mechatronics and Automotive, Chizhou Vocational And Technical College, Chizhou, China",2021 International Conference on Digital Society and Intelligent Systems (DSInS),10-Jan-22,2021,,,111,115,"Sign language recognition plays an important role in real-time sign language translation, communication for deaf people, education and human-computer interaction. However, vision-based sign language recognition faces difficulties such as insufficient data, huge network models and poor timeliness. We use VTN (Video Transformer Net) to construct a lightweight sign language translation network. We construct the dataset called CSL_BS (Chinese Sign Language-Bank and Station) and two-way VTN to train isolated sign language and compares it with I3D (Inflated three Dimension). Then I3D and VTN are respectively used as feature extraction modules to extract the features of continuous sign language sequences, which are used as the input of the continuous sign language translation decoding network (seq2seq). Based on CSL-BS, two-way VTN achieves 87.9% accuracy while two-way I3D is 84.2%. And the recognition speed is increased by 46.8%. In respect of continuous sign language translation, the accuracy of VTN_seq2seq is 73.5% while I3D_seq2seq is 71.2%, the recognition speed is 13.91s and 26.54s respectively.",,978-1-6654-0630-7,10.1109/DSInS54396.2021.9670588,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9670588,component;continuous sign language recognition;Isolated sign language recognition;sign language feature extraction;encoding-decoding network;sign language dataset,Human computer interaction;Gesture recognition;Assistive technologies;Streaming media;Parallel processing;Feature extraction;Transformers,computer vision;feature extraction;human computer interaction;language translation;natural language processing;sign language recognition,translation method;education;human-computer interaction;vision-based sign language recognition;lightweight sign language translation network;Chinese sign language-bank and station;sign language sequences;sign language translation decoding network;CSL-BS;recognition speed;VTN,,2,,21,IEEE,10-Jan-22,,,IEEE,IEEE Conferences,Yes ,Yes,No,No,"In this paper, we use the VTN recognizes isolated sign language and extracts continuous sign language features which be saved as the input of seq2seq model.(pg-2)",,
"Real-time sign languages recognition based on hausdorff distance, Hu invariants and neural network",S. Ãzbay; M. Safar,"Department of Electrical and Electronics Engineering, University of Gaziantep, Gaziantep, Turkey; Department of Electrical and Electronics Engineering, University of Gaziantep, Gaziantep, Turkey",2017 International Conference on Engineering and Technology (ICET),08-Mar-18,2017,,,1,8,"In this paper, a universal sign languages recognition system is proposed using two different approaches. Although a variety of sign languages exist in use, the proposed system recognizes three widely used sign languages as the American Sign Language, British Sign Language and Turkish Sign Language to make the recognition universal. One of the proposed approach is primarily based on Hausdorff distance and Hu invariants as the feature vectors and the system first focuses on how to process the hand movements and then it recognizes the different letters by the help of Hausdorff distance and Hu invariants measurements. The second approach is implemented by a feed forward neural network structure and recognition is provided with training data as three different sign language alphabets. It is shown that both approaches operate successfully while experimental results demonstrate that neural network based method provides superior performance.",,978-1-5386-1949-0,10.1109/ICEngTechnol.2017.8308204,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8308204,,Gesture recognition;Assistive technology;Shape;Training;Computer vision;Real-time systems,feature extraction;feedforward neural nets;sign language recognition,hausdorff distance;universal sign languages recognition system;American Sign Language;British Sign Language;Turkish Sign Language;Hu invariants measurements;feed forward neural network structure;neural network based method;universal recognition;real-time sign languages recognition;sign language alphabets;training data;feature vectors,,3,,17,IEEE,08-Mar-18,,,IEEE,IEEE Conferences,Yes ,Yes,,,,,
Sign Language Detection using Action Recognition,V. H. Iyer; U. M. Prakash; A. Vijay; P. Sathishkumar,"Department of Computer Science and Engineering, SRM Institute of Science and Technology, Chennai, India; Department of Computer Science and Engineering, SRM Institute of Science and Technology, Chennai, India; Department of Computer Science and Engineering, SRM Institute of Science and Technology, Chennai, India; Department of Computer Science and Engineering, PGP college of Engineering and Technology, Chennai, India",2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),18-Jul-22,2022,,,1682,1685,"Sign Language Detection has become crucial and effective for humans and research in this area is in progress and is one of the applications of Computer Vision. Earlier works included detection using static signs with the help of a simple deep learning-based Convolutional Neural Network. This proposal is based on continuous detection of image frames in real-time using action detection so as to detect the action performed by the user. The model uses LSTM neural network model after identifying keypoints using mediapipe holistic which includes face, pose and hand features. The proposed work is done by collecting key value points for training and testing, pre-processing the data, and creating labels and features. It saves the weights and evaluates the model using confusion matrix accuracy.",,978-1-6654-3789-9,10.1109/ICACITE53722.2022.9823484,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9823484,Neural Network (Convolutional and Recurrent);Long-Short Term Memory(LSTM);Mediapipe Holistic;Computer Vision;Sign Language Detection,Training;Computational modeling;Face recognition;Neural networks;Gesture recognition;Assistive technologies;Real-time systems,computer vision;convolutional neural nets;feature extraction;learning (artificial intelligence);matrix algebra;recurrent neural nets;sign language recognition,sign language detection;action recognition;static signs;continuous detection;LSTM neural network model;simple deep learning based convolutional neural network;computer vision;image frames;real-time systems;matrix accuracy,,1,,21,IEEE,18-Jul-22,,,IEEE,IEEE Conferences,Yes ,Yes,No,No,As mentioned there had been many models developed over the years for sign language detection but the combination of mediapipe holistic and LSTM serves a model with very 8 high accuracy.(pg-3),,
Different Model for Hand Gesture Recognition with a Novel Line Feature Extraction,M. R. Mahmood; A. M. Abdulazeez,"Department of Computer, University of Zakho, Iraq-Kurdistan; Department of Computer, University of Zakho, Iraq-Kurdistan",2019 International Conference on Advanced Science and Engineering (ICOASE),30-May-19,2019,,,52,57,"Hand gestures are commonly used for communication between both impaired community and normal people. Sign languages stand for the human languages of deaf people. They form the most growing domain of research worldwide. A number of techniques was developed in this area lately. It is recognized by means of deducing the features involved in the use of hand gesture. As a matter of fact, various approaches, namely the vision-based, the data-glove-based, the colored-marker and the Electromyogram (EMG) approaches have been utilized by researchers to recognize the different hand gestures implemented in many different fields such as the whole approaches which can be divided into four main categories, viz. Data Collection, Image Processing, Feature Extraction and Gesture Recognition. Only few of those categories have been discussed in this paper to be compared between the accuracy rates by applying Artificial Neural Network (ANN) classification. This classification is based on different models and a novel method for Real-Time Hand Gesture Recognition System (RTHGRS). The latter has used one line (fifty features) extracted from black and white processed images to recognize the numbers from (1-10) in Kurdish Sign Language (KurdSL) using one hand only with accuracy 98%.",,978-1-5386-9343-8,10.1109/ICOASE.2019.8723731,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8723731,sign language;hand gesture recognition;kurdish sign language (kurdsl);artificial neural network;feature extraction,Feature extraction;Gesture recognition;Image color analysis;Electromyography;Sensors;Real-time systems;Assistive technology,data gloves;electromyography;feature extraction;handicapped aids;neural nets;sign language recognition,impaired community;human languages;deaf people;data-glove-based system;real-time hand gesture recognition system;image processing;electromyogram approaches;colored-marker;novel line feature extraction;Kurdish sign language;artificial neural network classification;data collection;vision-based method,,5,,31,IEEE,30-May-19,,,IEEE,IEEE Conferences,Yes ,Yes,No,No,,,
Real-Time American Sign Language Realization Using Transfer Learning With VGG Architecture,M. D. Khan; B. S. Patro; R. Ranjan; M. C. Behera; R. Kumar; U. Raj,"School Of Electronics, KIIT Deemed University, Bhubaneswar, India; School Of Electronics, KIIT Deemed University, Bhubaneswar, India; School Of Electronics, KIIT Deemed University, Bhubaneswar, India; School Of Electronics, KIIT Deemed University, Bhubaneswar, India; School Of Electronics, KIIT Deemed University, Bhubaneswar, India; School Of Electronics, KIIT Deemed University, Bhubaneswar, India","2021 IEEE 4th International Conference on Computing, Power and Communication Technologies (GUCON)",02-Nov-21,2021,,,1,5,"Sign language is a barrier between the hearing impaired and the hearing community. In the modern world, despite the technical advancements, it is still a struggle for specially-abled people to communicate and express their thoughts with the rest of the world. A system is built that will reduce the gap for the specially-abled, the system recognizes the hand gesture and translates it into corresponding letters. ASL data set is used. The software is built on Python and inherits the power of deep learning. A convolutional neural network is used for image classification. The system was able to classify all the alphabets from A-Z with an accuracy of 98.89 percent. The software aims to eradicate the communication barrier between the hearing-impaired community and the rest of the world. The software provides a medium for the inarticulate community to express their thoughts to everyone out there.",,978-1-7281-9951-1,10.1109/GUCON50781.2021.9573677,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9573677,Sign Language Realization;Deep Learning;Convolutional Neural Network;Computer Vision;Random Forest;real-time,Training;Deep learning;Transfer learning;Gesture recognition;Computer architecture;Auditory system;Assistive technologies,convolutional neural nets;deep learning (artificial intelligence);gesture recognition;handicapped aids;image classification;natural language processing,ASL;deep learning;convolutional neural network;image classification;hearing-impaired community;transfer learning;VGG architecture;hand gesture recognition;American sign language realization;Python,,,,24,IEEE,02-Nov-21,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""The
implemented deep learning application was designed to
identify hand gestures into one of the 26 English alphabets
thus implementing an artificial way of sign language
communication."" p.4.",,12
Real-Time Sign Language Recognition in Complex Background Scene Based on a Hierarchical Clustering Classification Method,T. -Y. Pan; L. -Y. Lo; C. -W. Yeh; J. -W. Li; H. -T. Liu; M. -C. Hu,"Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan",2016 IEEE Second International Conference on Multimedia Big Data (BigMM),18-Aug-16,2016,,,64,67,"Cameras are embedded in many mobile/wearable devices and can be used for gesture recognition or even sign language recognition to help the deaf people communicate with others. In this paper, we proposed a vision-based gesture recognition system which can be used in environments with complex background. We design a method to adaptively update the skin color model for different users and various lighting conditions. Three kinds of features are combined to describe the contours and the salient points of hand gestures. Principle Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Support Vector Machine (SVM) are integrated to construct a novel hierarchical classification scheme. We evaluated the proposed recognition method on two datasets: (1) the CSL dataset collected by ourselves, in which images were captured in complex background. (2) The public ASL dataset, in which images of the same gesture were captured in different lighting conditions. Our method achieves the accuracies of 99.8% and 94%, respectively, which outperforms the existing works.",,978-1-5090-2179-6,10.1109/BigMM.2016.44,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7544998,gesture recognition;sign language;LDA;SVM;hierarchical clustering,Gesture recognition;Assistive technology;Feature extraction;Skin;Image color analysis;Sensors;Training,cameras;handicapped aids;image classification;image colour analysis;pattern clustering;principal component analysis;sign language recognition;support vector machines,real-time sign language recognition;complex background scene;hierarchical clustering classification method;cameras;mobile devices;deaf people;vision-based gesture recognition system;skin color model;principle component analysis;PCA;linear discriminant analysis;LDA;support vector machine;SVM;hierarchical classification scheme;CSL dataset;ASL dataset;lighting conditions;wearable devices,,21,,13,IEEE,18-Aug-16,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""our proposed sign language recognition
system consists of two phases. In the training phase, we
collect images of 26 hand gestures and develop a robust
hand segmentation method to generate contour information.
Three kinds of features are then extracted to represent the
gesture and a dimension reduction step is applied to find
the most discriminative features for the final classification
step.""  p.65.",,13
Hand gesture recognition system with real-time palm tracking,I. Hussain; A. K. Talukdar; K. K. Sarma,"Dept. of Electronics and Communication Engg., Gauhati University, Guwahati, India; Dept. of Electronics and Communication Engg., Gauhati University, Guwahati, India; Dept. of Electronics and Communication Engg., Gauhati University, Guwahati, India",2014 Annual IEEE India Conference (INDICON),05-Feb-15,2014,,,1,6,"Over the years Vision based real time gesture recognition system has witnessed an exponential growth because of its manifoldness applications, ranging from sign language to virtual reality and its ability to interact with system efficiently through HCI. In this paper, we have proposed a hand gesture recognition system for American Sign Language recognition using important features of hand such as fingertips, palm center etc. The system is capable of recognizing hand gestures even when the fore-arm is involved and it can tolerate a certain rotation of palm and fore-arm. We have implemented principal component analysis to remove ambiguity between two similar types of gestures and given emphasis to detect movement epenthesis by means optical flow technique.",2325-9418,978-1-4799-5364-6,10.1109/INDICON.2014.7030571,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7030571,American Sign language (ASL);human computer interface (HCI);hue-saturation-value model (HSV);convex-hull;fingertips detection;optical flow;movement epenthesis (ME);principal component analysis (PCA),Gesture recognition;Image color analysis;Optical imaging;Hidden Markov models;Principal component analysis;Assistive technology;Skin,human computer interaction;image sequences;palmprint recognition;principal component analysis;sign language recognition,real-time palm tracking;vision-based real-time hand gesture recognition system;Ameriacn Sign Language recognition;hand features;fingertips;palm center;palm rotation;fore-arm rotation;principal component analysis;movement epenthesis detection;optical flow technique,,15,,15,IEEE,05-Feb-15,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Real time hand gesture recognition system for android devices,H. Lahiani; M. Elleuch; M. Kherallah,"National School of Electronics and Telecommunications, Univesrity of Sfax, Sfax, Tunisia; National School of Computer Science (ENSI), University of Manouba, Manouba, Tunisia; Faculty of Sciences, Univesrity of Sfax, Sfax, Tunisia",2015 15th International Conference on Intelligent Systems Design and Applications (ISDA),13-Jun-16,2015,,,591,596,"Hand gestures are natural and intuitive communication way for the human being to interact with his environment. They serve to designate or manipulate objects, to enhance speech, or communicate in a noisy place. They can also be a separate language. Gestures can have different meanings according to the language or culture. They can also be a way to interact with machines. The subject of our research concerns the design and development of computer vision methods for recognizing hand gestures by a mobile device. We have proposed a system based on SVM for recognizing various hand gestures. The system consists of four steps: hand segmentation, smoothing, feature extraction and classification. The idea here is to allow the smartphone to perform all necessary steps to recognize gestures without the need to connect to a computer in which a database is located to perform training process. With this system, all steps can be done by the smartphone. In this paper, for image acquisition, frontal camera of the smartphone is used. After that frames are gotten from the video, the color sampling is done which is followed by making binary representation of the hand, and then contours representing the hand were described with convex polygons to get information about fingertips and finally the input gesture was recognized using proper classifier.",2164-7151,978-1-4673-8709-5,10.1109/ISDA.2015.7489184,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489184,Hand gesture recognition;Image Segmentation;Android;SVM;Human Computer Interaction,Smoothing methods;Support vector machines;Image edge detection;Image color analysis,Android (operating system);computer vision;gesture recognition;image colour analysis;image representation;real-time systems;smart phones;support vector machines,real time hand gesture recognition system;Android devices;sign language;computer vision;mobile device;SVM;smartphone;color sampling;binary representation,,31,,15,IEEE,13-Jun-16,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""the steps that we
have used for recognizing different hand gestures are ColorBased Hand Segmentation, Median filter for smoothing, and
[mally Support Vector Machine as a proper classifier."" p.596.",,14
Real Time Gesture Detection Using Convolutional Neural Network,R. Shamalik; S. Koli,"Department of E&TC, G H Raisoni College of Engineering and Management Pune, India and BVCOEW, Pune, India; Department of E&TC, Dr.D.Y.Patil School Of Engineering, Pune, India","2022 IEEE 4th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)",22-Dec-22,2022,,,1,4,"Gestures make touch less connectivity and systems easier to use. In many applications, such as Augmented Reality (AR) and Virtual Reality (VR), computer vision is crucial. Systems created for people with physical disabilities and sign language detection rely heavily on gesture detection. In this study, Convolutional Neural Networks (CNN) are used to demonstrate a real-time system that enables efficient feature extraction for gesture detection. A CNN is trained for certain gestures that are essential in Human Machine Interaction (HMI), primarily for persons who struggle to communicate vocally. The performance of thresholding and depth perception, in addition to gesture detection, enables reliable results to be produced even in noisy backgrounds. Effective background elimination and contours aid in properly mapping hand gestures in a diverse range of backgrounds.",,978-1-6654-6246-4,10.1109/ICCCMLA56841.2022.9989195,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9989195,CNN;Computer Vision;Gesture Detection;HMI,Computer vision;Solid modeling;Pandemics;Graphics processing units;Streaming media;Feature extraction;Real-time systems,augmented reality;computer vision;feature extraction;gesture recognition;handicapped aids;human computer interaction;neural nets;virtual reality,Augmented Reality;CNN;computer vision;Convolutional Neural network;Convolutional Neural Networks;hand gestures;real-time system;sign language detection;time gesture detection,,,,11,IEEE,22-Dec-22,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,""" The proposed CNN is first
trained with 400 samples of each gesture to test its
effectiveness, and then the number of samples are raised for
each gesture to assess the highest levels of accuracy."" p.2.",,15
Real-Time Hand Detection using Convolutional Neural Networks for Costa Rican Sign Language Recognition,J. Zamora-Mora; M. ChacÃ³n-Rivas,"Escuela de IngenierÃ­a del Software, Universidad Cenfotec, San Jose, Costa Rica; Escuela de ComputaciÃ³n, Instituto TecnolÃ³gico de Costa, Cartago, Costa Rica",2019 International Conference on Inclusive Technologies and Education (CONTIE),30-Jan-20,2019,,,180,1806,"Sign language is the natural language for the deaf, something that comes naturally as a form of non-verbal communication between signers, ruled by a set of grammars that is in constant evolution as the universe of signs represents a small fraction of all words in Spanish. This limitation combined with the lack of knowledge in sign language by verbal speakers creates a separation where both parties (signers and non-signers) are unable to efficiently communicate, a problem that increases under a specific context such as emergency situations, where first-response teams such as EMTs, firefighters or police officers might be unable to properly attend an emergency as interactions between the involved parties becomes a barrier for decision making when time is scarce. Developing a cognitive-capable tool that serves to recognize sign language in a ubiquitous way, is a must to reduce barriers between the deaf and emergency corps under this context. Hand detection is the first step toward building a Costa Rican sign language (LESCO) recognition framework. Important advances in computing, particularly in the area of deep learning, open a new frontier for object recognition that can be leveraged to build a hand detection module. This study trains the MobileNet V1 convolutional neural network against the EgoHands dataset from Indiana University's UI Computer Vision Lab to determine if the dataset itself is sufficient to detect hands in LESCO videos, from five different signers that wear short-sleeve shirts and under complex backgrounds. Those requirements are key to determine the usefulness of the solution as consulted bibliography performs tests with single-color backgrounds and long-sleeve shirts that ease the classification tasks under controlled environments only. The two-step experiment obtained 1) a mean average precision of 96.1% for the EgoHands dataset and 2) a 91% average accuracy for hand detection across the five LESCO videos. Despite the high accuracy reported by the tests in this paper, the hand detection module was unable to detect certain hand shapes such as closed fists and open hands pointing perpendicular to the camera lens, which suggests that the complex egocentric views as captured in the EgoHands dataset might be insufficient for proper hand detection for Costa Rican sign language.",,978-1-7281-5436-7,10.1109/CONTIE49246.2019.00042,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8971435,Sign language recognition;LESCO;hand detection;convolutional neural network;deep learning;machine learning,Assistive technology;Gesture recognition;Feature extraction;Videos;Training;Real-time systems;Object detection,cognition;computer vision;convolutional neural nets;decision making;handicapped aids;image colour analysis;learning (artificial intelligence);object recognition;real-time systems;sign language recognition,time hand detection;convolutional neural networks;natural language;nonverbal communication;nonsigners;deaf emergency;Costa Rican sign language recognition framework;hand detection module;MobileNet V1 convolutional neural network;EgoHands dataset;LESCO videos;hand shapes;open hands;Indiana University UI Computer Vision Lab;single-color backgrounds;signers,,5,,32,IEEE,30-Jan-20,,,IEEE,IEEE Conferences,Yes ,Yes,,,,,
OkkhorNama: BdSL Image Dataset For Real Time Object Detection Algorithms,D. Talukder; F. Jahara; S. Barua; M. M. Haque,"Department of Computer Science and Engineering, Chittagong University of Engineering and Technology, Chattogram, Bangladesh; Department of Computer Science and Engineering, Chittagong University of Engineering and Technology, Chattogram, Bangladesh; Department of Computer Science and Engineering, Chittagong University of Engineering and Technology, Chattogram, Bangladesh; Department of Computer Science and Engineering, Chittagong University of Engineering and Technology, Chattogram, Bangladesh",2021 IEEE Region 10 Symposium (TENSYMP),04-Oct-21,2021,,,1,6,"In recent years, lots of researches are being conducted to interpret Bangladeshi Sign Language (BdSL) to the means that general people can communicate with people having a hearing impairment and reduce the verbal gap between them. Computer Vision is playing a vital role in this regard by developing a sustainable system to understand the signs for machine translations. To obtain optimal performance, along with the state-of-the-art CNN model, the requirement of a high-quality sign language dataset cannot be foreseen. In this paper, we have introduced a new image dataset OkkhorNama for Fingerspelled Bangladeshi Sign Language including all 46 signs with images over 12K. In each of the images, bounding boxes are carefully annotated and labeled. OkkhorNama contains images of high resolution, good quality, and adequate variation making it ideal to train object detection and localization algorithms that would perform well on real-world applications. The OkkhorNama dataset is compared with other datasets where OkkhorNama significantly outperforms other datasets in number and trained model performance. The dataset is publicly available for future research and development.",2642-6102,978-1-6654-0026-8,10.1109/TENSYMP52854.2021.9550907,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9550907,computer vision;dataset;bangla sign language;object detection,Location awareness;Training;Computer vision;Computational modeling;Gesture recognition;Object detection;Assistive technologies,computer vision;convolutional neural nets;handicapped aids;image resolution;language translation;natural language processing;object detection;sign language recognition,fingerspelled Bangladeshi sign language;real time object detection algorithms;machine translations;sustainable system;computer vision;hearing impairment;BdSL image dataset;OkkhorNama dataset;localization algorithms;high-quality sign language dataset;CNN model,,,,19,IEEE,04-Oct-21,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,""" A reason for YOLOv5s object detection model
performing well is due to simplicity of sign language
dataset as each image contains only one class. "" p.5.",,16
A Comprehensive Review on Sign Language Recognition Using Machine Learning,J. Singh; D. Singh,"School of Computer Science and Engineering Lovely Professional University, Punjab, India; School of Computer Science and Engineering Lovely Professional University, Punjab, India","2022 10th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",08-Dec-22,2022,,,1,6,"The people of the dumb and deaf community adopt sign language to talk with one another. Sign language is an effective method for the interaction between speech and hearing-impaired individuals and a normal person without any requirement of an interpreter. Every nation has its own sign language, which has been used by the impaired residents of that particular country to communicate either with normal individuals or with a computer system in real-time. The recognition of sign language becomes an emerging research domain in computer vision. The researchers find several difficulties to recognize them correctly and adequately as the sign instance varies with appearance as well as motion. This paper explained all the challenges faced by the researchers while developing or designing a real-time system for sign language recognition. The main intent of this paper is to review the numerous machine learning methodologies that are utilized by various researchers to present a real- time system for the recognition of sign language. The performances of the existing system are also compared with each other to identify which machine learning methodology is generating accurate as well as an effective result. Based on this review paper, further future research work can be conducted as this paper also presented the directions of present future research.",,978-1-6654-7433-7,10.1109/ICRITO56286.2022.9965118,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9965118,Machine Learning;Sign Language recognition system;Artificial intelligence;Detection system,Image recognition;Face recognition;Gesture recognition;Machine learning;Assistive technologies;Streaming media;Market research,computer vision;handicapped aids;hearing;learning (artificial intelligence);real-time systems;sign language recognition,computer vision;machine learning;real-time system;sign instance varies;sign language recognition,,,,32,IEEE,08-Dec-22,,,IEEE,IEEE Conferences,Yes ,Yes,Yes,No,,,
Image Translation of Bangla and English Sign Language to Written Language using Convolutional Neural Network,M. I. Bismoy; F. Shahrear; A. Mitra; D. M. Bikash; F. Afrin; S. Roy; H. Arif,"Dept. of CSE, Brac University, Dhaka, Bangladesh; Dept. of CSE, Brac University, Dhaka, Bangladesh; Dept. of CSE, Brac University, Dhaka, Bangladesh; Dept. of CSE, Brac University, Dhaka, Bangladesh; Dept. of CSE, Brac University, Dhaka, Bangladesh; Dept. of CSE, Brac University, Dhaka, Bangladesh; Dept. of CSE, Brac University, Dhaka, Bangladesh","2022 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",30-Dec-22,2022,,,1,6,"One particular thing that differentiates humans from other species is their abilities to interact. to communicate with others, humans invented languages as units. There are 6500 languages in this world and English has been established as a global language. However, there are a ton of physically disabled human beings who are deprived of expressing their emotions through verbal language. Therefore, sign language has been discovered by expressing feelings with the help of signs which is mainly done by moving body parts: hands in particular. Although, it is so rare to find a research where both Bangladeshi Sign Language (BdSL) and American Sign Language (ASL) is translated, previously some of the prominent researchers worked on primary or secondary ASL and BdSL datasets separately and obtained high accuracy (> 97%) based on their algorithmic approaches. The executed system focuses on implementing both of the aforementioned sign languages combinedly as well as working on both the primary and secondary datasets using single algorithmic approach in order to resolve the two way communication and a better understanding of communicating through sign language. In this thesis, the advantages of real world pictures of Bangladeshi Sign Languages will be used to run an algorithm which will convert sign language to written language using Sequential Convolutional Neural Network method. The system will be able to detect both BdSL and ASL regarding any background with the accuracy of 95.23% and 98.45% respectively.",,978-1-6654-7095-7,10.1109/ICECCME55909.2022.9988088,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9988088,Sequential Convolutional Neural Network;Sign Language;ASL;BdSL,Mechatronics;Machine learning algorithms;Open Access;Image processing;Gesture recognition;Assistive technologies;Real-time systems,computer vision;gesture recognition;handicapped aids;neural nets,aforementioned sign languages;American Sign Language;Bangladeshi Sign Language;English Sign Language;global language;physically disabled human beings;verbal language;written language,,,,10,IEEE,30-Dec-22,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""The proposed system will
divide the image with 128*128 pixels and then cross check it
with the BdSL dataset created by the authors to understand
which alphabet it is"" p.1.",,17
Survey on Real Time Hand Gesture Recognition,S. S. Kakkoth; S. Gharge,"Department of Electronics and telecommunications, V.E.S Institute of Technology, Mumbai, India; Department of Electronics and telecommunications, V.E.S Institute of Technology, Mumbai, India","2017 International Conference on Current Trends in Computer, Electrical, Electronics and Communication (CTCEEC)",06-Sep-18,2017,,,948,954,"Since ancient times, gestures has been the basic mode of communication, until humans developed vocal communication, but still, non-vocal communication is equally significant. These hand gestures have various application not only in entertainment and utility sections like human computer interaction, gaming but also in human life improvement sections like real time traffic signal control system and sign language recognition for the deaf and dumb. The major steps involved in implementing this recognition system include data acquisition, segmentation and tracking, feature extraction and gesture recognition. Hand gesture technologies can be basically divided into Sensor Based, Vision Based and Depth Based techniques. Each technique has its pro's and con's while being used for various applications. Based on the research works conducted by researchers various techniques implemented at each step can modified according to the improving hardware and software framework developments. This paper thus presents an amalgamation of various techniques and its substeps which can come handy while working on real time hand gesture recognition.",,978-1-5386-3243-7,10.1109/CTCEEC.2017.8455041,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8455041,hand gesture recognition;RGB camera;RGB-D data;hand glove sensor;contour;convex hull;Color space,Gesture recognition;Real-time systems;Image color analysis;Image segmentation;Feature extraction;Heuristic algorithms;Three-dimensional displays,data acquisition;feature extraction;gesture recognition;handicapped aids;human computer interaction,traffic signal control system;data acquisition;software framework developments;hand gesture technologies;feature extraction;recognition system;sign language recognition;human computer interaction;utility sections;hand gestures;nonvocal communication;vocal communication,,6,,17,IEEE,06-Sep-18,,,IEEE,IEEE Conferences,Yes ,Yes,Yes,No,,,
Sign Language Gesture Recognition through Computer Vision,C. N. Nyaga; R. D. Wario,"Department of Computer Science and Informatics, University of the Free State, Private Bag X13, Kestell, Republic of South Africa; Department of Computer Science and Informatics, University of the Free State, Private Bag X13, Kestell, Republic of South Africa",2018 IST-Africa Week Conference (IST-Africa),23-Jul-18,2018,,,Page 1 of 8,Page 8 of 8,"This paper presents a study that was conducted as a usability test on an existing gesture recognition system. Computer vision gesture recognition can offer hope in creation of a real time interpreter system that can solve the communication barrier that exists between the deaf and the hearing who don't understand sign language. The objectives of this study were to determine the effectiveness of the system, to determine the efficiency of the system and to determine the satisfaction of the deaf participants on the use of the system. In the study, 7 deaf participants evaluated the usability of the gesture recognition system. The researcher employed observation data collection technique followed by an interview which was facilitated by a sign language teacher. The participants found the system to be effective and efficient. All the participants also appeared to be interested, desire to be involved and were motivated by the system.",2576-8581,978-1-905824-60-1,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8417200,computer vision;deaf;gesture recognition;sign language;usability,Task analysis;Gesture recognition;Assistive technology;Interviews;Computer vision;Usability;Testing,computer vision;gesture recognition;handicapped aids;hearing,sign language gesture recognition;usability test;computer vision gesture recognition;sign language teacher;deaf participants;real time interpreter system;observation data collection,,,,18,,23-Jul-18,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
A system for teaching sign language using live gesture feedback,D. Kelly; J. McDonald; C. Markham,"NUI Maynooth, Ireland; NUI Maynooth, Ireland; NUI Maynooth, Ireland",2008 8th IEEE International Conference on Automatic Face & Gesture Recognition,10-Apr-09,2008,,,1,2,This paper presents a computer vision based virtual learning environment for teaching communicative hand gestures used in sign language. A virtual learning environment was developed to demonstrate signs to the user. The system then gives real time feedback to the user on their performance of the demonstrated sign. Gesture features are extracted from a standard web-cam video stream and shape and trajectory matching techniques are applied to these features to determine the feedback given to the user.,,978-1-4244-2153-4,10.1109/AFGR.2008.4813350,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4813350,,Education;Handicapped aids;Feedback;Shape;Deafness;Real time systems;Feature extraction;Streaming media;Principal component analysis;Performance analysis,computer aided instruction;computer vision;feedback;gesture recognition;virtual reality,sign language teaching;live gesture feedback;computer vision;virtual learning environment;communicative hand gestures;real time feedback;standard web-cam video stream;shape matching techniques;trajectory matching techniques,,3,,5,IEEE,10-Apr-09,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Sensor Dataglove for Real-time Static and Dynamic Hand Gesture Recognition,M. A. A. Faisal; F. F. Abir; M. U. Ahmed,"Department of Electrical and Electronic Engineering, University of Dhaka, Dhaka, Bangladesh; Department of Electrical and Electronic Engineering, University of Dhaka, Dhaka, Bangladesh; Department of Electrical and Electronic Engineering, University of Dhaka, Dhaka, Bangladesh","2021 Joint 10th International Conference on Informatics, Electronics & Vision (ICIEV) and 2021 5th International Conference on Imaging, Vision & Pattern Recognition (icIVPR)",18-Oct-21,2021,,,1,7,"Hand gesture recognition has been a widely explored field of Human Activity Recognition (HAR). In this work, we have presented a sensor-based hand gesture recognition framework to classify both static and dynamic hand gestures in real-time using a dataglove that contains a 3-axis accelerometer (ACC), a 3-axis gyroscope, and 5 flex sensors. We have collected data from 35 volunteers performing 14 static and 3 dynamic gestures wearing the dataglove. We have preprocessed the raw flex sensor data using digital filtering techniques and performed mathematical operations on the accelerometer and gyroscope data for determining accurate orientation and motion profile. Four classical machine learning algorithms were used and compared on both datasets. We have achieved maximum accuracy of 99.53% for static gestures and 98.64% for dynamic gestures using the K-Nearest Neighbors (KNN) classifier. Our proposed framework provides real-time wireless hand gesture detection for Human-Computer Interaction (HCI) and Sign Language Recognition (SLR).",,978-1-6654-4923-6,10.1109/ICIEVicIVPR52578.2021.9564226,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564226,Human-computer interaction;real-time hand gesture recognition;dataglove;flex sensor;IMU;KNN,Accelerometers;Wireless communication;Human computer interaction;Wireless sensor networks;Flexible printed circuits;Dynamics;Gesture recognition,accelerometers;data gloves;gyroscopes;human computer interaction;image classification;image filtering;image sensors;nearest neighbour methods;sign language recognition,real-time wireless hand gesture detection;human activity recognition;sensor-based hand gesture recognition framework;real-time static hand gesture recognition;dynamic hand gesture recognition;3-axis accelerometer;3-axis gyroscope;raw flex sensor data;sensor data glove;HAR;digital filtering technique;classical machine learning algorithms;k-nearest neighbors classifier;KNN classifier;human-computer interaction;HCI;sign language recognition;SLR,,3,,29,IEEE,18-Oct-21,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Recognize Vietnamese Sign Language Using Deep Neural Network,L. Huynh; V. Ngo,"Information Technology Department, Ho Chi Minh City University of Education, Ho Chi Minh City, Viet Nam; Information Technology Department, Ho Chi Minh City University of Education, Ho Chi Minh City, Viet Nam",2020 7th NAFOSTED Conference on Information and Computer Science (NICS),02-Feb-21,2020,,,191,196,"World Health Organization published an article called `Deafness and hearing loss' in March 2020, it said that more than 466 million people in the world lost their hearing ability, and 34 million of them were children. Sign Language has been born and developed for a long time, but its application to communicate has met with many inadequacies and difficulties. Many methods of Computer Vision-based approach gave good results on Sign Language Alphabet Recognition but all of them require the perfect result from background removing step. However, when it comes to real life, removing a complex background is too difficult for any simple background removing algorithms. In this work, our main purpose is to build a model based on deep learning that can recognize Vietnamese Sign Language Alphabet in a complex environment. Results obtained show a robust accuracy of this model in recognizing Vietnamese Sign Language Alphabet.",,978-0-7381-0553-6,10.1109/NICS51282.2020.9335904,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9335904,VSL;sign language;deep learning;object detection;artificial intelligence,Pediatrics;Assistive technology;Neural networks;Gesture recognition;Auditory system;Organizations;Real-time systems,computer vision;deep learning (artificial intelligence);handicapped aids;natural language processing;sign language recognition,deep neural network;hearing loss;computer vision;sign language alphabet recognition;deep learning;Vietnamese sign language recognition;deafness,,1,,21,IEEE,02-Feb-21,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""we are trying to build a Deep Learning model that
firstly can recognize sign language on a smaller scale, which
is the alphabet of sign language, because each letter in the
sign language is made of two elements, the shape of the hand
and the direction of the palm and they can be treated as one
being hand shape with color values. "" p.192.",,18
Sign Language Gesture Recognition using Zernike Moments and DTW,S. Mathur; P. Sharma,"Computer Science Department, Visvesvaraya National Institute of Technology, Nagpur, India; Computer Science Department, Visvesvaraya National Institute of Technology, Nagpur, India",2018 5th International Conference on Signal Processing and Integrated Networks (SPIN),27-Sep-18,2018,,,586,591,"Since the last few decades, a dominant area of research in the vision community has been the gesture recognition, mainly for the purpose of Human Computer Interaction (HCI) and recognition of sign language. In this paper, we are using Zernike Moments as shape descriptors. The proposed system for recognizing sign language mainly consists of following five modules: (1) gesture segmentation based on motion detection analysis, (2) real time detection of both hand regions and face region, (3) key frame extraction for removing redundant frames, (4) the feature extraction phase consists of tracking the hands trajectory in terms of orientation, tracking distance of hands from the centre of the face and determining the hand posture using rotation invariant Zernike Moments and finally (5) gesture recognition based on these extracted features using Dynamic Time Warping (DTW) methodology.",,978-1-5386-3045-7,10.1109/SPIN.2018.8474179,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8474179,gesture recognition;Zernike Moment;DTW;sign language recognition;HCI,Feature extraction;Gesture recognition;Assistive technology;Face;Shape;Videos;Signal processing algorithms,feature extraction;image motion analysis;image segmentation;shape recognition;sign language recognition,sign language gesture recognition;shape descriptors;motion detection analysis;feature extraction phase;rotation invariant Zernike Moments;gesture segmentation;key frame extraction;human computer interaction;HCI;dynamic time warping methodology;DTW methodology,,1,,12,IEEE,27-Sep-18,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""In this paper, we have proposed a system for sign language
gesture recognition which makes use of only one camera
thereby, making our system cost effective"" p.587.",,19
Maldivian Sign Language Recognition Application,R. Adam; A. Khalid,"Faculty of Engineering Science and Technology, Maldives National University, Maleâ, Maldives; Faculty of Engineering Science and Technology, Maldives National University, Maleâ, Maldives","2022 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",30-Dec-22,2022,,,1,6,"Maldivian Sign Language (MVSL) is the national sign language for the deaf or the hearing-impaired people in Maldives. These people with disabilities could easy communicate with other deaf people but face great difficulty conversing with ordinary people. There are many recognition applications and models made for many sign languages with different approaches. Sadly, no such technology has been made to recognize MVSL signs. Therefore, this research aims to recognize three dynamic MVSL signs (âwaterâ, âpromiseâ and âreadâ) in real-time and later expand to recognize more signs to create a translation application for MVSL. We propose an approach with MediaPipe holistic to extract key point landmarks from the face, two hands and the pose from the videos and build the model for recognition using LSTM. The model was able to obtain an accuracy of 93%.",,978-1-6654-7095-7,10.1109/ICECCME55909.2022.9988108,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9988108,Computer Vision;Machine Learning;MediaPipe;Neural Network;Maldivian Sign Language (MVSL);LSTM,Mechatronics;Face recognition;Computational modeling;Gesture recognition;Assistive technologies;Real-time systems;Videos,feature extraction;handicapped aids;linguistics;recurrent neural nets;sign language recognition,dynamic MVSL signs;hearing-impaired people;LSTM;Maldivian sign language recognition application;national sign language,,,,19,IEEE,30-Dec-22,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""In this paper, we presented Maldivian Sign
Language recognition using MediaPipe holistic by
extracting the key points from the video frames."" p.6.",,20
Real-time sign language gesture recognition using still-image comparison & motion recognition,D. Kumarage; S. Fernando; P. Fernando; D. Madushanka; R. Samarasinghe,"Department of Information Technology, SLIIT, Malabe, Sri Lanka; Department of Information Technology, SLIIT, Malabe, Sri Lanka; Department of Information Technology, SLIIT, Malabe, Sri Lanka; Department of Information Technology, SLIIT, Malabe, Sri Lanka; Department of Information Technology, SLIIT, Malabe, Sri Lanka",2011 6th International Conference on Industrial and Information Systems,10-Oct-11,2011,,,169,174,"A sign language is a language which uses visually transmitted sign patterns, instead of acoustically conveyed sound patterns, to deliver the meaning. Sign languages are typically constructed by simultaneous combination of hand shapes, orientations and movements of the hands, arms or body, with facial expressions to fluidly express a speaker's thoughts. This paper presents a less costly approach to develop a computer vision based sign language recognition application in real time context with motion recognition. We explore new concepts of breaking down motion gestures to sub components for parallel processing and mapping motion data into static data representations. This concept can be used to identify sign language gestures, without performing computational intensive tasks of each and every frame captured. Moreover, sign language gestures can be evaluated with minimal image processing and map the motion to linear/non-linear equations using functionalities proposed in this paper.",2164-7011,978-1-4577-0035-4,10.1109/ICIINFS.2011.6038061,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038061,Sign Language Recognition;American Sign Language;Motion Recognition;Computer Vision;Gesture Recognition,Cameras;Handicapped aids;Equations;Databases;Mathematical model;Image color analysis;Streaming media,computer vision;gesture recognition;image motion analysis;nonlinear equations;parallel processing;ubiquitous computing,real-time sign language gesture recognition;still-image comparison;motion recognition;visually transmitted sign pattern;sound pattern;facial expression;computer vision;motion gesture;parallel processing;mapping motion data;static data representation;computational intensive task;linear-nonlinear equation,,8,,20,IEEE,10-Oct-11,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Hand Gesture Recognition using YOLO Models for Hearing and Speech Impaired People,S. Saxena; A. Paygude; P. Jain; A. Memon; V. Naik,"School of Computer Engineering and Technology, Dr.Vishwanath Karad MIT World Peace University, Pune, India; School of Computer Engineering and Technology, Dr.Vishwanath Karad MIT World Peace University, Pune, India; School of Computer Engineering and Technology, Dr.Vishwanath Karad MIT World Peace University, Pune, India; School of Computer Engineering and Technology, Dr.Vishwanath Karad MIT World Peace University, Pune, India; School of Computer Engineering and Technology, Dr.Vishwanath Karad MIT World Peace University, Pune, India",2022 IEEE Students Conference on Engineering and Systems (SCES),27-Sep-22,2022,,,1,6,"Sign language recognition can help hearing and speech impaired persons to communicate with the rest of the world. Hand gesture recognition is widely used in the entertainment sector for cars, gaming, and other devices. In the healthcare industry, clinicians can utilise this recognition technique to handle digital images instead of touch screens or computer keyboards during medical procedures where sterilization is necessary, as well as to automatically recognise surgical gestures. For real-time, image-based hand gesture identification, recent improvements in machine learning and object detection approaches provide improved and more efficient results. We used the deep learning-based object detection models YOLOX and YOLOv5 to work on five different hand gestures for recognition in our research project. Both models were released at the same time, but YOLOX was significantly more precise and faster. For YOLOv5, we got the highest mAP score of 98%, the lowest mAP score of 89.6%, the precision score of 98.8%, and the recall score of 82.6%. Our best mAP for YOLOX was 99.55%, while our last mAP was 98.5%. In this research study, we evaluated the performance of the two models and investigated changes in the architecture that enabled YOLOX to perform so much better than YOLOv5.",,978-1-6654-8072-7,10.1109/SCES55490.2022.9887751,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887751,Hand Gesture;Computer Vision;CNN;YOLOv5;YOLOX;mAP,Training;Image recognition;Surgery;Speech recognition;Object detection;Gesture recognition;Auditory system,gesture recognition;handicapped aids;human computer interaction;learning (artificial intelligence);object detection,YOLO models;hearing speech impaired people;sign language recognition;hand gesture recognition;recognition technique;touch screens;computer keyboards;surgical gestures;image-based hand gesture identification;deep learning-based object detection models YOLOX;YOLOv5;different hand gestures,,,,15,IEEE,27-Sep-22,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""Not only among the YOLO versions, but also
among most CNN models, the YOLOv5 model was thought
to produce the greatest results. "" p.1.",,21
Real-Time Sign Language Recognition Using a Consumer Depth Camera,A. Kuznetsova; L. Leal-TaixÃ©; B. Rosenhahn,"Institute fuer Informationsverarbeitung, Leibniz University Hannover, Hannover, Germany; Institute fuer Informationsverarbeitung, Leibniz University Hannover, Hannover, Germany; Institute fuer Informationsverarbeitung, Leibniz University Hannover, Hannover, Germany",2013 IEEE International Conference on Computer Vision Workshops,06-Mar-14,2013,,,83,90,"Gesture recognition remains a very challenging task in the field of computer vision and human computer interaction (HCI). A decade ago the task seemed to be almost unsolvable with the data provided by a single RGB camera. Due to recent advances in sensing technologies, such as time-of-flight and structured light cameras, there are new data sources available, which make hand gesture recognition more feasible. In this work, we propose a highly precise method to recognize static gestures from a depth data, provided from one of the above mentioned devices. The depth images are used to derive rotation-, translation- and scale-invariant features. A multi-layered random forest (MLRF) is then trained to classify the feature vectors, which yields to the recognition of the hand signs. The training time and memory required by MLRF are much smaller, compared to a simple random forest with equivalent precision. This allows to repeat the training procedure of MLRF without significant effort. To show the advantages of our technique, we evaluate our algorithm on synthetic data, on publicly available dataset, containing 24 signs from American Sign Language(ASL) and on a new dataset, collected using recently appeared Intel Creative Gesture Camera.",,978-1-4799-3022-7,10.1109/ICCVW.2013.18,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755883,hand gesture recognition;ESF;random forest;range sensor,Assistive technology;Gesture recognition;Training;Vegetation;Vectors;Cameras;Sensors,cameras;computer vision;feature extraction;human computer interaction;sign language recognition,real-time sign language recognition;consumer depth camera;gesture recognition;computer vision;human computer interaction;HCI;RGB camera;time-of-flight cameras;structured light cameras;depth data;depth images;rotation-invariant features;translation-invariant features;scale-invariant features;multilayered random forest;MLRF;feature vector classification;hand sign recognition;American Sign Language;ASL;Intel creative gesture camera,,69,,24,IEEE,06-Mar-14,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Analysis of Sign Language Gestures Using Size Functions and Principal Component Analysis,D. Kelly; J. McDonald; T. Lysaght; C. Markham,"N.U.I. Maynooth Computer Science Department, Company Kildare, Ireland; N.U.I. Maynooth Computer Science Department, Company Kildare, Ireland; N.U.I. Maynooth Computer Science Department, Company Kildare, Ireland; N.U.I. Maynooth Computer Science Department, Company Kildare, Ireland",2008 International Machine Vision and Image Processing Conference,12-Sep-08,2008,,,31,36,This paper presents a computer vision based virtual learning environment for teaching communicative hand gestures used in Sign Language. A virtual learning environment was developed to demonstrate signs to the user. The system then gives real time feedback to the user on their performance of the demonstrated sign. Gesture features are extracted from a standard web-cam video stream and shape and trajectory matching techniques are applied to these features to determine the feedback given to the user.,,978-0-7695-3332-2,10.1109/IMVIP.2008.17,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4624381,Sign Language;Size Functions;Principal Component Analysis,Shape;Handicapped aids;Principal component analysis;Feature extraction;Equations;Mathematical model;Distance measurement,computer aided instruction;computer vision;feature extraction;gesture recognition;handicapped aids;image matching;principal component analysis,sign language gesture;size function;principal component analysis;computer vision;virtual learning;communicative hand gesture;feature extraction;trajectory matching technique,,4,,10,IEEE,12-Sep-08,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Spelling it out: Real-time ASL fingerspelling recognition,N. Pugeault; R. Bowden,"Centre of Vision, Speech and Signal Processing, University of Surrey, UK; Centre of Vision, Speech and Signal Processing, University of Surrey, UK",2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops),16-Jan-12,2011,,,1114,1119,"This article presents an interactive hand shape recognition user interface for American Sign Language (ASL) finger-spelling. The system makes use of a Microsoft Kinect device to collect appearance and depth images, and of the OpenNI+NITE framework for hand detection and tracking. Hand-shapes corresponding to letters of the alphabet are characterized using appearance and depth images and classified using random forests. We compare classification using appearance and depth images, and show a combination of both lead to best results, and validate on a dataset of four different users. This hand shape detection works in real-time and is integrated in an interactive user interface allowing the signer to select between ambiguous detections and integrated with an English dictionary for efficient writing.",,978-1-4673-0063-6,10.1109/ICCVW.2011.6130290,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130290,,Shape;Feature extraction;User interfaces;Handicapped aids;Vectors;Vegetation;Real time systems,gesture recognition;graphical user interfaces;image classification;natural languages;object detection;object tracking;real-time systems;shape recognition,real-time ASL fingerspelling recognition;interactive hand shape recognition user interface;American sign language;ASL finger-spelling;Microsoft Kinect device;depth images;OpenNI-NITE framework;hand detection;hand tracking;random forests;image classification;English dictionary,,217,3,15,IEEE,16-Jan-12,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
A Systematic Review on the Chronological Development of Bangla Sign Language Recognition Systems,A. Khatun; M. S. Shahriar; M. H. Hasan; K. Das; S. Ahmed; M. S. Islam,"Department of CSE, IUBAT, Dhaka, Bangladesh; Department of CSE, IUBAT, Dhaka, Bangladesh; Department of CSE, IUBAT, Dhaka, Bangladesh; Department of CSE, IUBAT, Dhaka, Bangladesh; Department of CSE, Islamic University of Technology, Dhaka, Bangladesh; Department of CSE, IUBAT, Dhaka, Bangladesh","2021 Joint 10th International Conference on Informatics, Electronics & Vision (ICIEV) and 2021 5th International Conference on Imaging, Vision & Pattern Recognition (icIVPR)",18-Oct-21,2021,,,1,9,"Despite sign language being the primary medium of conversation with deaf and dumb (DD) individuals, it is hard to grasp for any class of people, making communication immensely challenging. Bengali being one of the most spoken languages globally, significant research works on Bangla Sign Language (BdSL) have come to light to deal with this challenge. In recent years, researchers from diverse backgrounds have proposed solutions to automate the process of BdSL recognition. In this literature review, we have explored the trends of research on BdSL by comparing incorporated features and evaluation results of systems between 2002-2021 and techniques applied on different existing and new datasets. We have also collected and combined metadata from datasets of all BdSL Alphabets and numerics used so far. The literature findings of this paper indicate that most proposed models perform well on images containing static and single-handed signs but, performance falls significantly in complex backgrounds. Moreover, we focused on identifying insights and similarities of the existing system, finding research gaps and propose possible future directions. We expect this article will draw the attention of more researchers into this domain and this is the first identifiable academic literature review of BdSL recognition systems to the best of our knowledge.",,978-1-6654-4923-6,10.1109/ICIEVicIVPR52578.2021.9564157,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564157,Bengali Sign Language;Real-time Sign Language Recognition;Sign Language Datasets;BdSL;Intelligent Assistant Interaction with Deaf and Dumb,Systematics;Bibliographies;Gesture recognition;Assistive technologies;Metadata;Market research;Numerical models,computer vision;gesture recognition;handicapped aids;learning (artificial intelligence);meta data;natural language processing;neural nets;object detection;sign language recognition,complex backgrounds;identifiable academic literature review;BdSL recognition systems;systematic review;chronological development;Bangla Sign Language recognition systems;spoken languages;diverse backgrounds;different existing;BdSL Alphabets;literature findings,,2,,49,IEEE,18-Oct-21,,,IEEE,IEEE Conferences,Yes ,Yes,Yes,No,,,
Sign Language Apprehension using Convolution Neural Networks,M. P. Kane; S. Fernandes; R. Fonseca; S. Desai; A. Shetye; A. Sharma,"Padre Conceicao College of Engineering, Margao, Goa, India; Padre Conceicao College of Engineering, Panaji, Goa, India; Padre Conceicao College of Engineering, Panaji, Goa, India; Padre Conceicao College of Engineering, Porvorim, Goa, India; Padre Conceicao College of Engineering, Porvorim, Goa, India; Padre Conceicao College of Engineering, Margao, Goa, India",2022 13th International Conference on Computing Communication and Networking Technologies (ICCCNT),26-Dec-22,2022,,,1,7,"Sign language is one of the oldest and most natural forms of language for communication. Most people donât know sign language and practitioners are veritably delicate to come by, thus weâve come up with a real time system using neural networks for finger spelling grounded American subscribe Language(ASL). In our system, the hand is first passed through a complication filter and after the filter is applied the hand is passed through a classifier which predicts the class of the hand gestures. Sign language isnât enough for communication of people with hearing capability or people with speech disability. The gestures made by the people with disabilities get mixed or disordered for someone who has noway learned this language. Communication should be in both ways. In this paper, we introduce Sign Language recognition using American Sign Language. The user must be suitable to capture images of hand gestures using a web camera in this analysis, and the system must prognosticate and show the name of the captured image. The captured image undergoes a series of processing steps which include multiple Computer Vision ways similar as the conversion to gray-scale, threshold operation. Convolutional Neural Network(CNN) is used to train our model and identify the images. Our model has achieved accuracy about 94%.",,978-1-6654-5262-5,10.1109/ICCCNT54827.2022.9984249,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9984249,CNN: Convolutional Neural Networks;HMM: Hidden Markov Model;ASL: American Sign Language;ISL: Indian Sign Language;ROI: Region of Interest;KNN: K-Nearest Neighbour;ReLu: Rectified Linear Unit;RGB: Red;Green;Blue;SLR: Sign Language Recognition;TMS: Transcranial Magnetic Stimulation,Training;Computational modeling;Neural networks;Hidden Markov models;Gesture recognition;Speech recognition;Assistive technologies,computer vision;gesture recognition;handicapped aids;image sensors;learning (artificial intelligence);neural nets;sign language recognition,American Sign Language;convolution Neural networks;hand gestures;Sign Language apprehension;Sign Language recognition,,,,30,IEEE,26-Dec-22,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Indian Sign Language Gesture Recognition using Image Processing and Deep Learning,N. K. Bhagat; Y. Vishnusai; G. N. Rathna,"Department of Electrical Engineering, Indian Institute of Science, Bengaluru, Karnataka; Department of E&C, R.V. College of Engineering, Bangalore, India; Department of Electrical Engineering, Indian Institute of Science, Bangalore, India",2019 Digital Image Computing: Techniques and Applications (DICTA),02-Jan-20,2019,,,1,8,"Speech impaired people use hand based gestures to communicate. Unfortunately, the vast majority of the people are not aware of the semantics of these gestures. In a attempt to bridge the same, we propose a real time hand gesture recognition system based on the data captured by the Microsoft Kinect RGB-D camera. Given that there is no one to one mapping between the pixels of the depth and the RGB camera, we used computer vision techniques like 3D contruction and affine transformation. After achieving one to one mapping, segmentation of the hand gestures was done from the background noise. Convolutional Neural Networks (CNNs) were utilised for training 36 static gestures relating to Indian Sign Language (ISL) alphabets and numbers. The model achieved an accuracy of 98.81% on training using 45,000 RGB images and 45,000 depth images. Further Convolutional LSTMs were used for training 10 ISL dynamic word gestures and an accuracy of 99.08% was obtained by training 1080 videos. The model showed accurate real time performance on prediction of ISL static gestures, leaving a scope for further research on sentence formation through gestures. The model also showed competitive adaptability to American Sign Language (ASL) gestures when the ISL models weights were transfer learned to ASL and it resulted in giving 97.71% accuracy.",,978-1-7281-3857-2,10.1109/DICTA47822.2019.8945850,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945850,Gesture Recognition;Indian Sign Language;Convolutional Neural Networks;LSTMs;Microsoft Kinect,Feature extraction;Training;Gesture recognition;Cameras;Mathematical model;Real-time systems;Three-dimensional displays,convolutional neural nets;image segmentation;learning (artificial intelligence);sign language recognition,image processing;deep learning;RGB camera;computer vision techniques;affine transformation;hand gestures;ISL static gestures;Indian sign language gesture recognition;real time hand gesture recognition system;background noise;convolutional neural networks,,27,,17,IEEE,02-Jan-20,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""we used depth based segmentation
by taking data from the Microsoft Kinect RGB-D camera.
Using the information from the depth channel of the Kinect,
the hand gesture region can easily be segmented from the
background."" p.1.",,22
Real-time detection of hand gestures,P. Muzyka; M. Frydrysiak; E. Roszkowska,"Electronics Faculty, Wroclaw University of Technology; Electronics Faculty, Wroclaw University of Technology; Department of Cybernetics and Robotics, Wroclaw University of Technology",2016 21st International Conference on Methods and Models in Automation and Robotics (MMAR),26-Sep-16,2016,,,168,173,"This paper presents one of the approaches to the gesture recognition problem, which might be valuable in development of social robots, companions for deaf-mute people or in design of new control methods. A universal PC application for gesture recognition in image sequence acquired from a simple USB camera in real-time was made. The application recognises hand gestures which indicate chosen letters from the sign language alphabet and can be adjusted to any hand-shape pattern. The algorithm is resistant to most lighting changes, different sizes and shapes of users' hands and background patterns.",,978-1-5090-1866-6,10.1109/MMAR.2016.7575127,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575127,Gesture recognition;Pattern matching;Image processing;Computer vision;NI Vision,Pattern matching;Gesture recognition;Lighting;Gray-scale;Image segmentation;Feature extraction,cameras;gesture recognition;image sequences;object detection;palmprint recognition,real-time detection;hand gesture detection;gesture recognition problem;social robots;deaf-mute people;control methods;universal PC application;image sequence;USB camera;sign language alphabet;hand-shape pattern,,1,,9,IEEE,26-Sep-16,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Gesture Based Operating System Control,Y. V. Parkale,"Electronics and Telecommunication Department, College of Engineering, Malegaon (Bk), Maharashtra, India",2012 Second International Conference on Advanced Computing & Communication Technologies,15-Mar-12,2012,,,318,323,"Human Computer Interaction keeps moving toward interfaces which are more natural and intuitive to use, in comparison to traditional keyboard and mouse. Hand gestures are an important modality for human computer interaction (HCI) [1]. Compared to many existing interfaces, hand gestures have the advantages of being easy to use, natural, and intuitive. Successful applications of hand gesture recognition include computer games control [2], human-robot interaction [3], and sign language recognition [4], to name a few. Vision-based recognition systems can give computers the capability of understanding and responding to hand gestures. The aim of this technique is the proposal of a real time vision system for its application within visual interaction environments through hand gesture recognition, using general-purpose hardware and low cost sensors, like a simple personal computer and an USB web cam, so any user could make use of it in his office or home. The basis of our approach is a fast segmentation process to obtain the moving hand from the whole image, which is able to deal with a large number of hand shapes against different backgrounds and lighting conditions, and a recognition process that identifies the hand posture from the temporal sequence of segmented hands. The use of a visual memory (Stored database) allows the system to handle variations within a gesture and speed up the recognition process through the storage of different variables related to each gesture. In this paper, we have successfully implemented a vision-based system that can interpret a user's gestures in real time with Speech Application Programming Interface (SAPI) to control windows O.S. actions like single click, double click, forward, backward, media player, notepad, calculator.",2327-0659,978-1-4673-0471-9,10.1109/ACCT.2012.58,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6168382,Human machine interface (HMI);Hand gestures,Image color analysis;Speech;Speech recognition;Color;Context;Mice;Cameras,application program interfaces;gesture recognition;human computer interaction;image segmentation;lighting;operating systems (computers);real-time systems;shape recognition,gesture based operating system control;human computer interaction;HCI;hand gesture recognition;computer games control;human-robot interaction;sign language recognition;vision-based recognition systems;real time vision system;visual interaction environments;general-purpose hardware;sensors;personal computer;USB web cam;image segmentation process;hand shape recognition;background condition;lighting condition;hand posture recognition;visual memory;stored database;segmented hand temporal sequence;speech application programming interface;Windows operating system,,8,,9,IEEE,15-Mar-12,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Intelligent Signs Language Understanding with Autonomous Landmarks for E-learning Context,M. Jamil Hussain; A. Shaoor,"Dept. of Computer Science, Air University, Islamabad, Pakistan; Dept. of Computer Science, Air University, Islamabad, Pakistan",2022 19th International Bhurban Conference on Applied Sciences and Technology (IBCAST),30-Dec-22,2022,,,219,224,"Human sign language understanding has become very important task nowadays, which is attracting interest of researchers. In this research work, an efficient sign language recognition system has been presented. The system is based on extraction of landmarks. The Mediapipe and OpenCV (Open Computer Vision) are used for extraction of landmarks. The extracted landmarks are further utilized to generate new type of features. The machine learning (ML) based classifiers have been proposed to classify the hand signs from the extracted features. The random forest has been selected as a base classifier. It presented accuracy rates of 98.76% over ISL (Irish sign language) dataset and 98.68% over ASL (American Sign Language) dataset. Other ML-based classifiers are also selected to work with new efficient features such as support vector machines (SVM), k-nearest neighbors (KNN), decision trees and naive Bayes. The results of these classifiers have also been compared. The base classifier performance is very efficient when compared to others and it works in real time (15 to 30 processed frames per second).",2151-1411,978-1-6654-6051-4,10.1109/IBCAST54850.2022.9990143,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9990143,Machine learning;hand gestures recognition;decision tree;features extraction;random forest;landmarks,Support vector machines;Electronic learning;Heuristic algorithms;Gesture recognition;Forestry;Assistive technologies;Feature extraction,artificial intelligence;computer vision;decision trees;feature extraction;image classification;image recognition;nearest neighbour methods;pattern classification;random forests;shape recognition;support vector machines,American sign language;ASL dataset;autonomous landmarks;base classifier performance;decision trees;e-learning context;extracted landmarks;hand signs;human sign language understanding;intelligent sign language understanding;Irish sign language;k-nearest neighbors;KNN;machine learning based classifiers;ML-based classifiers;naive Bayes;open computer vision;random forest;sign language recognition system;support vector machines;SVM,,,,86,IEEE,30-Dec-22,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""New type of feature has been extracted
in this study, which improved performance of the overall
algorithm.""  p.222.",,23
Novel Technique for Isolated Sign Language Based on Fingerspelling Recognition,A. Y. Dawod; N. Chakpitak,"International College Of Digital Innovation(ICDI), Chiang Mai University, Muang, Chiang Mai, Thailand, 50200; International College Of Digital Innovation(ICDI), Chiang Mai University, Muang, Chiang Mai, Thailand, 50200","2019 13th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",06-Feb-20,2019,,,1,8,"Sign language is used by deaf and hard hearing people to exchange information between their own community and with other people. Fingerspelling recognition method from isolate sign language has attracted research interest in computer vision and human-computer interaction based on a novel technique. The essential for real-time recognition of isolate sign language has grown with the emergence of better-capturing devices such as Kinect sensors. The purpose of this paper is to design a user independent framework for automatic recognition of American Sign Language which can recognize several one-handed dynamic isolated signs and interpreting their meaning. We built datasets as a raw data for alphabets (A-Z) or numbers (1-20) by used left-hand the 3D point (XL, YL, ZL) or switch by right-hand (XR, YR, ZR) centroid as one of contribution. The proposed approach was tested for gestures that involve left-hand or right-hand and was compared with other approach and gave better accuracy. Two machine learning methods are involved like Hidden Conditional Random Field (HCRF), and Random Decision Forest (RDF) for the classification part. The third contribution based on low lighting condition and cluttered background. In this research work is achieved for recognition accuracy over 99.7%.",2573-3214,978-1-7281-2741-5,10.1109/SKIMA47702.2019.8982452,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8982452,isolate sign language;fingerspelling recognition;american sign language;hand gesture recognition;raw data,,computer vision;feature extraction;gesture recognition;handicapped aids;image classification;learning (artificial intelligence);sign language recognition,automatic recognition;American sign language;dynamic isolated signs;deaf hearing people;hard hearing people;fingerspelling recognition method;real-time recognition;isolated sign language,,3,,17,IEEE,06-Feb-20,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""we reported a comparative study of using
isolated sign language recognition and feature extractionbased approaches in recognizing hand shapes."" p.8.",,24
Recognizing & interpreting Indian Sign Language gesture for Human Robot Interaction,A. Nandy; S. Mondal; J. S. Prasad; P. Chakraborty; G. C. Nandi,"Robotics & Artificial Intelligence Lab, Indian Institute of Information Technology, Allahabad, India; Robotics & Artificial Intelligence Lab, Indian Institute of Information Technology, Allahabad, India; Robotics & Artificial Intelligence Lab, Indian Institute of Information Technology, Allahabad, India; Robotics & Artificial Intelligence Lab, Indian Institute of Information Technology, Allahabad, India; Robotics & Artificial Intelligence Lab, Indian Institute of Information Technology, Allahabad, India",2010 International Conference on Computer and Communication Technology (ICCCT),18-Nov-10,2010,,,712,717,"This paper describes a novel approach towards recognizing of Indian Sign Language (ISL) gestures for Humanoid Robot Interaction (HRI). An extensive approach is being introduced for classification of ISL gesture which imparts an elegant way of interaction between humanoid robot HOAP-2 and human being. ISL gestures are being considered as a communicating agent for humanoid robot which is being used in this context explicitly. It involves different image processing techniques followed by a generic algorithm for feature extraction process. The classification technique deals with the Euclidean distance metric. The concrete HRI system has been established for initiation based learning mechanism. The Real time robotics simulation software, WEBOTS has been adopted to simulate the classified ISL gestures on HOAP-2 robot. The JAVA based software has been developed to deal with the entire HRI process.",,978-1-4244-9034-9,10.1109/ICCCT.2010.5640434,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5640434,Indian Sign Language;Gesture;Real Time;Euclidean distance;WEBOTS simulation software;HOAP-2 humanoid robot,Humanoid robots;Joints;Real time systems;Histograms;Software;Robot kinematics,feature extraction;gesture recognition;humanoid robots;human-robot interaction;image classification;image processing;robot vision,Indian sign language gesture interpretion;human robot interaction;humanoid robot;image processing technique;feature extraction process;classification technique;Euclidean distance metric;concrete HRI system;WEBOTS;real time robotics simulation software;HOAP-2 robot;JAVA based software,,29,1,18,IEEE,18-Nov-10,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
British Sign Language Recognition In The Wild Based On Multi-Class SVM,M. Quinn; J. I. Olszewska,"School of Computing and Engineering, University of the West of Scotland, United Kingdom; School of Computing and Engineering, University of the West of Scotland, United Kingdom",2019 Federated Conference on Computer Science and Information Systems (FedCSIS),07-Oct-19,2019,,,81,86,"Developing assistive, cost-effective, non-invasive technologies to aid communication of people with hearing impairments is of prime importance in our society, in order to widen accessibility and inclusiveness. For this purpose, we have developed an intelligent vision system embedded on a smartphone and deployed in the wild. In particular, it integrates both computer vision methods involving Histogram of Oriented Gradients (HOG) and machine learning techniques such as multi-class Support Vector Machine (SVM) to detect and recognize British Visual Language (BSL) signs automatically. Our system was successfully tested on a real-world dataset containing 13,066 samples and shown an accuracy of over 99% with an average processing time of 170ms, thus appropriate for real-time visual signing.",2300-5963,978-83-952357-8-8,10.15439/2019F274,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8860003,,Support vector machines;Assistive technology;Visualization;Kernel;Gesture recognition;Machine vision;Histograms,computer vision;handicapped aids;hearing;learning (artificial intelligence);sign language recognition;smart phones;support vector machines,noninvasive technologies;hearing impairments;intelligent vision system;smartphone;computer vision methods;machine learning techniques;real-time visual signing;multiclass SVM;multiclass support vector machine;British sign language recognition;British visual language;histogram of oriented gradients;HOG;real-world dataset,,1,,21,,07-Oct-19,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""In this work, we propose the development of an accessible,
intelligent vision system for real-time, automated BSL recognition in the wild. This assistive technology is inbuilt as a
smartphone application, using computer-vision algorithms to
process the images captured in real-time by the smartphone camera and translating the detected hand pose into a letter
using the machine learning technique called Support Vector
Machine"" p.81. ",,25
"Automated Bangla sign language translation system: Prospects, limitations and applications",M. T. Hoque; M. Rifat-Ut-Tauwab; M. F. Kabir; F. Sarker; M. N. Huda; K. Abdullah-Al-Mamun,"Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; Department of Computer Science and Engineering, Ahsanullah University of Science and Technology, Dhaka, Bangladesh; Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; Department of Computer Science and Engineering, University of Liberal Arts Bangladesh, Dhaka, Bangladesh; Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh; Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh","2016 5th International Conference on Informatics, Electronics and Vision (ICIEV)",01-Dec-16,2016,,,856,862,"Deaf and Dumb people are a big part of growing community where they use sign language i.e., gestures for communicating with others. But interaction between normal and hearing impaired people becomes difficult as most of the normal people can't understand sign language gestures meaning and on the other hand deaf and dumb people can't understand naturally spoken language. There are about 70 million deaf hearing-impaired as well as hearing people in the world who use sign language as their first language or mother tongue. In Bangladesh alone, there are around 2.4 million people use sign language. Therefore, automatic sign language translation system is vital to improve communication for hearing impaired people. This paper represents a review on existing Bangla Sign Language translation systems to communicate with hearing impaired people and also proposes a real time bidirectional Bangla Sign Language translator. Analyzing the existing systems provides us necessary information about their working process, success rate, system drawbacks and limitations indicating the fields where those systems need to upgrade for a better performance. Based on the analysis to overcome the limitations, we propose a bidirectional communication system that includes conversion of naturally spoken Bangla Language to Bangla Sign Language and vice versa. Our proposal includes real time dynamic bidirectional Bangle Sign Language translation system which is preciously needed for a fruitful communication with hearing impaired people.",,978-1-5090-1269-5,10.1109/ICIEV.2016.7760123,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760123,Bangla Sign Language;Bidirectional System;Static;Dynamic;Gesture Recognition;Real time Detection,Gesture recognition;Assistive technology;Real-time systems;Auditory system;Streaming media;Speech;Search engines,handicapped aids;language translation;natural language processing,Bangla sign language translation system;deaf people;dumb people;bidirectional communication system;hearing impaired people,,8,,23,IEEE,01-Dec-16,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
A method for hand tracking and motion recognizing in Chinese sign language,Zou Wei; Yuan Kui; Liu Jindong; Luo Bencheng,"Laboratory of Engineering Science for Complex System Institute of Automatic, Chinese Academy and Sciences, Beijing, China; Laboratory of Engineering Science for Complex System Institute of Automatic, Chinese Academy and Sciences, Beijing, China; Laboratory of Engineering Science for Complex System Institute of Automatic, Chinese Academy and Sciences, Beijing, China; Laboratory of Engineering Science for Complex System Institute of Automatic, Chinese Academy and Sciences, Beijing, China",2001 International Conferences on Info-Tech and Info-Net. Proceedings (Cat. No.01EX479),06-Aug-02,2001,3,,543,549 vol.3,"In this paper, a novel method of hand tracking is introduced, which can get the depth of hand by fusing the information sampled by devices of computer vision and bend-sensors fixed on the elbow. Based on this, a new coded representation is proposed to represent the movement trajectory, which can be used in the Chinese Sign Language recognition. Experiment results have shown that the method proposed in this paper is quite effective.",,0-7803-7010-4,10.1109/ICII.2001.983113,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=983113,,Tracking;Handicapped aids;Computer vision;Cameras;Elbow;Pattern recognition;Real time systems;Laboratories;Postal services;Trajectory,gesture recognition;computer vision;tracking;sensor fusion;angular measurement,hand tracking;motion recognition;Chinese sign language;hand depth;computer vision;bend-sensors;elbow;coded representation,,,,13,IEEE,06-Aug-02,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""The basic idea of this method is to get the planar coordinates of hand using computer
vision, and get the depth information of hand using the bending sensors fixed on the elbow of the
user"" p.544.",,26
Real-Time Vision-Based Chinese Sign Language Recognition with Pose Estimation and Attention Network,S. Cheng; C. Huang; Z. Wang; J. Wang; Z. Zeng; F. Wang; Q. Ding,"School of Computer Science, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China; School of Computer Science, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China",2021 IEEE International Conference on Robotics and Biomimetics (ROBIO),28-Mar-22,2021,,,1210,1215,"Currently, communication between deaf and hearing people is still facing great problems both at the research level and at the application level. Also, practical automatic sign language interpretation methods have become a relatively large demand. In this paper, we will recognize video sign language based on computer vision to capture skeletal key point information using multi-head attention mechanism and long and short-term memory mechanism. We construct a database of Chinese video sign language consisting of 30 words, with a total data volume of more than 1200. Meanwhile, the experiments of our proposed framework on this dataset achieved 85% accuracy rate. The experimental results show that our proposed method has the characteristics of high accuracy and light weight in the problem of Chinese sign language recognition.",,978-1-6654-0535-5,10.1109/ROBIO54168.2021.9739638,National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Technology Development; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739638,Chinese Sign Language Recognition;Pose Recognition;Multi-Head Attention;LSTM,Databases;Face recognition;Conferences;Pose estimation;Gesture recognition;Assistive technologies;Streaming media,computer vision;feature extraction;gesture recognition;handicapped aids;pose estimation;sign language recognition,time vision-based Chinese sign language recognition;pose estimation;attention network;deaf hearing people;great problems;research level;practical automatic sign language interpretation methods;relatively large demand;computer vision;skeletal key point information;multihead attention mechanism;short-term memory mechanism;Chinese video sign language,,,,14,IEEE,28-Mar-22,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Using YOLOv5 Algorithm to Detect and Recognize American Sign Language,T. F. Dima; M. E. Ahmed,NA; NA,2021 International Conference on Information Technology (ICIT),26-Jul-21,2021,,,603,607,"Sign language is a system of communication using visual gestures and signs, and is generally used by people who have speech or hearing difficulties. To make those people included in the verbal communication community it's important to under-stand the gesture they're using to communicate. Often people who do not use the gesture in real life could not understand what the gesture represents. In this paper, we're proposing a solution to detect the alphabet and numbers that are each gesture providing. There are already some methods proposed that use deep learning for recognizing sign language. However, the effective uses of these models are limited. We are proposing a YOLOv5 based solution as it's lightweight, fast, and has good accuracy. We use a benchmark data set (MU_HandImages_ASL) to train and evaluate the model. We achieved 95% precision, 97% recall, 98% map@0.5, and 98% map@0.5:0.95 score which is adequate to recognize the gesture in real-time.",,978-1-6654-2870-5,10.1109/ICIT52682.2021.9491672,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9491672,American Sign Language(ASL);YOLOv5;Object Recognition;Computer Vision,Deep learning;Visualization;Assistive technology;Gesture recognition;Auditory system;Benchmark testing;Real-time systems,deep learning (artificial intelligence);handicapped aids;sign language recognition,verbal communication community;visual gestures;American sign language;YOLOv5 algorithm,,9,,30,IEEE,26-Jul-21,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
"On intelligent avatar communication using Korean, Chinese and Japanese sign-languages: an overview",Sang-Woon Kim; Zhe-Xue Li; Y. Aoki,"Department of Computer Science and Engineering, Myongji University, Yongin si, South Korea; Department of Computer Science and Engineering, Myongji University, Yongin si, South Korea; Graduate School of Engineering, Hokkaido University, Sapporo, Japan","ICARCV 2004 8th Control, Automation, Robotics and Vision Conference, 2004.",25-Jul-05,2004,1,,747,752 Vol. 1,"This paper gives an overview on an intelligent avatar communication system using Korean, Chinese and Japanese sign-languages to overcome the linguistic barrier between different languages. First of all, to achieve real-time communication, an intelligent communication method based on a client-server architecture and a sign-language translation method are considered. Also, to generate realistic gesture images, CG animation techniques and emotional expression methods are reviewed. Finally, to overcome the limitation based on different avatar models, a new communication message format using a FBML (facial body markup language) is investigated. Experimental results show a possibility that the methods could be used for intelligent sign-language communications between avatars of Korean, Japanese and Chinese on the Internet.",,0-7803-8653-1,10.1109/ICARCV.2004.1468921,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1468921,,Avatars;Natural languages;Character generation;Handicapped aids;Intelligent systems;Web and internet services;Facial animation;Computer science;Image generation;Markup languages,language translation;natural language interfaces;avatars;Internet;client-server systems;computer animation;realistic images,intelligent avatar communication;Korean;Chinese;Japanese;sign-language translation;real-time communication;client-server architecture;realistic gesture images;CG animation techniques;emotional expression methods;communication message format;facial body markup language;FBML;Internet,,3,5,13,IEEE,25-Jul-05,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""Using
the FBML, we can 'define a standard document format that contains the messages to be transferred between different models. Also with the FBML, we can
synchronize easily and transmit the message comper
nents such as text messages, action units of FACS
(Facial Action Coding System) [11] and the joint angles of the arms gesture and the hands shape"" p.748.",,27
Real-time American Sign Language recognition from video using hidden Markov models,T. Starner; A. Pentland,"Perceptual Computing Section, The Media Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Perceptual Computing Section, The Media Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA",Proceedings of International Symposium on Computer Vision - ISCV,06-Aug-02,1995,,,265,270,"Hidden Markov models (HMMs) have been used prominently and successfully in speech recognition and, more recently, in handwriting recognition. Consequently, they seem ideal for visual recognition of complex, structured hand gestures such as are found in sign language. We describe a real-time HMM-based system for recognizing sentence level American Sign Language (ASL) which attains a word accuracy of 99.2% without explicitly modeling the fingers.",,0-8186-7190-4,10.1109/ISCV.1995.477012,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=477012,,Handicapped aids;Hidden Markov models;Real time systems;Fingers;Speech recognition;Shape;Natural languages;Face recognition;Laboratories;Handwriting recognition,hidden Markov models;handicapped aids;image recognition;real-time systems,American Sign Language recognition;hidden Markov models;visual recognition;hand gestures;sign language;American Sign Language;real-time;HMM-based system,,251,5,19,IEEE,06-Aug-02,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""we describe an estensible system which
uses a single color camera to track hands in real time
and interprets American Sign Language (ASL) using
Hidden Markov Models (HMM)"" p.265.",,28
Video-based continuous sign language recognition using statistical methods,B. Bauer; H. Hienz; K. . -F. Kraiss,"Department of Technical Computer Science, Aachen, Germany; Department of Technical Computer Science, Aachen, Germany; Department of Technical Computer Science, Aachen, Germany",Proceedings 15th International Conference on Pattern Recognition. ICPR-2000,06-Aug-02,2000,2,,463,466 vol.2,"This paper deals with the development of a video-based recognition system of continuous sign language. The system aims for an automatic signer dependent recognition of sign language sentences, based on a lexicon of 97 signs of German Sign Language. The recognition system is based on hidden Markov models with one model for each sign. A single video camera is utilised for data acquisition. Beamsearch is employed for the recognition task. For a better result a language model is implemented, which is able to handle a-priori knowledge of the training corpus. Different results are given for a vocabulary of 52 respectively, 97 signs with different language models (Unigram and Bigram) employed. The system achieves all accuracy of 91.8% based on a lexicon of 97 signs without a language model and 93.2% with employed Bigrams.",1051-4651,0-7695-0750-6,10.1109/ICPR.2000.906112,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=906112,,Handicapped aids;Statistical analysis;Natural languages;Hidden Markov models;Cameras;Vocabulary;Deafness;Computer science;Data acquisition;Mouth,computer vision;handicapped aids;image segmentation;hidden Markov models;real-time systems,sign language recognition;statistical grammars;German Sign Language;hidden Markov models;image segmentation;real time systems;Bigram model,,36,,8,IEEE,06-Aug-02,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Sign Language Translator Using Deep Learning Techniques,S. Krishnamurthi; I. M,"Department of Computer Science, BMSCE, Bangalore; Department of Computer Science, BMSCE, Bangalore","2021 Fourth International Conference on Electrical, Computer and Communication Technologies (ICECCT)",29-Nov-21,2021,,,1,5,"Sign language is mainly used by hearing impaired people for communication. And it has been an active research topic for the past 2 decades. Although sign language seems to be familiar in recent times, it's still quite a challenge for non-signers to communicate with signers leading to the widening of communication gap between them. Deep learning models are widely being used for motion and gesture recognition. With advancement in deep learning techniques and computer vision, this topic is gaining more attention. The main aim of the proposed model in this paper is to implement a sign language translator using deep learning techniques like Convolutional Neural Network (CNN) and help to bridge the gap between signers and non-signers. The model recognizes all the numbers[0-9], alphabets[a-z] and custom user-defined unigram symbols with better accuracy both in real time and for static sign image classification. A web application is built with the trained model to make it accessible to everyone. The front end of the web application is built with the help of a python library called gradio for simple and user friendly interface.",,978-1-6654-1480-7,10.1109/ICECCT52121.2021.9616795,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9616795,sign language recognition;pattern recognition;deep learning;neural networks;convolutional neural network,Deep learning;Webcams;Computational modeling;Neural networks;Gesture recognition;Assistive technologies;Libraries,computer vision;feature extraction;gesture recognition;handicapped aids;image classification;learning (artificial intelligence);neural nets;sign language recognition,deep learning models;deep learning techniques;sign language translator;signers;nonsigners;static sign image classification;communication gap,,1,,21,IEEE,29-Nov-21,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Evaluating the translation of speech to virtually-performed sign language on AR glasses,L. T. Nguyen; F. Schicktanz; A. Stankowski; E. Avramidis,"Technische UniversitÃ¤t, Berlin, Germany; Technische UniversitÃ¤t, Berlin, Germany; German Research Center for Artificial Intelligence (DFKI), Berlin, Germany; German Research Center for Artificial Intelligence (DFKI), Berlin, Germany",2021 13th International Conference on Quality of Multimedia Experience (QoMEX),30-Jun-21,2021,,,141,144,"This paper describes the proof-of-concept evaluation for a system that provides translation of speech to virtually performed sign language on augmented reality (AR) glasses. The discovery phase via interviews confirmed the idea for a signing avatar displayed within the users field of vision through AR glasses. In the evaluation of the first prototype through a wizard-of-Oz-experiment, the presented AR solution received a high acceptance rate among deaf and hard-of-hearing persons. However, the machine learning based method used to generate sign language from video still lacks the required accuracy for fully preserving comprehensibility. Signed sentences with large recognisable arm movements were understood better than sentences relying mainly on finger movements, where only a small interaction space is visible.",2472-7814,978-1-6654-3589-5,10.1109/QoMEX51781.2021.9465430,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9465430,sign language translation;real-time translation;augmented reality;AR glasses;avatar;inclusion,Avatars;Assistive technology;Fingers;Prototypes;Glass;Gesture recognition;Machine learning,augmented reality;avatars;computer animation;gesture recognition;handicapped aids;learning (artificial intelligence);sign language recognition;wearable computers,sign language;AR glasses;proof-of-concept evaluation;augmented reality glasses;signing avatar;signed sentences;finger movements;small interaction space;hard-of-hearing persons,,1,,12,IEEE,30-Jun-21,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
An Optimum Approach to Indian Sign Language Recognition using Efficient Convolution Neural Networks,R. Kumar; N. Bhardwaj; N. S. Kumar,"School of Computing Science and Engineering, Galgotias University, Greater Noida, India; School of Computing Science and Engineering, Galgotias University, Greater Noida, India; School of Computing Science and Engineering, Galgotias University, Greater Noida, India","2022 4th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)",28-Mar-23,2022,,,1563,1567,"The most important component of every discussion is language, and sign language is the most formal mode of communication for persons who are deaf or mute. Communication, as we all know, is one of the most important components of modern life. The importance of communication is increasing in tandem with our maturation in life. The purpose of this project is to develop a system capable of recognizing all of these signs and serving as a bridge for signers and non-signers to communicate and interpret sign meanings. Technology is rapidly evolving, and now that we have powerful systems, a wealth of data, and a diverse set of sources, we can design a system to assist people with disabilities. We will develop a user-independent framework for automatically recognizing Indian Sign Language in this project, which will allow for feature extraction, recognition, and interpretation of a variety of one-handed, dynamic isolated signals. The two most important and fundamental aspects of SLR research are isolated sign recognition and continuous sign recognition. Hand form, hand position, and so on are the primary hand traits that can be used in sign language. The extra-clung functionality can be implemented in two ways. Either one is tracking-based or it is not. This essay focuses on one such approach that we devised to overcome significant communication barriers. The technique is divided into three stages: pre-processing, texture extraction, and recognition.",,978-1-6654-7436-8,10.1109/ICAC3N56670.2022.10074173,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10074173,Sign Language using Computer Vision Techniques;Feature extraction;Recognition;Indian Sign Language;CNN for Sign Language Recognition;Machine Learning approach for Sign Language Recognition,Convolution;Neural networks;Gesture recognition;Assistive technologies;Feature extraction;Real-time systems;Smart devices,,,,,,15,IEEE,28-Mar-23,,,IEEE,IEEE Conferences,Yes ,Yes,,,,,
Real-time American sign language recognition using desk and wearable computer based video,T. Starner; J. Weaver; A. Pentland,"The Media Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; The Media Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; The Media Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA",IEEE Transactions on Pattern Analysis and Machine Intelligence,06-Aug-02,1998,20,12,1371,1375,We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American sign language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon.,1939-3539,,10.1109/34.735811,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=735811,,Handicapped aids;Wearable computers;Hidden Markov models;Cameras;Speech recognition;Face recognition;Computer vision;Computer Society;Real time systems;Pattern recognition,handicapped aids;computer vision;pattern recognition;motion estimation;hidden Markov models;real-time systems,American sign language recognition;real-time systems;hidden Markov model;computer vision;gesture recognition;motion analysis;pattern recognition,,765,42,23,IEEE,06-Aug-02,,,IEEE,IEEE Journals,Yes ,Yes,No,Yes,"""we describe two extensible systems which use one
color camera to track unadorned hands in real time and interpret
American Sign Language using hidden Markov models. "" p.1.",,29
Hand Gesture Recognition Using CAMSHIFT Algorithm,S. M. Nadgeri; S. D. Sawarkar; A. D. Gawande,"Department of Computer Engineering, MGMCET, Navi Mumbai, India; Department of Computer Engineering, DMCOE, Navi Mumbai, India; Department of IT, MGMCET, Navi Mumbai, India",2010 3rd International Conference on Emerging Trends in Engineering and Technology,31-Jan-11,2010,,,37,41,"Hand gesture is an active area of research in the computer vision, mainly for the purpose of sign language recognition and Human Computer Interaction. In this paper, an attempt is made to propose a system to recognize alphabet characters (A-Z) in real-time from color image sequences using âContinuous Adaptive Mean Shift Algorithm (CAMSHIFT)â tracking algorithm. The proposed system is based on three main stages: Skin color detection, hand tracking and hand gesture recognition. Various Experiments were performed and the results demonstrate that, the system can successfully recognize hand gestures using CAMSHIFT algorithm.",2157-0485,978-1-4244-8481-2,10.1109/ICETET.2010.63,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5698287,Hand gesture;Human machine interaction;Meanshift algorithm;Hand tracking;and CAMSHIFT algorithm,,computer vision;gesture recognition;human computer interaction;image colour analysis;image sequences;object detection,hand gesture recognition;CAMSHIFT algorithm;computer vision;sign language recognition;human computer interaction;alphabet characters recognition;color image sequences;continuous adaptive mean shift algorithm;skin color detection;hand tracking,,31,,15,IEEE,31-Jan-11,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""we describe two extensible systems which use one
color camera to track unadorned hands in real time and interpret
American Sign Language using hidden Markov models. "" p.5.",,30
Real-Time Detection and Understanding of Isolated Protruded Fingers,S. Chandran; A. Sawa,"Computer Science & Engineering Department IIT Bombay & CfAR, University of Maryland, Mumbai, India; Computer Science & Engineering Department, IIT Bombay, Mumbai, India",2004 Conference on Computer Vision and Pattern Recognition Workshop,24-Jan-05,2004,,,152,152,"Making sense of hand gestures in applications such as novel user interfaces is an active area of research. Many current methods use complex 3D model of non-rigid hands and involved paradigms in solving the general problem. In this paper we look at a subproblem of sign language alphabet recognition where gestures are made with protruded fingers. In contrast to the more intricate schemes, we propose a simple, fast classification algorithm using two dimensional projection of Euler angles. Not only is our scheme novel, it lends itself to a real time implementation.",,,10.1109/CVPR.2004.426,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1384950,Projected Euler Angles;Gesture;Sign language;Fingers;Appearance-based,Fingers;Handicapped aids;Computer vision;Computer science;Application software;User interfaces;Video sequences;Pattern recognition;Data gloves;Cameras,,Projected Euler Angles;Gesture;Sign language;Fingers;Appearance-based,,,1,,IEEE,24-Jan-05,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Sign Language Recognition by Combining Statistical DTW and Independent Classification,J. F. Lichtenauer; E. A. Hendriks; M. J. T. Reinders,"Faculty of Electrical Engineering, Mathematics, and Computer Science, Information and Communication Theory Group, Delft University of Technnology, Delft, Netherlands; Faculty of Electrical Engineering, Mathematics, and Computer Science, Information and Communication Theory Group, Delft University of Technnology, Delft, Netherlands; Faculty of Electrical Engineering, Mathematics, and Computer Science, Information and Communication Theory Group, Delft University of Technnology, Delft, Netherlands",IEEE Transactions on Pattern Analysis and Machine Intelligence,19-Sep-08,2008,30,11,2040,2046,"To recognize speech, handwriting, or sign language, many hybrid approaches have been proposed that combine dynamic time warping (DTW) or hidden Markov models (HMMs) with discriminative classifiers. However, all methods rely directly on the likelihood models of DTW/HMM. We hypothesize that time warping and classification should be separated because of conflicting likelihood modeling demands. To overcome these restrictions, we propose using statistical DTW (SDTW) only for time warping, while classifying the warped features with a different method. Two novel statistical classifiers are proposed - combined discriminative feature detectors (CDFDs) and quadratic classification on DF Fisher mapping (Q-DFFM) - both using a selection of discriminative features (DFs), and are shown to outperform HMM and SDTW. However, we have found that combining likelihoods of multiple models in a second classification stage degrades performance of the proposed classifiers, while improving performance with HMM and SDTW. A proof-of-concept experiment, combining DFFM mappings of multiple SDTW models with SDTW likelihoods, shows that, also for model-combining, hybrid classification can provide significant improvement over SDTW. Although recognition is mainly based on 3D hand motion features, these results can be expected to generalize to recognition with more detailed measurements such as hand/body pose and facial expression.",1939-3539,,10.1109/TPAMI.2008.123,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4527247,Time series analysis;Face and gesture recognition;Markov processes;Classifier design and evaluation;Real-time systems;3D/stereo scene analysis;Vision and Scene Understanding;Artificial Intelligence;Computing Methodology;Time series analysis;Face and gesture recognition;Markov processes;Classifier design and evaluation;Real-time systems;3D/stereo scene analysis;Vision and Scene Understanding;Artificial Intelligence;Computing Methodology,Handicapped aids;Hidden Markov models;Handwriting recognition;Speech recognition;Face recognition;Shape;Distortion;Computer vision;Detectors;Degradation,feature extraction;gesture recognition;image classification;image motion analysis;statistical analysis,sign language recognition;statistical DTW;independent classification;dynamic time warping;statistical classifier;combined discriminative feature detector;quadratic classification on DF Fisher mapping;discriminative feature selection;hand motion feature;gesture recognition,"Algorithms;Artificial Intelligence;Data Interpretation, Statistical;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Pattern Recognition, Automated",139,,26,IEEE,23-May-08,,,IEEE,IEEE Journals,Yes ,No,Yes,No,,,
Hand Gesture Recognition for Deaf People Interfacing,I. G. Incertis; J. G. Garcia-Bermejo; E. Z. Casanova,"CARTIF Foundation, Spain; University of Valladolid, Spain; University of Valladolid, Spain",18th International Conference on Pattern Recognition (ICPR'06),18-Sep-06,2006,2,,100,103,"In this paper, an approach for deaf-people interfacing using computer vision is presented. The recognition of alphabetic static signs of the Spanish Sign Language is addressed. The proposed approach combines a number of norms to evaluate the distance of the current sign, to the sign models stored in a database (a dictionary). This solution leads to a largely selective criterion. The method is simple enough to provide real-time recognition, and works suitably for most letters.",1051-4651,0-7695-2521-0,10.1109/ICPR.2006.619,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1699157,,Deafness;Handicapped aids;Pattern recognition;Dictionaries;Fingers;Histograms;Computer vision;Databases;Man machine systems;Graphics,computer vision;gesture recognition;handicapped aids;human computer interaction,hand gesture recognition;deaf people interfacing;computer vision;alphabetic static signs;Spanish sign language;dictionary;real-time recognition,,17,,17,IEEE,18-Sep-06,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Extraction of 3D hand shape and posture from image sequences for sign language recognition,H. Fillbrandt; S. Akyol; K. . -F. Kraiss,"Chair of Technical Computer Science, RWTH Aachen University of Technology, Germany; Chair of Technical Computer Science, RWTH Aachen University of Technology, Germany; Chair of Technical Computer Science, RWTH Aachen University of Technology, Germany",2003 IEEE International SOI Conference. Proceedings (Cat. No.03CH37443),27-Oct-03,2003,,,181,186,"We propose a novel method for extracting natural hand parameters from monocular image sequences. The purpose is to improve a vision-based sign language recognition system by providing detail information about the finger constellation and the 3D hand posture. Therefore, the hand is modelled by a set of 2D appearance models, each representing a limited variation range of 3D hand shape and posture. The single models are linked to each other according to the natural neighbourhood of the corresponding hand status. During an image sequence, necessary model transitions are executed towards one of the current neighbour models. The natural hand parameters are calculated from the shape and texture parameters of the current model, using a relation estimated by linear regression. The method is robust against large differences between subsequent frames and also against poor image quality. It can be implemented in real-time and offers good properties to handle occlusion and partly missing image information.",,0-7695-2010-3,10.1109/AMFG.2003.1240841,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240841,,Shape;Image sequences;Handicapped aids;Image recognition;Data mining;Fingers;Linear regression;Robustness;Image quality,image sequences;image texture;gesture recognition;feature extraction;regression analysis;computer vision;hidden feature removal,3D hand shape extraction;3D hand posture;monocular image sequences;vision-based sign language recognition system;finger constellation;2D appearance model transitions;natural hand parameters;linear regression;image quality;occlusion handling;image information,,24,,16,IEEE,27-Oct-03,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""The method presented in this paper has been tested with several specially recorded images sequences of a single hand the deforms,rotates and moves in 3D space ,covering all the appearances of the hand that are included in the used model""",,31
A framework for the integration of gesture and posture recognition using HMM and SVM,O. Rashid; A. Al-Hamadi; B. Michaelis,"Institute for Electronics, Signal Processing and Communications IESK, Otto von Guericke University Magdeburg, Magdeburg, Germany; Institute for Electronics, Signal Processing and Communications IESK, Otto von Guericke University Magdeburg, Magdeburg, Germany; Institute for Electronics, Signal Processing and Communications IESK, Otto von Guericke University Magdeburg, Magdeburg, Germany",2009 IEEE International Conference on Intelligent Computing and Intelligent Systems,28-Dec-09,2009,4,,572,577,"For a successful real-time vision-based HCI system, inference from natural visual method is crucial. In this paper, we have aimed to provide interaction through gesture and posture recognition for alphabets and numbers. In addition, data fusion is carried out which integrates these systems to extract multiple meanings at the same time. 3D information is exploited for segmentation and detection of face and hands using normal Gaussian distribution and depth information. For gesture, orientation of two consecutive hand centroid points is computed which is then quantized to generate code words. HMM is trained by Baum Welch algorithm and classified by Viterbi path algorithm. In posture recognition, American Sign Language is recognized for static alphabets and numbers. Feature vectors are computed from statistical and geometrical properties of the hand and are used to train SVM for classification and recognition. Moreover, curvature analysis is carried out for alphabets to avoid misclassifications. Experimental results of the proposed framework successfully integrate both gesture and posture recognition system at decision level fusion whereas the gesture system achieves recognition rate of 98% (i.e. for alphabets and numbers) and the posture recognition system with recognition rates of 98.65% and 98.6% for ASL alphabets and numbers respectively.",,978-1-4244-4754-1,10.1109/ICICISYS.2009.5357615,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5357615,Integration;Gesture Recognition;Posture Recognition;Feature Extraction;Application,Hidden Markov models;Support vector machines;Face detection;Real time systems;Human computer interaction;Data mining;Gaussian distribution;Viterbi algorithm;Handicapped aids;Support vector machine classification,face recognition;feature extraction;Gaussian distribution;gesture recognition;hidden Markov models;human computer interaction;normal distribution;sensor fusion;support vector machines,gesture recognition;posture recognition;hidden Markov model;support vector machine;real-time vision-based HCI system;human computer interaction;data fusion;face detection;face segmentation;normal Gaussian distribution;Baum Welch algorithm;Viterbi path algorithm;American Sign Language,,25,,13,IEEE,28-Dec-09,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Real-time Dynamic Sign Recognition using MediaPipe,Y. Farhan; A. Ait Madi,"Advanced Systems Engineering Laboratory National Schools of Applied Sciences, Ibn Tofail University, Kenitra, Morocco; Advanced Systems Engineering Laboratory National Schools of Applied Sciences, Ibn Tofail University, Kenitra, Morocco","2022 IEEE 3rd International Conference on Electronics, Control, Optimization and Computer Science (ICECOCS)",21-Dec-22,2022,,,1,7,"People live in societies, so communication is necessary to express their needs and ideas. Deaf-mute people use sign language as their means of communication, but most people are unaware of its meaning, so creating an effective system to help them communicate is important. This paper proposes an American Sign Language (ASL) recognition system for 12 dynamic signs in real-time using the MediaPipe framework and a Long Short-Term Memory (LSTM) network. To improve system performance, only the relevant features were extracted from the dynamic sign video, and two new useful features were generated (angles and distances). The proposed system achieved a test accuracy of 97,2%.",,978-1-6654-5723-1,10.1109/ICECOCS55148.2022.9982822,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982822,American Sign Language;dynamic signs;MediaPipe;LSTM;computer vision,System performance;Gesture recognition;Assistive technologies;Streaming media;Feature extraction;Real-time systems;Skin,feature extraction;handicapped aids;recurrent neural nets;sign language recognition,American sign language recognition system;ASL;deaf-mute people;dynamic sign recognition;dynamic sign video;long short-term memory network;LSTM network;mediapipe framework,,,,20,IEEE,21-Dec-22,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Real-time sign language letter and word recognition from depth data,D. Uebersax; J. Gall; M. Van den Bergh; L. Van Gool,"BIWI, ETH Zurich, Switzerland; BIWI, ETH Zurich, Switzerland; BIWI, ETH Zurich, Switzerland; BIWI, ETH Zurich, Switzerland",2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops),16-Jan-12,2011,,,383,390,"In this work, we present a system for recognizing letters and finger-spelled words of the American sign language (ASL) in real-time. To this end, the system segments the hand and estimates the hand orientation from captured depth data. The letter classification is based on average neighborhood margin maximization and relies on the segmented depth data of the hands. For word recognition, the letter confidences are aggregated. Furthermore, the word recognition is used to improve the letter recognition by updating the training examples of the letter classifiers on-line.",,978-1-4673-0063-6,10.1109/ICCVW.2011.6130267,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130267,,Handicapped aids;Real time systems;Accuracy;Image segmentation;Training data;Training;Vectors,gesture recognition;image classification;optimisation,real time sign language letter recognition;word recognition;depth data;finger spelled word;American sign language;hand segmentation;hand orientation;captured depth data;average neighborhood margin maximization;letter classifiers;training examples,,49,,28,IEEE,16-Jan-12,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
System based on machine vision for translation of fingerspelling alphabet to latin alphabet,D. Lancheros-Cuesta; C. S. Schlenker; L. F. Morales,"Ingenieria en AutomatizaciÃ³n, Universidad de La Salle, Bogota, Colombia; Ingenieria en AutomatizaciÃ³n, Universidad de La Salle, Bogota, Colombia; Universidad de La Salle, Bogota, CO",2015 10th Iberian Conference on Information Systems and Technologies (CISTI),30-Jul-15,2015,,,1,5,"On the nowadays society exist a lot of communication problems, particularly when the persons has sensory disabilities as deafness or blindness. This problem take place at the moment of interpreting the sign language. The present paper shows the development of a current research project that integrates an intelligent system in the recognition of images and its reproduction in hardware interpretations ends. For those purposes, a system of image acquisition with a webcam and an interface was implemented in Matlab, through which the video was displayed in real time, the image of the point gained, together with the translation of Colombian sign language. This system was trained to recognize 4-letter alphabet, obtaining an average error of 2%, concluding that such application was effective for translating letters acquired both the right hand or the left; similarly concluded that the type of radial neural network proves to be very useful for this type of operation and the higher classification have this training, the results will be more accurate cast. Finally it is important to note that the system integrates hardware with Arduino system that displays real-time translation in this system was trained to recognize 4 letters of the alphabet, giving an average error of 2%, the same way the PNN neural network proves to be very useful for this type of sorting operations.",2166-0727,978-9-8998-4345-5,10.1109/CISTI.2015.7170450,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7170450,Neuronal Networks;Artificial Vision;Smart Recognition;Image Treatment,MATLAB;Biological neural networks;Training;Assistive technology;Image recognition;Gesture recognition,computer vision;handicapped aids;image sensors;language translation;neural nets;sign language recognition,machine vision;fingerspelling alphabet translation;Latin alphabet;communication problems;sensory disabilities;deafness;blindness;images recognition;hardware interpretations;image acquisition;webcam;Matlab;Colombian sign language translation;4-letter alphabet;radial neural network;Arduino system;real-time translation;PNN neural network;sorting operations,,,,6,,30-Jul-15,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Real-time disparity computation and 3D reprojection for hand gesture recognition,M. A. Laskar; A. J. Das; A. K. Talukdar; K. K. Sarma,"Department of ECE, Gauhati University, Guwahati, India; Department of ECE, Gauhati University, Guwahati, India; Department of ECE, Gauhati University, Guwahati, India; Department of ECT, Gauhati University, Guwahati, India",2015 International Symposium on Advanced Computing and Communication (ISACC),11-Jan-16,2015,,,78,83,"Human hand gestures as a natural way of interaction and communicating with computers is becoming an emerging and demanding field of research due to its various applications like sign language recognition, human computer interaction, gaming, virtual reality etc. Hand gesture recognition under 2D environment has some limitations as the information about the other dimension (z-axis) is missed. So, hand gesture recognition under 3D environment is becoming a growing field of research. In this paper, we have implemented the realtime disparity computation and proposed a novel technique to detect gesture of Front and Back along with forward and backward movement towards and from the camera respectively. Our technique is based on stereo-vision and we have used the disparity map-based intensity measure of segmented hand and changing of its intensity as feature to classify and recognize the gesture. Stereo calibration followed by rectification is done to get the rectified images and correspondence gives the depth map. Finally three dimensional reprojection is performed. Our technique works well for the gestures and the results are promising.",,978-1-4673-6708-0,10.1109/ISACC.2015.7377319,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7377319,Stereo vision;stereo calibration;stereo matching;disparity map;reprojection,Image segmentation;Three-dimensional displays;Face;Image color analysis,gesture recognition;palmprint recognition;stereo image processing,real-time disparity computation;3D reprojection;hand gesture recognition;2D environment;stereo-vision;disparity map-based intensity measure;stereo calibration;three dimensional reprojection,,,,19,IEEE,11-Jan-16,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""The main
objective of our work is to implement a resource limited 3D
hand gesture recognition system to classify the Front and Back
gestures along with forward and backward movement towards
and from the camera respectively by applying stereo vision
techniques with the help of a inexpensive stereo camera."" p.1.",,32
Depth-Based Hand Pose Segmentation with Hough Random Forest,W. -J. Tsai; J. -C. Chen; K. W. Lin,"Department of Computer Science and Information Engineering, National Kaohsiung University of Applied Sciences, Kaohsiung, Taiwan; Department of Computer Science and Information Engineering, National Kaohsiung University of Applied Sciences, Kaohsiung, Taiwan; Department of Computer Science and Information Engineering, National Kaohsiung University of Applied Sciences, Kaohsiung, Taiwan",2016 3rd International Conference on Green Technology and Sustainable Development (GTSD),26-Dec-16,2016,,,166,167,"With the development of image processing and computer vision technology, using gesture to communicate with the machine will not only appear in scientific move or just a conceptual product. Gesture recognition is a topic in computer science and language technology with the goal of interpreting human gestures via mathematical algorithms. With this, we can have a more convenient life. Therefore, our goal is using image processing algorithm to effectively recognize the correct gesture from camera and present a user-friendly human-machine interface. In recent years, gesture recognition has become a popular and important issue. It can be used for robot control, appliances control, gaming control, etc. We present an algorithm which is able to correctly calculate the accurate finger joint position and then evaluate the gesture. This algorithm is divided into two parts: hand position detection and gesture recognition. In gesture detection, we capture the depth image from depth camera to solve the problem caused by illumination and background. In gesture recognition, we use an object recognizing algorithm to make the difficult evaluating finger movement problem corresponds to an easier voting classify problem. Depth camera helps our classifier avoid the illumination problem from incorrectly recognizing object.",,978-1-5090-3638-7,10.1109/GTSD.2016.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7796640,Hough forest;Hand Segmentation,Training;Computer vision;Vegetation;Conferences;Pattern recognition;Pose estimation;Real-time systems,cameras;gesture recognition;Hough transforms;human computer interaction;image classification;image motion analysis;image segmentation;object detection;object recognition;pose estimation,classifier;voting classify problem;finger movement;object recognition;illumination;depth camera;depth image;gesture detection;hand position detection;finger joint position;user-friendly human-machine interface;mathematical algorithms;human gestures interpretation;gesture recognition;computer vision technology;image processing;Hough random forest;depth-based hand pose segmentation,,,,9,IEEE,26-Dec-16,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
A Framework for Real-time Traffic Sign Detection and Recognition using Grassmann Manifolds,A. Gupta; A. Choudhary,"School of Computer and Systems Sciences, Jawaharlal Nehru University, New Delhi, India; School of Computer and Systems Sciences, Jawaharlal Nehru University, New Delhi, India",2018 21st International Conference on Intelligent Transportation Systems (ITSC),09-Dec-18,2018,,,274,279,"We propose a novel, real-time framework for traffic sign detection and recognition using a camera mounted on the dashboard of a moving vehicle. Traffic sign detection and recognition plays an important role in Advanced Driver Assistance Systems (ADAS) as it helps increase driving safety. However, it is very challenging to detect and recognize traffic signs because of challenges such as perspective distortion, illumination variation, occlusion and motion blur from moving vehicle. In our framework, we use Hue Saturation Value (HSV) color filtering for traffic sign detection. In our Grassmann manifold based traffic sign recognition framework, we create subspaces of each unique traffic sign. These subspaces accommodate the uncertainties that occur during detection and variations in different instances of the same traffic sign. These subspaces lie on a Grassmann manifold and we use discriminant analysis on Grassmann manifolds for recognising them. We have carried out extensive experiments on multiple publicly available traffic sign datasets and compared our proposed framework with multiple state-of-the-art methods. Experimental results show that our system is robust and has a high degree of accuracy.",2153-0017,978-1-7281-0323-5,10.1109/ITSC.2018.8569556,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8569556,Traffic sign detection and recognition;intelligent vehicles;machine learning;computer vision,Image color analysis;Manifolds;Real-time systems;Cameras;Lighting;Colored noise;Feature extraction,driver information systems;image colour analysis;object detection;object recognition;road safety;sign language recognition;traffic engineering computing,real-time framework;moving vehicle;Grassmann manifold based traffic sign recognition framework;unique traffic sign;multiple publicly available traffic sign datasets;real-time traffic sign detection;advanced driver assistance systems,,4,,21,IEEE,09-Dec-18,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Gestural teleoperation of a mobile robot based on visual recognition of sign language static handshapes,C. Tzafestas; N. Mitsou; N. Georgakarakos; O. Diamanti; P. Maragos; S. . -E. Fotinea; E. Efthimiou,"School of Electrical and Computer Engineering, National and Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National and Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National and Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National and Technical University of Athens, Athens, Greece; School of Electrical and Computer Engineering, National and Technical University of Athens, Athens, Greece; Institute of Language and Speech Processing, ILSP-ATHENA R. C., Marousi, Greece; Institute of Language and Speech Processing, ILSP-ATHENA R. C., Marousi, Greece",RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication,10-Nov-09,2009,,,1073,1079,"This paper presents results achieved in the frames of a national research project (titled ldquoDIANOEMArdquo), where visual analysis and sign recognition techniques have been explored on Greek Sign Language (GSL) data. Besides GSL modelling, the aim was to develop a pilot application for teleoperating a mobile robot using natural hand signs. A small vocabulary of hand signs has been designed to enable desktopbased teleoperation at a high-level of supervisory telerobotic control. Real-time visual recognition of the hand images is performed by training a multi-layer perceptron (MLP) neural network. Various shape descriptors of the segmented hand posture images have been explored as inputs to the MLP network. These include Fourier shape descriptors on the contour of the segmented hand sign images, moments, compactness, eccentricity, and histogram of the curvature. We have examined which of these shape descriptors are best suited for real-time recognition of hand signs, in relation to the number and choice of hand postures, in order to achieve maximum recognition performance. The hand-sign recognizer has been integrated in a graphical user interface, and has been implemented with success on a pilot application for real-time desktop-based gestural teleoperation of a mobile robot vehicle.",1944-9437,978-1-4244-5081-7,10.1109/ROMAN.2009.5326235,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5326235,,Mobile robots;Handicapped aids;Shape;Image segmentation;Vocabulary;Telerobotics;Image recognition;Multilayer perceptrons;Neural networks;Multi-layer neural network,gesture recognition;graphical user interfaces;image segmentation;mobile robots;multilayer perceptrons;robot vision;telerobotics,gestural teleoperation;mobile robot;visual recognition;sign language static handshapes;greek sign language;multi-layer perceptron;neural network;sign recognition techniques;various shape descriptors;segmented hand posture images;hand signs;graphical user interface;Fourier shape descriptors;telerobotic control,,1,,21,IEEE,10-Nov-09,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Visual human machine interface by gestures,M. Frigola; J. Fernandez; J. Aranda,"Automatic Control & Computer Engineering Dpt, Universitat PoliltÃ¨cnica de Catalunya, Barcelona, Spain; Automatic Control & Computer Engineering Dpt, Universitat PoliltÃ¨cnica de Catalunya, Barcelona, Spain; Automatic Control & Computer Engineering Dpt, Universitat PoliltÃ¨cnica de Catalunya, Barcelona, Spain",2003 IEEE International Conference on Robotics and Automation (Cat. No.03CH37422),10-Nov-03,2003,1,,386,391 vol.1,"Like oral communication, gestures are a natural way to carry out human machine interface. In the early days of robotic systems, human gesture was used to control robot movements by means of a master-slave structure. In spite of the use if robot programming languages, manual control is the most reliable way to carry out complex tasks in unstructured environments. In these situations, a non-contact, passive and remote system can be helpful to control a teleoperated robot by means of human gestures. In this paper, a vision system able to detect, locate and track the head and hands of a human body is presented. The system uses several calibrated cameras placed around the operator scenario to locate the body parts of a person in 3D. The system combines different computer vision techniques to increase the reliability of the body parts detection: image movement detection, user skin colour segmentation and stereo. The data provided by these modules are focused looking for coherence according to the human body dimensions. With the scheme proposed it is possible to obtain a low-cost real-time system for human computer interfacing based in a natural way of communication (gestures). Civil area such as big robots in shipyards, mines, public works or cranes is some possible applications.",1050-4729,0-7803-7736-2,10.1109/ROBOT.2003.1241626,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1241626,,Robot control;Control systems;Oral communication;Communication system control;Master-slave;Robot programming;Human factors;Machine vision;Magnetic heads;Robot vision systems,motion control;robot programming;telerobotics;mobile robots;robot vision;stereo image processing;image segmentation;real-time systems;sensor fusion;gesture recognition,visual human machine interface;human gestures;vision system;image movement detection;user skin colour segmentation;human body dimensions;real-time system;teleoperated robot;hand detection;head detection;data fusion;3D model validation;stereo information,,10,,9,IEEE,10-Nov-03,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Gesture-based interaction and communication: automated classification of hand gesture contours,L. Gupta; Suwei Ma,"Department of Electrical Engineering, Southem Illinois University, Carbondale, Carbondale, IL, USA; Department of Electrical Engineering, Southem Illinois University, Carbondale, Carbondale, IL, USA","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",07-Aug-02,2001,31,1,114,120,"The accurate classification of hand gestures is crucial in the development of novel hand gesture-based systems designed for human-computer interaction (HCI) and for human alternative and augmentative communication (HAAC). A complete vision-based system, consisting of hand gesture acquisition, segmentation, filtering, representation and classification, is developed to robustly classify hand gestures. The algorithms in the subsystems are formulated or selected to optimality classify hand gestures. The gray-scale image of a hand gesture is segmented using a histogram thresholding algorithm. A morphological filtering approach is designed to effectively remove background and object noise in the segmented image. The contour of a gesture is represented by a localized contour sequence whose samples are the perpendicular distances between the contour pixels and the chord connecting the end-points of a window centered on the contour pixels. Gesture similarity is determined by measuring the similarity between the localized contour sequences of the gestures. Linear alignment and nonlinear alignment are developed to measure the similarity between the localized contour sequences. Experiments and evaluations on a subset of American Sign Language (ASL) hand gestures show that, by using nonlinear alignment, no gestures are misclassified by the system. Additionally, it is also estimated that real-time gesture classification is possible through the use of a high-speed PC, high-speed digital signal processing chips and code optimization.",1558-2442,,10.1109/5326.923274,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=923274,,Image segmentation;Filtering;Signal processing algorithms;Human computer interaction;Noise robustness;Gray-scale;Histograms;Background noise;Joining processes;Handicapped aids,gesture recognition;handicapped aids;interactive systems;computer vision;image classification;image segmentation;image representation;spatial filters,gesture-based interaction;gesture-based communication;automated classification;hand gesture contours;human-computer interaction;alternative communication devices;augmentative communication;vision-based system;hand gesture acquisition;image segmentation;filtering;image representation;gray-scale image;histogram thresholding algorithm;morphological filtering approach;background noise removal;object noise removal;samples;perpendicular distances;contour pixels;window end-points;gesture similarity determination;localized contour sequences;linear alignment;nonlinear alignment;American Sign Language;real-time gesture classification;high-speed PC;high-speed digital signal processing chips;code optimization,,76,1,13,IEEE,07-Aug-02,,,IEEE,IEEE Journals,No,No,Yes,No,,,
Gesture recognition matching based on dynamic skeleton,W. Jingyao; Y. Naigong; E. Firdaous,"Beijing Key Lab of the Computational Intelligence and Intelligent System, Beijing University of Technology, Beijing; Beijing Key Lab of the Computational Intelligence and Intelligent System, Beijing University of Technology, Beijing; Beijing Key Lab of the Computational Intelligence and Intelligent System, Beijing University of Technology, Beijing",2021 33rd Chinese Control and Decision Conference (CCDC),30-Nov-21,2021,,,1680,1685,"Gestures, as a basic human feature, occupy an important position in human-computer interaction and other fields as well. In order to accurately recognize gestures and eliminate environmental interference, this paper proposes a gesture recognition matching method based on dynamic bones. This method uses the Mask R-CNN model and exponential filtering to identify and calibrate the key points of the hand. Through the segmentation and feature extraction of real-time frame images, the combined network is used to obtain stable and accurate hand bone key points. Based on the idea of constructing a Spatio-temporal graph convolutional network model based on ST-GCN, a skeletal point information gesture data set is constructed and sent to the network for training. Finally, template matching is used to realize gesture recognition. The experimental results show that the method can eliminate the environmental interference to the greatest extent, as well as the incomplete traditional data set, and the model accuracy defects caused by the lack of special samples. The recognition accuracy of the Chinese sign language database can reach 87.02%. Compared with previous researches on gesture recognition, it has improved accuracy and robustness.",1948-9447,978-1-6654-4089-9,10.1109/CCDC52312.2021.9601572,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9601572,Computer Vision;Gesture Recognition;ST-GCN;Machine Learning;Mask R-CNN,Training;Technological innovation;Filtering;Gesture recognition;Interference;Bones;Data models,bone;feature extraction;gesture recognition;image matching;image segmentation,gesture recognition;dynamic skeleton;basic human feature;human-computer interaction;environmental interference;dynamic bones;Mask R-CNN model;segmentation;feature extraction;stable hand bone key points;accurate hand bone key points;Spatio-temporal graph convolutional network model;skeletal point information gesture data set,,1,,29,IEEE,30-Nov-21,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Dynamic Color Recognition for Video Game Controller,A. Saner; A. Sharma; A. Patil; A. Soni; P. More,"Dept. of Computer Science and Engineering, MIT ADT University, Pune, MH, India; Dept. of Computer Science and Engineering, MIT ADT University, Pune, MH, India; Dept. of Computer Science and Engineering, MIT ADT University, Pune, MH, India; Dept. of Computer Science and Engineering, MIT ADT University, Pune, MH, India; Dept. of Computer Science and Engineering, MIT ADT University, Pune, MH, India","2021 International Conference on Computing, Communication and Green Engineering (CCGE)",24-May-22,2021,,,1,3,"In this paper, we have introduce a real-time color recognition framework using openCv and PyObjC API. Hand Gestures with color detection are a natural form of communication between humans as well as between humans and machines. They are also useful in fields like sign language recognition, human-machine interface, and behavior understanding. The focus of creating colors recognition to create better communication between humans and computers for conveying information. The proposed project aims to create a video game controller wherein we can use our color detection and play the video Game on the computer. Several combinations of hand motions can be detected using deep learning and neural networks, rather than simple image processing and machine learning.",,978-1-6654-1509-5,10.1109/CCGE50943.2021.9776403,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9776403,Machine Learning;Computer vision;Game;Deep Learning,Computers;Human computer interaction;Image color analysis;Webcams;Neural networks;Games;Gesture recognition,computer games;deep learning (artificial intelligence);feature extraction;gesture recognition;human computer interaction;image colour analysis,dynamic color recognition;video game controller;real-time color recognition framework;PyObjC API;hand gestures;color detection;natural form;sign language recognition;human-machine interface;behavior understanding;colors recognition;hand motions;image processing;machine learning;openCv,,1,,6,IEEE,24-May-22,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Gesture Recognition based on Deep Convolutional Neural Network,P. Jayanthi; P. R. K. Sathia Bhama,"Department of Information Technology, Anna University, Chennai, India; Department of Computer Technology, Anna University, Chennai, India",2018 Tenth International Conference on Advanced Computing (ICoAC),23-Dec-19,2018,,,367,372,"A sign language is visually conveyed signal patterns to pass on meaning by concurrently coalescing hand shapes and direction and arms movement, body movement, facial expressions to articulate gracefully a narrator's feelings. Movement of the hand called hand gesture is the part of sign used by hearing-impaired inhabitants. It plays a key role for finger spelling in which signers spell out a word as a chain of hand shapes or hand flight equivalent to individual letters. Hand gesture recognition research classified as Glove based and vision based; Glove based system affects the natural signing ability. Vision based category is not using any devices or gloves, is the most natural way. Data acquisition can be done with camera. Conventional method of gesture recognition is time consuming so the usage of deep CNN is to prevent the segmentation or detection stage which favors faster real-time process.",,978-1-7281-0353-2,10.1109/ICoAC44903.2018.8939060,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8939060,Sign Language Recognition (SLR);Segmentation;Feature Extraction;Gesture Recognition;Cyber Glove;Hidden Morkov Model (HMM);Artificial Neural Network (ANN);deep Convoluting Neural Network (CNN),Gesture recognition;Assistive technology;Image color analysis;Feature extraction;Colored noise;Shape;Image segmentation,cameras;convolutional neural nets;data acquisition;data gloves;emotion recognition;face recognition;handicapped aids;human computer interaction;image segmentation;learning (artificial intelligence);sign language recognition,deep convolutional neural network;sign language;hand shapes;body movement;facial expressions;hearing-impaired inhabitants;finger spelling;signers spell;hand flight equivalent;hand gesture recognition research;natural signing ability;vision based category;deep CNN;glove based system;narrator feelings;hand direction;arms movement;data acquisition,,,,37,IEEE,23-Dec-19,,,IEEE,IEEE Conferences,Yes ,Yes,No,Yes,"""the idea is to design a network capable of
recognizing the hand signs without localizing the hand areas
with a good accuracy rate in a more efficient manner by
preventing the recognition and extraction phases instead
performing both the task with a single network using deep
CNN. "" p.371.",,33
An active boosting-based learning framework for real-time hand detection,Thuy Thi Nguyen; Nguyen Dang Binh; H. Bischof,"Institute for Computer Graphics and Vision, Graz University of Technology, Graz, Austria; Institute for Computer Graphics and Vision, Graz University of Technology, Graz, Austria; Institute for Computer Graphics and Vision, Graz University of Technology, Graz, Austria",2008 8th IEEE International Conference on Automatic Face & Gesture Recognition,10-Apr-09,2008,,,1,6,"Human hand detection problem has important applications in sign language and human machine interfaces. In this work, we present a novel approach for learning a vision-based hand detection system. The main contribution is a robust on-line boosting-based framework for real-time detection of a hand in unconstrained environments. The use of efficient representative features allows fast computation while dealing with vast changing of hand appearances and background. Interactive on-line training allows efficiently train and improve the detector. Moreover, we propose a strategy to efficiently improve the performance meanwhile reduce hand labeling effort. Besides, if necessary, we use a verification process to prevent âdriftingâ of classifier over time. The proposed method is practically favorable as it meets the requirements of real-time performance, accuracy and robustness. It works well with reasonable amount of training samples and is computational efficient. Experiments for detection of hands in challenging data sets show the outperform of our approach.",,978-1-4244-2153-4,10.1109/AFGR.2008.4813315,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4813315,,Boosting;Detectors;Computer vision;Humans;Labeling;Face detection;Application software;Handicapped aids;Robustness;Computational efficiency,image recognition;learning (artificial intelligence),active boosting-based learning framework;real-time hand detection;human hand detection problem;sign language;human machine interfaces;vision-based hand detection system;robust online boosting-based framework;hand labeling,,3,1,28,IEEE,10-Apr-09,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
LDCRFs-based hand gesture recognition,M. Elmezain; A. Al-Hamadi,"Computer Science Department, Faculty of Science, Tanta University, Egypt; Institute for Electronics, Signal Processing and Communications, Otto-von-Guericke-University Magdeburg, Germany","2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",13-Dec-12,2012,,,2670,2675,"This paper proposes a system to recognize isolated American Sign Language and numbers in real-time from Bumblebee stereo camera using Latent-Dynamic Conditional Random Fields (LDCRFs). Our system is based on three main stages: preprocessing, feature extraction and classification. In preprocessing stage, color and 3D depth map are used to detect and track the hand. The second stage, combining features of location, orientation and velocity with respected to Polar systems are used. The depth information is to identify the region of interest and consequently reduces the cost of searching and increases the processing speed. In the final stage, the hand gesture path is recognized using LDCRFs, which are more restricted to the number of hidden states owned by each class label to make training and inferencing processes tractable. Experimental results demonstrate that, our system can successfully recognize gestures with 96.14% recognition rate. Such results have the potential to compare very favorably to those of other investigators published in the literature.",1062-922X,978-1-4673-1714-6,10.1109/ICSMC.2012.6378150,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6378150,Gesture recognition;Pattern recognition;Computer vision;Human computer interaction,Gesture recognition;Hidden Markov models;Feature extraction;Skin;Training;Vectors;Handicapped aids,cameras;feature extraction;human computer interaction;image classification;image colour analysis;inference mechanisms;object tracking;sign language recognition;stereo image processing,LDCRF-based hand gesture recognition rate;isolated American sign language recognition;number recognition;Bumblebee stereo camera;latent-dynamic conditional random fields;preprocessing stage;feature extraction stage;classification stage;color information;3D depth map;hand tracking;hand detection;location feature;orientation feature;velocity feature;polar systems;region-of-interest identification;hand gesture path;class label;inference;training processes,,1,,14,IEEE,13-Dec-12,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Hand Gesture Recognition Using Densely Connected Deep Residual Network and Channel Attention Module for Mobile Robot Control,J. P. Sahoo; S. P. Sahoo; S. Ari; S. K. Patra,"Department of Electronics and Communication Engineering, National Institute of Technology Rourkela, Odisha, Rourkela, India; School of Electronics Engineering (SENSE), Vellore Institute of Technology, Tamil Nadu, Vellore, India; Department of Electronics and Communication Engineering, National Institute of Technology Rourkela, Odisha, Rourkela, India; Department of Electronics and Communication Engineering, Indian Institute of Information Technology, Vadodara, Gandhinagar, Gujarat, India",IEEE Transactions on Instrumentation and Measurement,03-Mar-23,2023,72,,1,11,"Navigating a mobile robot in an indoor or outdoor environment is an interesting research area for humanârobot collaboration (HRC). In such a scenario, hand gesture offers some unique abilities for HRC to provide nonverbal communication between the user and the robot. This article proposes a novel real-time hand gesture recognition (HGR) technique for mobile robot control application. A compact convolutional neural network (CNN)-based HGR system, denoted as densely connected residual channel attention module (DRCAM), is proposed to recognize the vision-based hand gestures effectively. Since fingers are the most vital sign for HGR, an attention mechanism using multiscale representation is proposed, which emphasizes finger information more effectively. The proposed CNN model employs the cascading structure of residual blocks with a multiscale channel attention module to learn low- to high-level information of hand gestures. In addition, the cascading structures are connected through dense connectivity, which strengthens the feature propagation and facilitates feature reuse. Experiments are conducted on an ingenuously developed dataset and a publicly available American sign language finger-spelling (ASL-FS) benchmarked dataset to evaluate the performance of the proposed technique. The experimental result illustrates that the proposed DRCAM network outperforms the state-of-the-art methods in terms of mean accuracy (MA) using the leave-one-subject-out cross-validation (LOO CV) test. Finally, the training model is used to develop a software-based user interface system for the control of a mobile robot in a real-time environment.",1557-9662,,10.1109/TIM.2023.3246488,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054153,Convolutional neural network (CNN);dense connectivity;densely connected residual channel attention module (DRCAM);depth sensor;hand gesture recognition (HGR);human-robot collaboration (HRC);mobile robot control;multiscale attention,Convolutional neural networks;Gesture recognition;Robots;Mobile robots;Three-dimensional displays;Computer architecture;Cameras,convolutional neural nets;gesture recognition;human-robot interaction;learning (artificial intelligence);mobile robots,ASL-FS benchmarked dataset;cascading structure;CNN-based HGR system;compact convolutional neural network-based HGR system;densely connected deep residual network;densely connected residual channel attention module;DRCAM network;finger information;high-level information;HRC;human-robot collaboration;indoor environment;mobile robot control application;multiscale channel attention module;multiscale representation;nonverbal communication;outdoor environment;publicly available American sign language finger-spelling;real-time environment;real-time hand gesture recognition technique;residual blocks;software-based user interface system;vision-based hand gestures,,,,33,IEEE,27-Feb-23,,,IEEE,IEEE Journals,Yes ,No,Yes,No,,,
Machine Learning for Real Time Poses Classification Using Kinect Skeleton Data,Y. Choubik; A. Mahmoudi,"Faculty of Sciences, Mohammed V University, Rabat, Morocco; LIMIARF-OSST Laboratory and Ecole Normale Superieure, Mohammed V University, Rabat, Morocco","2016 13th International Conference on Computer Graphics, Imaging and Visualization (CGiV)",12-May-16,2016,,,307,311,"Poses recognition is an important research topic because some situations require silent communication (sign language, surgeon poses to the nurse for assistance etc.). Traditionally, poses recognition requires high quality expensive cameras and complicated computer vision algorithms. This is not the case thanks to the Microsoft Kinect sensor which provides an inexpensive and easy way for real time user interaction. In this paper, we proposed a real time human poses classification technique, by using skeleton data provided by the Kinect sensor. Different users performed a set of tasks from a vocabulary of eighteen poses. From skeleton data of each pose, twenty features are extracted so that they are invariant with respect to the user's size and its position in the scene. We then compared the generalization performances of four machine learning algorithms, support vectors machines (SVM), artificial neural networks (ANN), k-nearest neighbors (KNN) and Bayes classifier (BC). The method used in this work shows that SVM outperforms the other algorithms.",,978-1-5090-0811-7,10.1109/CGiV.2016.66,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7467728,Poses classification;Kinect;SVM;Cross-validation,Support vector machines;Training data;Kernel;Skeleton;Feature extraction;Artificial neural networks;Prediction algorithms,Bayes methods;computer vision;feature extraction;human computer interaction;image classification;image sensors;learning (artificial intelligence);neural nets;object recognition;pose estimation;support vector machines;user interfaces,machine learning algorithms;real time human poses classification technique;Kinect skeleton data;poses recognition;sign language;high quality expensive cameras;complicated computer vision algorithms;Microsoft Kinect sensor;real time user interaction;feature extraction;support vectors machines;SVM;artificial neural networks;ANN;k-nearest neighbors;KNN;Bayes classifier;BC,,14,,6,IEEE,12-May-16,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Comparative Analysis of Feature Detection and Extraction Techniques for Vision-based ISLR system,A. Tyagi; S. Bansal; A. Kashyap,"Computer Science and Engineering, Maharishi Markandehswar (Deemed to be)University, Ambala, India; Computer Science and Engineering, Maharishi Markandehswar (Deemed to be)University, Ambala, India; Computer Science and Engineering, Maharishi Markandehswar (Deemed to be)University, Ambala, India","2020 Sixth International Conference on Parallel, Distributed and Grid Computing (PDGC)",15-Jan-21,2020,,,515,520,"Sign language recognition is a highly adaptive interface between the deaf-mute community and machines. In India, Indian Sign Language (ISL) plays a significant role in the deaf-mute society, breaking communication distancing. Extracting features from the input image is crucial in vision-based Indian Sign Language Recognition (ISLR). This paper addresses feature detection and extraction techniques used in the ISLR. This paper categorizes existing techniques into three broad groups: scale-based, intensity-based, and hybrid techniques. SIFT (Scale Invariant Feature Transform), SURF (Speeded up Robust Features), FAST (Features from Accelerated Segment Test), BRIEF (Binary Robust Independent Elementary Features), and ORB (Oriented FAST and rotated BRIEF) are the techniques that have been evaluated and compared for intensity scaling, occlusion, orientation, affine transformation, blurring, and illumination. Results were generated in terms of key point detected, time-taken, and the match rate. SIFT is consistent in most circumstances, though it is slow. FAST is the fastest with good performance like ORB, and BRIEF shows its advantages in affine transformation and intensity changes.",2573-3079,978-1-7281-7132-6,10.1109/PDGC50313.2020.9315777,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9315777,Feature extraction;Keypoints;descriptor;SIFT;SURF;FAST;BRIEF;ORB;ISLR;ISL,Feature extraction;Detectors;Gesture recognition;Training;Real-time systems;Lighting;Handheld computers,affine transforms;feature extraction;image matching;sign language recognition,extraction techniques;vision-based ISLR system;deaf-mute community;deaf-mute society;communication distancing;input image;vision-based Indian Sign Language Recognition;feature detection;SIFT;Scale Invariant Feature Transform;Features from Accelerated Segment Test;Binary Robust Independent Elementary Features;Oriented FAST and rotated BRIEF;intensity scaling;affine transformation;Speeded up Robust Features,,3,,27,IEEE,15-Jan-21,,,IEEE,IEEE Conferences,Yes ,Yes,No,No,"""a comparative study on the
implementation of feature detection and extraction technique
is done. Techniques such as SIFT, SURF, FAST,BRIEF, and
ORB are generally used as a feature extraction technique for
vision-based ISLR system. These techniques are evolved
from each other and are the most popular ones in computer
vision. "" p.515",,
A vision-based hand motion parameter capturing for HCI,Chingyu Weng; Chuanyu Tseng; Mengfen Ho; Chunglin Huang,"Electrical Engineering Department, National Tsing Hua University, Hsinchu, Taiwan; Electrical Engineering Department, National Tsing Hua University, Hsinchu, Taiwan; Department of Electronic Engineering, Hsiuping Institute of Technology, Taichung, Taiwan; Electrical Engineering Department, National Tsing Hua University, Hsinchu, Taiwan","2008 International Conference on Audio, Language and Image Processing",08-Aug-08,2008,,,1219,1224,"In this paper, we develop a real-time motion capturing system. Based on 3-D hand model with structural constraint and kinematical constraint, we may estimate human hand state in high degree of freedom space by using the proposed separable state-based particle filter (SSBPF). We separate the states of the variable and track five different fingers individually by using the particle filter. Combining the hand model and SSBPF, we implement a human hand gesture reconstruction system",,978-1-4244-1723-0,10.1109/ICALIP.2008.4590116,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4590116,,Fingers;Solid modeling;Three dimensional displays;Particle filters;Joints;Computational modeling;Tracking,computer vision;gesture recognition;human computer interaction;image reconstruction;motion estimation;particle filtering (numerical methods);state estimation,vision-based hand motion parameter capturing;HCI;real-time motion capturing system;3D hand model;kinematical constraint;human hand state estimation;freedom space;separable state-based particle filter;human hand gesture reconstruction system,,2,,10,IEEE,08-Aug-08,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
Video-based handshape recognition using a handshape structure model in real time,K. Grobel; H. Hienz,"Lehrstuhl fÃ¼r Technische Informatik, RWTH Aachen University of Technology, Aachen, Germany; Lehrstuhl fÃ¼r Technische Informatik, RWTH Aachen University of Technology, Aachen, Germany",Proceedings of 13th International Conference on Pattern Recognition,06-Aug-02,1996,3,,446,450 vol.3,This paper presents a visual prototype for the recognition of 32 different handshapes used in finger spelling and in German sign language. A signer performs handshapes in front of a single video camera. In order to reach real-time performance the signer wears a coloured glove. The handshapes are recognized by the system with an accuracy of 94%. A description of the overall system is given and an approach to distinguish handshapes by their structure is presented. The lack of information within a recorded image is compensated by the use of a handshape structure model. This model is based on the features of the coloured areas and on relations between these areas.,1051-4651,0-8186-7282-X,10.1109/ICPR.1996.546987,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=546987,,Handicapped aids;Image recognition;Prototypes;Cameras;Fingers;Natural languages;Deafness;Image processing;Humans;Printing,computer vision,handshape recognition;handshape structure model;real time systems;finger spelling;German sign language;video camera;computer vision;colour coding;feature extraction,,2,1,10,IEEE,06-Aug-02,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Computer-Assisted Culture Learning in an Online Augmented Reality Environment Based on Free-Hand Gesture Interaction,M. -T. Yang; W. -C. Liao,"Department of Computer Science and Information Engineering, National Dong-Hwa University, Hualien, Taiwan; Department of Computer Science and Information Engineering, National Dong-Hwa University, Hualien, Taiwan",IEEE Transactions on Learning Technologies,02-Jul-14,2014,7,2,107,117,"The physical-virtual immersion and real-time interaction play an essential role in cultural and language learning. Augmented reality (AR) technology can be used to seamlessly merge virtual objects with real-world images to realize immersions. Additionally, computer vision (CV) technology can recognize free-hand gestures from live images to enable intuitive interactions. Therefore, we incorporate the latest AR and CV algorithms into a Virtual English Classroom, called VECAR, to promote immersive and interactive language learning. By wearing a pair of mobile computing glasses, users can interact with virtual contents in a three-dimensional space by using intuitive free-hand gestures. We design three cultural learning activities that introduce students to authentic cultural products and new cultural practices, and allow them to examine various cultural perspectives. The objectives of the VECAR are to make cultural and language learning appealing, improve cultural learning effectiveness, and enhance interpersonal communication between teachers and students.",1939-1382,,10.1109/TLT.2014.2307297,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6750035,Augmented reality;gesture recognition;computer-assisted language learning;VECAR,Cultural differences;Global communication;Three-dimensional displays;Augmented reality;Cameras;Educational institutions,augmented reality;computer aided instruction;gesture recognition;history;interactive devices,computer-assisted culture learning;online augmented reality environment;free-hand gesture interaction;physical-virtual immersion;language learning;AR technology;computer vision technology;CV technology;gesture recognition;virtual English classroom;VECAR;mobile computing glasses;intuitive free-hand gestures;cultural learning activities;learning effectiveness;cultural products;cultural practices;cultural perspectives,,47,,30,IEEE,27-Feb-14,,,IEEE,IEEE Journals,Yes ,No,Yes,No,,,
Traffic sign detection in static images using Matlab,M. A. Garcia; M. A. Sotelo; E. M. Gorostiza,"Department of Electronics, University of AlcalÃ¡, Madrid, Spain; Department of Electronics, University of AlcalÃ¡, Madrid, Spain; Department of Electronics, University of AlcalÃ¡, Madrid, Spain",EFTA 2003. 2003 IEEE Conference on Emerging Technologies and Factory Automation. Proceedings (Cat. No.03TH8696),03-Dec-03,2003,2,,212,215 vol.2,"In this paper a system for off-line traffic sign detection is shown. Matlab-image-processing toolbox is used for this purpose. The vision-based traffic sign detection module developed in this work manages 172/spl times/352 color images in RGB (red, green, blue) format. The first step in the algorithm is to obtain the gradient image and its vertical edge projection. In a second step, a color and shape analysis is performed.",,0-7803-7937-3,10.1109/ETFA.2003.1248701,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1248701,,MATLAB;Image edge detection;Color;Shape;Computer languages;Intelligent vehicles;Computer vision;Application software;Computational efficiency;Real time systems,traffic engineering computing;object detection;computer vision;road vehicles;image colour analysis;edge detection,off-line traffic sign detection;Matlab-image-processing toolbox;172/spl times/352 color images;gradient image;vertical edge projection;color analysis;shape analysis;static images,,5,,7,IEEE,03-Dec-03,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Computer-Aided Interpreter for Hearing and Speech Impaired,P. Suresh; N. Vasudevan; N. Ananthanarayanan,"Department of Electronics and Communication Engineering, Sri Venkateswara College of Engineering, Sriperumbudur, India; Department of Electronics and Communication Engineering, Sri Venkateswara College of Engineering, Sriperumbudur, India; Department of Electronics and Communication Engineering, Sri Venkateswara College of Engineering, Sriperumbudur, India","2012 Fourth International Conference on Computational Intelligence, Communication Systems and Networks",23-Aug-12,2012,,,248,253,"The difficulties faced by hearing and speech impaired people in communicating with others and among themselves can be easily overcome by building an assistive communication system. This real time communication system enables differently impaired people to communicate among themselves without an intermediate human translator. The proposed system is a potential human-computer and computer-human interaction for hearing and speech impaired people with normal people. This is achieved using Natural Voice processing and Digital Image Processing algorithms. The algorithm used, takes necessary precautions to combat a noisy environment and is particularly effective for human voice as it represents the speech in the Mel Cepstrum domain (B.P.Bogert, M.J.R.Healy, and J.W.Tukey-1963). A database is used to maintain the back-end of the system and efficient matching algorithms are employed. The system simulation has been done in MATLAB to build this community assistive digital communication system.",,978-1-4673-2640-7,10.1109/CICSyN.2012.53,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6274349,Speech recognition;feature extraction;Mel frequency;Digital Signal Processing;Dynamic Time Warping Algorithm;Hidden Markov Modeling Algorithm;Speaker Recognition System;Image Processing,Speech;Algorithm design and analysis;Databases;Noise;Auditory system;Hidden Markov models;Speech recognition,cepstral analysis;gesture recognition;handicapped aids;hearing;human computer interaction;image processing;signal representation;speech processing;vision defects,computer-aided interpreter;hearing impaired people;speech impaired people;real time communication system;intermediate human translator;human-computer interaction;computer-human interaction;normal people;natural voice processing;digital image processing algorithm;noisy environment;speech representation;Mel cepstrum domain;matching algorithm;system simulation;MATLAB;assistive digital communication system;sign language gesture;hand gesture,,5,,4,IEEE,23-Aug-12,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Image processing based remote control with robot arm simulator,Jiafeng Zhang; Feifei Zhang; Masanori Ito,"Tokyo University of Marine Science and Technology, Koto, Tokyo, Japan; Tokyo University of Marine Science and Technology, Koto, Tokyo, Japan; Tokyo University of Marine Science and Technology, Koto, Tokyo, Japan",2009 ICCAS-SICE,13-Nov-09,2009,,,2344,2348,"Motion estimation provides an attractive choice to cumbersome interface devices for human computer interaction (HCI). Worthy of note, visual recognition of hand gestures can help achieving in an easy and natural way for interaction between human and computer. The interfaces of HCI and other virtual reality systems depend on accurate, real-time hand and fingertip tracking for association between real objects and relative digital information. They cost expensively and complicated operations make them troublesome. We are developing a real-time, view-based gesture recognition system. Optical flow is estimated and segmented into motion fragments. Using artificial neural network (ANN), it can compute and estimate the motion of gesture. Comparing with the traditional approaches, theoretical and experimental results show that this method has more simple hardware and algorithm demands but more effectiveness. It can be used in remote control system for understanding the human body languages.",,978-4-907764-34-0,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5335126,motion estimation;optical flow;ANN;remote control,Image processing;Robot control;Process control;Motion estimation;Human computer interaction;Real time systems;Artificial neural networks;Computational modeling;Virtual reality;Costs,gesture recognition;human computer interaction;image segmentation;image sequences;manipulators;motion estimation;neural nets;robot vision;telerobotics;virtual reality,image processing;robot arm simulator;motion estimation;interface devices;human computer interaction;virtual reality systems;real-time hand tracking;fingertip tracking;view-based gesture recognition system;optical flow estimation;optical flow segmentation;artificial neural network;remote control system,,,,8,,13-Nov-09,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Max-pooling convolutional neural networks for vision-based hand gesture recognition,J. Nagi; F. Ducatelle; G. A. Di Caro; D. CireÅan; U. Meier; A. Giusti; F. Nagi; J. Schmidhuber; L. M. Gambardella,"Dalle Molle Institute for Artificial Intelligence (IDSIA), Dalle Molle Institute for Artificial Intelligence (IDSIA), University of Lugano, Manno Lugano, Switzerland; Dalle Molle Institute for Artificial Intelligence (IDSIA), Dalle Molle Institute for Artificial Intelligence (IDSIA), University of Lugano, Manno Lugano, Switzerland; Dalle Molle Institute for Artificial Intelligence (IDSIA), Dalle Molle Institute for Artificial Intelligence (IDSIA), University of Lugano, Manno Lugano, Switzerland; Dalle Molle Institute for Artificial Intelligence (IDSIA), Dalle Molle Institute for Artificial Intelligence (IDSIA), University of Lugano, Manno Lugano, Switzerland; Dalle Molle Institute for Artificial Intelligence (IDSIA), Dalle Molle Institute for Artificial Intelligence (IDSIA), University of Lugano, Manno Lugano, Switzerland; Dalle Molle Institute for Artificial Intelligence (IDSIA), Dalle Molle Institute for Artificial Intelligence (IDSIA), University of Lugano, Manno Lugano, Switzerland; Department of Mechanical Engineering, University Tenaga National, Putrajaya, Selangor Darul Ehsan, Malaysia; Dalle Molle Institute for Artificial Intelligence (IDSIA), Dalle Molle Institute for Artificial Intelligence (IDSIA), University of Lugano, Manno Lugano, Switzerland; Dalle Molle Institute for Artificial Intelligence (IDSIA), Dalle Molle Institute for Artificial Intelligence (IDSIA), University of Lugano, Manno Lugano, Switzerland",2011 IEEE International Conference on Signal and Image Processing Applications (ICSIPA),02-Feb-12,2011,,,342,347,"Automatic recognition of gestures using computer vision is important for many real-world applications such as sign language recognition and human-robot interaction (HRI). Our goal is a real-time hand gesture-based HRI interface for mobile robots. We use a state-of-the-art big and deep neural network (NN) combining convolution and max-pooling (MPCNN) for supervised feature learning and classification of hand gestures given by humans to mobile robots using colored gloves. The hand contour is retrieved by color segmentation, then smoothened by morphological image processing which eliminates noisy edges. Our big and deep MPCNN classifies 6 gesture classes with 96% accuracy, nearly three times better than the nearest competitor. Experiments with mobile robots using an ARM 11 533MHz processor achieve real-time gesture recognition performance.",,978-1-4577-0242-6,10.1109/ICSIPA.2011.6144164,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6144164,,Image color analysis;Gesture recognition;Training;Convolution;Real time systems;Mobile robots,data gloves;gesture recognition;human-robot interaction;image colour analysis;image denoising;image retrieval;image segmentation;learning (artificial intelligence);mobile robots;neural nets;robot vision,hand gesture recognition;max-pooling convolutional neural networks;computer vision;HRI interface;human-robot interaction;mobile robots;supervised feature learning;image classification;colored gloves;image retrieval;color image segmentation;morphological image processing;image denoising;ARM 11 533MHz processor,,332,4,40,IEEE,02-Feb-12,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Architecture of Object Recognition to Implement in Hardware,S. -Y. Kwon; Y. -H. Kim; Y. -H. Lee; M. -J. Kim,"Department of Electronic Engineering, Kumoh National Institute of Technology, Gumi, Korea; Department of IT Convergence, Kumoh National Institute of Technology, Gumi, Korea; Department of Electronic Engineering, Kumoh National Institute of Technology, Gumi, South Korea; Withsystem, Gumi, Korea",2018 International Conference on Computational Science and Computational Intelligence (CSCI),02-Jan-20,2018,,,421,425,"Object recognition, a core technology that cannot be missed in the industrial revolution, is used in various fields such as face recognition, robot control, IoT and autonomous driving. In this paper, we propose hand region detection based on computer vision. We used a method of using the images obtained from the camera because the method of wearing the equipment is expensive and has the cumbersome disadvantage of wearing it every time. We also used FPGA as an accelerator for real-time image processing. We used YCbCr color domain transformation, median filter, morphology dilation and labeling for hand detection. In this paper, since the result of YCbCr color conversion is 0 or 1, the result of 24 bits is reduced to 1 bit, so the memory size can be reduced. YCbCr Since the result is 0 or 1, when median filter is executed with mask of size 3by3, it can be seen that the intermediate value is 1 when the addition result is 5 or more. The result of the median filter used in this paper is much faster than that of the conventional median filter, which is obtained by comparing the sizes of the pixels, since it can be obtained by 1 clock. The use of line buffers during morphological operations and labeling reduces the total amount of data to be loaded.",,978-1-7281-1360-9,10.1109/CSCI46756.2018.00085,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8947790,hand gesture recognition;human-computer interaction;computer vision;FPGA,Image color analysis;Labeling;Colored noise;Clocks;Morphology;Skin;Hardware design languages,computer vision;field programmable gate arrays;image colour analysis;median filters;object recognition,object recognition;hand region detection;computer vision;YCbCr color domain transformation;morphology dilation;hardware implementation;FPGA;real-time image processing accelerator;median filter,,1,,12,IEEE,02-Jan-20,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
A Novel Architectural Pattern to Support the Development of Human-Robot Interaction (HRI) Systems Integrating Haptic Interfaces and Gesture Recognition Algorithms,G. A. Farulla; L. O. Russo; V. Gallifuoco; M. Indaco,"Politecnico di Torino, Torino, TO, Italy; Politecnico di Torino, Torino, TO, Italy; Politecnico di Torino, Torino, TO, Italy; Politecnico di Torino, Lero - The Irish Software Engineering Research Centre, Limerick, Ireland",2015 IEEE Computer Society Annual Symposium on VLSI,29-Oct-15,2015,,,386,391,"Haptic and robotic interfaces are recently gaining momentum to be pervasively integrated in modern everyday life. In fact, they can be employed in several different fields, ranging from manipulation of small and dangerous objects to rehabilitation, assistive and service technologies, and are also integrated in mission critical systems. Modern research is rapidly shifting to investigate novel and more intuitive ways of controlling these interfaces. In particular, gesture-based control is one of the most interesting scenario for Human-Robot Interaction (HRI), since we human perceive gestures as a natural way of interaction with the external world. In this work we present a novel architectural pattern, entirely based on the Robotic Operating System (ROS), to support the development of applications and systems where computer vision techniques are applied to control robotic interfaces. As case study, the presented pattern is used to develop and assess the overall PARLOMA system. PARLOMA project aims at developing a system to enable remote communication between deaf-blind subjects. The system is designed to send, remotely and in real-time, messages in tactile Sign Language from a sender to a deaf-blind recipient (or many recipients) by integrating hand tracking and gesture recognition algorithms coupled with bio-mimetic haptic interfaces.",2159-3477,978-1-4799-8719-1,10.1109/ISVLSI.2015.112,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7309598,Human-Robot Interaction;Gesture Recognition;Haptic Interfaces;Service Robotics,Cameras;Peer-to-peer computing;Joints;Haptic interfaces;Robot vision systems;Gesture recognition,control engineering computing;gesture recognition;haptic interfaces;human-robot interaction;operating systems (computers);robot vision,architectural pattern;human-robot interaction systems;HRI systems;haptic interfaces;gesture recognition algorithms;robotic interfaces;rehabilitation;assistive technology;service technology;gesture-based control;robotic operating system;ROS;computer vision;robotic interface control;PARLOMA system;remote communication;deaf-blind subjects;tactile sign language;hand tracking;biomimetic hap tic interfaces,,,,27,IEEE,29-Oct-15,,,IEEE,IEEE Conferences,Yes ,No,Yes,No,,,
A Real-Time Gesture Recognition System with FPGA Accelerated ZynqNet Classification,R. NÃºÃ±ez-Prieto; P. C. GÃ³mez; L. Liu,"Department of Electrical and Information Technology, Lund University, Sweden; Dpto. de AutomÃ¡ tica, IngenierÃ­ a ElÃ©ctrica y ElectrÃ³ nica e InformÃ¡tica Industrial, Universidad PolitÃ©cnica de Madrid, Spain; Department of Electrical and Information Technology, Lund University, Sweden",2019 IEEE Nordic Circuits and Systems Conference (NORCAS): NORCHIP and International Symposium of System-on-Chip (SoC),21-Nov-19,2019,,,1,6,"This paper presents a real-time hand gesture recognition system by accelerating a convolutional neural network (CNN) using FPGA platform. More specifically, ZynqNet is adopted and modified to fulfill the classification task of recognizing the Swedish manual alphabet, which is used by sign language users for spelling purposes, also known as fingerspelling. Data augmentation and transfer learning techniques have been used during the training phase to improve the classification accuracy up to 80.1%, even with an 8-bit ZynqNet model. Extensive analysis of memory requirements and data processing patterns has been performed to enable optimization techniques, including memory partitioning and register arrays. The resulting FPGA implementation on a Xilinx UltraScale device avoids the use of off-chip memories, which together with block-wise processing scheduling, achieves an image rate of 23.5 frames per second (FPS) at 200 MHz clock frequency.",,978-1-7281-2769-9,10.1109/NORCHIP.2019.8906956,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8906956,ZynqNet;computer vision;CNN;hand gesture recognition;high-level synthesis;FPGA,,,,,2,,14,IEEE,21-Nov-19,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Multiple Parallel Vision-Based Recognition in a Real-Time Framework for Human-Robot-Interaction Scenarios,T. Rehrl; A. Bannat; J. Gast; F. Wallhoff; G. Rigoll; C. Mayer; Z. Riaz; B. Radig; S. Sosnowski; K. KÃ¼hnlenz,"Human-Machine Communication, Technische UniversitÃ¤t MÃ¼nchen, Munich, Germany; Human-Machine Communication, Technische UniversitÃ¤t MÃ¼nchen, Munich, Germany; Human-Machine Communication, Technische UniversitÃ¤t MÃ¼nchen, Munich, Germany; Human-Machine Communication, Technische UniversitÃ¤t MÃ¼nchen, Munich, Germany; Human-Machine Communication, Technische UniversitÃ¤t MÃ¼nchen, Munich, Germany; Chair for Image Understanding and Knowledge-Based Systems, Technische UniversitÃ¤t MÃ¼nchen, Munich, Germany; Chair for Image Understanding and Knowledge-Based Systems, Technische UniversitÃ¤t MÃ¼nchen, Munich, Germany; Chair for Image Understanding and Knowledge-Based Systems, Technische UniversitÃ¤t MÃ¼nchen, Munich, Germany; Department of Electrical Engineering and Information Technologies, Technische UniversitÃ¤t MÃ¼nchen, Munich, Germany; Department of Electrical Engineering and Information Technologies, Technische UniversitÃ¤t MÃ¼nchen, Munich, Germany",2010 Third International Conference on Advances in Computer-Human Interactions,15-Mar-10,2010,,,50,55,"Every day human communication relies on a large number of different communication mechanisms like spoken language, facial expressions, body pose and gestures, allowing humans to pass large amounts of information in short time. In contrast, traditional human-machine communication is often unintuitive and requires specifically trained personal. In this paper, we present a real-time capable framework that recognizes traditional visual human communication signals in order to establish a more intuitive human-machine interaction. Humans rely on the interaction partnerâs face for identification, which helps them to adapt to the interaction partner and utilize context information. Head gestures (head nodding and head shaking) are a convenient way to show agreement or disagreement. Facial expressions give evidence about the interaction partnersâ emotional state and hand gestures are a fast way of passing simple commands. The recognition of all interaction queues is performed in parallel, enabled by a shared memory implementation.",,978-1-4244-5694-9,10.1109/ACHI.2010.44,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5430123,real-time image processing;gesture recognition;human-robot interaction;facial expressions,Face recognition;Man machine systems;Hidden Markov models;Image recognition;Magnetic heads;Data mining;Human robot interaction;Active appearance model;Computer vision;Concurrent computing,face recognition;gesture recognition;human-robot interaction;robot vision,multiple parallel vision based recognition;human robot interaction scenarios;spoken language;facial expressions;human machine communication;visual human communication signals;human machine interaction,,3,2,37,IEEE,15-Mar-10,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Sign Language Fingerspelling Classification from Depth and Color Images Using a Deep Belief Network,L. Rioux-Maldague; P. GiguÃ¨re,"Carleton University, Ottawa, ON, CA; Department of Computer Science, UniversitÃ© Laval, QuÃ©bec City, Canada",2014 Canadian Conference on Computer and Robot Vision,19-May-14,2014,,,92,97,"Automatic sign language recognition is an open problem that has received a lot of attention recently, not only because of its usefulness to signers, but also due to the numerous applications a sign classifier can have. In this article, we present a new feature extraction technique for hand pose recognition using depth and intensity images captured from a Microsoft Kinect sensor. We applied our technique to American Sign Language finger spelling classification using a Deep Belief Network, for which our feature extraction technique is tailored. We evaluated our results on a multi-user data set with two scenarios: one with all known users and one with an unseen user. We achieved 99% recall and precision on the first, and 77% recall and 79% precision on the second. Our method is also capable of real-time sign classification and is adaptive to any environment or lightning intensity.",,978-1-4799-4337-1,10.1109/CRV.2014.20,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6816829,Deep Learning;Depth Features;Hand Pose Recognition;Fingerspelling,Feature extraction;Assistive technology;Training;Gesture recognition;Accuracy;Vectors;Cameras,feature extraction;image classification;image colour analysis;image sensors;palmprint recognition;pose estimation,American sign language fingerspelling classification;color images;depth images;deep belief network;automatic sign language recognition;feature extraction technique;hand pose recognition;intensity images;Microsoft Kinect sensor;multiuser data set;lightning intensity,,31,,15,IEEE,19-May-14,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Randomized decision forests for static and dynamic hand shape classification,C. Keskin; F. KiraÃ§; Y. E. Kara; L. Akarun,"Computer Engineering Department, BoÄaziÃ§i University, Istanbul, Turkey; Computer Engineering Department, BoÄaziÃ§i University, Istanbul, Turkey; Computer Engineering Department, BoÄaziÃ§i University, Istanbul, Turkey; Computer Engineering Department, BoÄaziÃ§i University, Istanbul, Turkey",2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops,16-Jul-12,2012,,,31,36,"This paper proposes a novel algorithm to perform hand shape classification using depth sensors, without relying on color or temporal information. Hence, the system is independent of lighting conditions and does not need a hand registration step. The proposed method uses randomized classification forests (RDF) to assign class labels to each pixel on a depth image, and the final class label is determined by voting. This method is shown to achieve 97.8% success rate on an American Sign Language (ASL) dataset consisting of 65k images collected from five subjects with a depth sensor. More experiments are conducted on a subset of the ChaLearn Gesture Dataset, consisting of a lexicon with static and dynamic hand shapes. The hands are found using motion cues and cropped using depth information, with a precision rate of 87.88% when there are multiple gestures, and 94.35% when there is a single gesture in the sample. The hand shape classification success rate is 94.74% on a small subset of nine gestures corresponding to a single lexicon. The success rate is 74.3% for the leave-one-subject-out scheme, and 67.14% when training is conducted on an external dataset consisting of the same gestures. The method runs on the CPU in real-time, and is capable of running on the GPU for further increase in speed.",2160-7516,978-1-4673-1612-5,10.1109/CVPRW.2012.6239183,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6239183,,Shape;Training;Sensors;Accuracy;Vegetation;Image segmentation;Decision trees,gesture recognition;image classification;image motion analysis;random processes,randomized decision forests;static hand shape classification;dynamic hand shape classification;depth sensor;RDF;class labels;depth image pixels;voting scheme;American Sign Language dataset;ASL dataset;image collection;ChaLearn Gesture Dataset;lexicon;motion cues;precision rate;hand shape classification success rate;leave-one-subject-out scheme;external dataset;CPU;GPU;hand gestures,,26,,18,IEEE,16-Jul-12,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Improved Multi-Modal Recognition Interface for Intelligent HCI Based on Speech and the KSSL Recognition,J. -h. Kim; K. -s. Hong,"School of Information and Communication Engineering, Sungkyunkwan University, Suwon, Kyunggi, South Korea; School of Information and Communication Engineering, Sungkyunkwan University, Suwon, Kyunggi, South Korea",2006 IEEE Odyssey - The Speaker and Language Recognition Workshop,20-Nov-06,2006,,,1,6,"Desktop PC and wire communications net-based traditional studies on pattern recognition and multimodal interaction have some restrictions (e.g. limitation of motion, conditionality in space and so on) and general problems according to using of the vision technologies for recognition and representation of the hap tic-gesture information. In this paper, we propose and implement multi-modal recognition interface (hereinafter, MMRI) integrating speech using voice-XML based on WWW and the post wearable PC-based gesture, it have purposes that recognizes and represents the Korean Standard Sign Language (hereinafter, KSSL) which is a dialogue system and interactive elements in the Korean deaf communities. The advantages of our approach are as follows: 1) it improves efficiency of the MMRI input module according to the technology of wireless communication, 2) it shows higher recognition performance than uni-modal recognition system (using gesture or speech), 3) it recognizes and represents continuous sign language of users with flexibility in real time and can offer to user a wider range of personalized and differentiated information using the MMRI more effectively. Experimental results, the MMRI deduces an average recognition rate of 96.1% about significant, dynamic and continuous the KSSL and speech of various users",,1-4244-0472-X,10.1109/ODYSSEY.2006.248108,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4013525,,Speech recognition;Human computer interaction;Pattern recognition;Space technology;Handicapped aids;Wire;World Wide Web;Deafness;Communications technology;Wireless communication,gesture recognition;human computer interaction;natural language interfaces;natural languages;speech recognition;speech-based user interfaces,multimodal recognition interface;MMRI;intelligent HCI;speech recognition;voice-XML;PC-based gesture recognition;Korean Standard Sign Language;KSSL,,,,10,IEEE,20-Nov-06,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Optimal Sign language recognition employing multi-layer CNN,P. Pranav; R. Katarya,"Dept. of Computer Science Engineering, Delhi Technological University, Delhi, India; Dept. of Computer Science Engineering, Delhi Technological University, Delhi, India","2022 4th International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)",28-Mar-23,2022,,,288,293,"We generally convey our ideas, thoughts, and facts through vocal communication. However, not everyone is gifted with the ability to express oneself. verbally. The deaf and mute populations have a difficult time expressing their thoughts and ideas to others; Sign Language (SL) is their most expressive mode of communication, but the majority of the general population is illiterate in SL; as a result, the mute and deaf have difficulty communicating with the rest of the world. A device that can reliably translate SL gestures to voice and vice versa in real-time is required to overcome this communication barrier. The current solutions are not real-time, have poor recognition accuracy, and need static environmental conditions. Some systems might need additional hardware components, such as expensive sensors, which boost the cost. The current methods allowing silent people to communicate their thoughts to others may be divided into two categories: systems based on computer vision and systems based on electrical sensors, each with its own set of benefits and drawbacks. In comparison to previous state-of-the-art approaches, we offer a Convolution neural network (CNN) based SLR system with many layers that gives great accuracy of 99.89%. Overall, we believe the study will serve as road map for future research in the domain of SLR.",,978-1-6654-7436-8,10.1109/ICAC3N56670.2022.10074397,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10074397,Convolution Neural Network(CNN);Deep Learning(DL);Sign Language;Support Vector Machine(SVM),Support vector machines;Convolution;Sociology;Neural networks;Gesture recognition;Assistive technologies;Sensor systems,,,,,,24,IEEE,28-Mar-23,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Robust ASL Fingerspelling Recognition Using Local Binary Patterns and Geometric Features,C. S. Weerasekera; M. H. Jaward; N. Kamrani,"Department of Electrical and Computer Systems Engineering, Monash University Sunway Campus, Malaysia; Department of Electrical and Computer Systems Engineering, Monash University Sunway Campus, Malaysia; Department of Electrical and Computer Systems Engineering, Monash University Sunway Campus, Malaysia",2013 International Conference on Digital Image Computing: Techniques and Applications (DICTA),23-Dec-13,2013,,,1,8,"Sign language recognition using computer vision techniques enables machines to function as interpreters of sign language while eliminating the need for cumbersome data gloves. In this paper, a robust approach for recognition of bare-handed static sign language is presented, using a novel combination of features. These include Local Binary Patterns (LBP) histogram features based on color and depth information, and also geometric features of the hand. Linear binary Support Vector Machine (SVM) classifiers are used for recognition, coupled with template matching in the case of multiple matches. An accurate hand segmentation scheme using the Kinect depth sensor is also presented. The resulting sign language recognition system could be employed in many practical scenarios and works in complex environments in real-time. It is also shown to be robust to changes in distance between the user and camera and can handle possible variations in fingerspelling among different users. The algorithm is tested on two ASL fingerspelling datasets where overall classification rates over 90% are observed.",,978-1-4799-2126-3,10.1109/DICTA.2013.6691521,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691521,,Histograms;Image color analysis;Image edge detection;Feature extraction;Cameras;Vectors;Color,computer vision;data gloves;image classification;image colour analysis;image matching;image segmentation;image sensors;sign language recognition;support vector machines,ASL fingerspelling recognition;geometric features;computer vision techniques;sign language interpreters;data gloves;bare-handed static sign language recognition;local binary patterns histogram features;LBP histogram features;color information;depth information;linear binary support vector machine classifier;SVM classifiers;template matching;hand segmentation scheme;Kinect depth sensor,,7,,16,IEEE,23-Dec-13,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Real time hand pose estimation using depth sensors,C. Keskin; F. KÄ±raÃ§; Y. E. Kara; L. Akarun,"Computer Engineering Department, Bogazici University, Istanbul, Turkey; Computer Engineering Department, Bogazici University, Istanbul, Turkey; Computer Engineering Department, Bogazici University, Istanbul, Turkey; Computer Engineering Department, Bogazici University, Istanbul, Turkey",2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops),16-Jan-12,2011,,,1228,1234,"This paper describes a depth image based real-time skeleton fitting algorithm for the hand, using an object recognition by parts approach, and the use of this hand modeler in an American Sign Language (ASL) digit recognition application. In particular, we created a realistic 3D hand model that represents the hand with 21 different parts. Random decision forests (RDF) are trained on synthetic depth images generated by animating the hand model, which are then used to perform per pixel classification and assign each pixel to a hand part. The classification results are fed into a local mode finding algorithm to estimate the joint locations for the hand skeleton. The system can process depth images retrieved from Kinect in real-time at 30 fps. As an application of the system, we also describe a support vector machine (SVM) based recognition module for the ten digits of ASL based on our method, which attains a recognition rate of 99.9% on live depth images in real-time1.",,978-1-4673-0063-6,10.1109/ICCVW.2011.6130391,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130391,,Joints;Training;Vegetation;Three dimensional displays;Estimation;Accuracy,image classification;object recognition;pose estimation;support vector machines,real time hand pose estimation;depth sensors;real-time skeleton fitting;object recognition;American sign language digit recognition;3D hand model;random decision forests;synthetic depth images;per pixel classification;Kinect;support vector machine;SVM based recognition module,,132,4,19,IEEE,16-Jan-12,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Real-time facial expression recognition based on features' positions and dimensions,H. Sako; A. V. W. Smith,"Hitachi Central Research Laboratory Limited, Kokubunji, Tokyo, Japan; Research & Development Centre, Dublin Laboratory (HDL), O'Reilly Institute, Trinity College, Hitachi Europe Limited, Dublin, Ireland",Proceedings of 13th International Conference on Pattern Recognition,06-Aug-02,1996,3,,643,648 vol.3,"This paper describes a method of real-time facial expression recognition which is based on automatic measurement of the facial features' dimension and the positional relationship between them. The method is composed of two parts, the facial feature extraction using matching techniques and the facial expression recognition using statistics of position and dimension of the features. The method is implemented in an experimental hardware system and the performance is evaluated. The extraction rates of the facial-area, the mouth and the eyes are about 100%, 96% and 90%, respectively, and the recognition rates of facial expression such as normal, angry, surprise, smile and sad expression are 54%, 89%, 86%, 53% and 71%, respectively, for a specific person. The whole processing speed is about 15 frames/second. Finally, we touch on some applications such as man-machine interface, automatic generation of facial graphic animation and sign language translation using facial expression recognition techniques.",1051-4651,0-8186-7282-X,10.1109/ICPR.1996.547025,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=547025,,Face recognition;Facial features;Position measurement;Statistics;Hardware;Mouth;Eyes;User interfaces;Graphics;Facial animation,computer vision,real-time systems;facial expression recognition;feature positions;feature dimensions;facial feature extraction;image matching;statistical analysis;computer vision;colour matching,,8,2,12,IEEE,06-Aug-02,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Incorporating On-demand Stereo for Real Time Recognition,T. Deselaers; A. Criminisi; J. Winn; A. Agarwal,"Human Language Technology and Pattern Recognition, RWTH Aachen University of Technology, Germany; Microsoft Research Limited, Cambridge, UK; Microsoft Research Limited, Cambridge, UK; Microsoft Research Limited, Cambridge, UK",2007 IEEE Conference on Computer Vision and Pattern Recognition,16-Jul-07,2007,,,1,8,"A new method for localising and recognising hand poses and objects in real-time is presented. This problem is important in vision-driven applications where it is natural for a user to combine hand gestures and real objects when interacting with a machine. Examples include using a real eraser to remove words from a document displayed on an electronic surface. In this paper the task of simultaneously recognising object classes, hand gestures and detecting touch events is cast as a single classification problem. A random forest algorithm is employed which adaptively selects and combines a minimal set of appearance, shape and stereo features to achieve maximum class discrimination for a given image. This minimal set leads to both efficiency at run time and good generalisation. Unlike previous stereo works which explicitly construct disparity maps, here the stereo matching costs are used directly as visual cue and only computed on-demand, i.e. only for pixels where they are necessary for recognition. This leads to improved efficiency. The proposed method is assessed on a database of a variety of objects and hand poses selected for interacting on a flat surface in an office environment.",1063-6919,1-4244-1179-3,10.1109/CVPR.2007.383136,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4270161,,Object detection;Pattern recognition;Shape;Cameras;Humans;Event detection;Costs;Visual databases;Hardware;Object recognition,image classification;image matching;stereo image processing,on-demand stereo;real time recognition;vision-driven applications;hand gestures;real objects;real eraser;image classification;random forest algorithm;disparity maps;stereo matching,,15,1,24,IEEE,16-Jul-07,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
VECAR: Virtual English Classroom with Markerless Augmented Reality and Intuitive Gesture Interaction,M. -T. Yang; W. -C. Liao; Y. -C. Shih,"NDHU, Hualien, Taiwan; NDHU, Hualien, Taiwan; NDHU, Hualien, Taiwan",2013 IEEE 13th International Conference on Advanced Learning Technologies,19-Sep-13,2013,,,439,440,"Augmented Reality (AR) is a technology that merges virtual objects with real-world images seamlessly. The power of real-time interaction and complete immersion makes AR ideal for language learning in that the exposure and motivation are two important factors. Therefore, we propose a Virtual English Classroom empowered with marker less AR technologies and 3D gesture interactions, called VECAR. The proposed vision-based interface is intuitive and convenient for users to play around with free hands, without the need of keyboard and mouse. The objective of VECAR is to make English teaching more interesting to increase learners' motivations.",2161-377X,978-0-7695-5009-1,10.1109/ICALT.2013.134,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601976,English Teaching;Markerless Augmented Reality;3D Gesture Interaction,Three-dimensional displays;Augmented reality;Education;Thumb;Cameras;Keyboards,augmented reality;computer aided instruction;gesture recognition;teaching,VECAR;virtual english classroom;markerless augmented reality;intuitive gesture interaction;AR;language learning;exposure factor;motivation factor;3D gesture interaction;vision-based interface;English teaching;learner motivation,,4,,6,IEEE,19-Sep-13,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Bringing the Wiki Collaboration Model to the Tabletop World,S. Baraldi; A. D. Bimbo; A. Valli,"Dipartimento di Sistemi e Informatica, UniversitÃ  degli Studi di Firenze, Italy; Dipartimento di Sistemi e Informatica, UniversitÃ  degli Studi di Firenze, Italy; Dipartimento di Sistemi e Informatica, UniversitÃ  degli Studi di Firenze, Italy",2006 IEEE International Conference on Multimedia and Expo,26-Dec-06,2006,,,333,336,"We present an interactive workspace that integrates wiki collaboration in knowledge-building activities with face-to-face scenarios like brainstorming or problem solving sessions. A wiki serves as the repository for knowledge elements, which are presented to co-located users in form of a concept map projected on a table. A computer vision module tracks multiple hands and fingers on the table surface and map elements can be manipulated using a simple gesture language. Also smart devices, like PDAs or tablets, can be connected to the system and participate in the interaction with their local input. The concept map and the wiki are synchronized in real-time, providing notifications to both co-located and distributed users and allowing a community shared awareness that enhances and enriches the knowledge building experience",1945-788X,1-4244-0367-7,10.1109/ICME.2006.262466,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4036604,,Collaboration;Collaborative work;Collaborative software;Problem-solving;Computer vision;Personal digital assistants;Buildings;Collaborative tools;Displays;Knowledge management,computer vision;gesture recognition;interactive systems;Internet;user interfaces,wiki collaboration model;tabletop world;interactive workspace;knowledge-building activity;face-to-face scenario;computer vision module;gesture language;smart device,,1,,12,IEEE,26-Dec-06,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Unravelling of Convolutional Neural Networks through Bharatanatyam Mudra Classification with Limited Data,A. P. Parameshwaran; H. P. Desai; M. Weeks; R. Sunderraman,"Department of Computer Science, Georgia State University (GSU), Atlanta, Georgia; Department of Computer Science, Georgia State University (GSU), Atlanta, Georgia; Department of Computer Science, Georgia State University (GSU), Atlanta, Georgia; Department of Computer Science, Georgia State University (GSU), Atlanta, Georgia",2020 10th Annual Computing and Communication Workshop and Conference (CCWC),12-Mar-20,2020,,,342,347,"Non-verbal forms of communication are universal, being free of any language barrier and widely used in all art forms. For example, in Bharatanatyam, an ancient Indian dance form, artists use different hand gestures, body postures and facial expressions to convey the story line. As identification and classification of these complex and multivariant visual images are difficult, it is now being addressed with the help of advanced computer vision techniques and deep neural networks. This work deals with studies in automation of identification, classification and labelling of selected Bharatnatyam gestures, as part of our efforts to preserve this rich cultural heritage for future generations. The classification of the mudras against their true labels was carried out using different singular pre-trained / non-pre-trained as well as stacked ensemble convolutional neural architectures (CNNs). In all, twenty-seven classes of asamyukta hasta (single hand gestures) data were collected from Google, YouTube and few real time performances by artists. Since the background in many frames are highly diverse, the acquired data is real and dynamic, compared to images from closed laboratory settings. The cleansing of mislabeled data from the dataset was done through label transferring based on distance-based similarity metric using convolutional siamese neural network. The classification of mudras was done using different CNN architecture: i) singular models, ii) ensemble models, and iii) few specialized models. This study achieved an accuracy of >95%, both in single and double transfer learning models, as well as their stacked ensemble model. The results emphasize the crucial role of domain similarity of the pre-training / training datasets for improved classification accuracy and, also indicate that doubly pre-trained CNN model yield the highest accuracy.",,978-1-7281-3783-4,10.1109/CCWC47524.2020.9031185,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9031185,Bharatanatyam;Convolutional Neural Networks;Hand Gestures;Convolutional Siamese Neural Network;Ensemble Models;Asamyukta Hastas,Feature extraction;Task analysis;Image edge detection;YouTube;Visualization;Lead,computer vision;convolutional neural nets;data acquisition;face recognition;gesture recognition;humanities;image classification;learning (artificial intelligence);neural net architecture,convolutional siamese neural network;CNN architecture;Bharatnatyam gestures;distance-based similarity metric;mislabeled data;stacked ensemble convolutional neural architectures;cultural heritage;deep neural networks;computer vision techniques;multivariant visual images;story line;facial expressions;body postures;hand gestures;ancient Indian dance form;art forms;Bharatanatyam mudra classification;double transfer learning models;single transfer learning models,,3,,25,IEEE,12-Mar-20,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
An Adaptive Real-time Gesture Detection Method Using EMG and IMU Series for Robot Control,X. Zhao; Y. Ma; J. Huang; J. Zheng; Y. Dong,"Institute of Computer Applied Technology of China Weapon Industry, Beijing, China; Institute of Computer Applied Technology of China Weapon Industry, Beijing, China; Institute of Computer Applied Technology of China Weapon Industry, Beijing, China; Institute of Computer Applied Technology of China Weapon Industry, Beijing, China; Institute of Computer Applied Technology of China Weapon Industry, Beijing, China",2021 IEEE International Conference on Unmanned Systems (ICUS),22-Dec-21,2021,,,539,547,"With the development of technology, people constantly put forward new requirements to the level and quality of human-computer interaction. In view of the intuitive and natural characteristics of sign language, gesture has become an important means of human-computer interaction. Therefore, a dynamic gesture recognition system which could adaptively recognize multiple gestures is the main development trend of gesture recognition system in the future. Though vision based dynamic gesture recognition system has always been one of the hotspots in the current scientific research field, its vulnerability to interference of light and obstacles is still a significate weakness that need to be overcome before utilizing in the real scenario. On this account, the current research constructs an actual time gesture recognition system fusing the physiological signal including Inertial Measurement Unit (IMU) and Electromyography (EMG), combined with adaptability design through self-adjusting threshold. Together, by evaluating experiment in 5 days, the adaptive classifier successfully identified 91.95% of 360 gestures with the latency of 10.89 ms, and the robots correctly responded to 88.61% of gestures in control dictionary.",,978-1-6654-3885-8,10.1109/ICUS52573.2021.9641278,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9641278,Gestsure Recognition;EMG;IMU;Muscle-Computer-Interfaces,Human computer interaction;Adaptive systems;Robot control;Redundancy;Gesture recognition;Assistive technologies;Electromyography,adaptive signal processing;electromyography;gesture recognition;human computer interaction;human-robot interaction;image fusion;mobile robots;sensor fusion;signal classification,human-computer interaction;EMG;adaptability design;adaptive classifier;IMU series;adaptive real-time gesture detection;self-adjusting threshold;physiological signal fusion;inertial measurement unit;electromyography;robot control;control dictionary,,3,,17,IEEE,22-Dec-21,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
3D hand pose estimation and classification using depth sensors,C. Keskin; F. KÄ±raÃ§; Y. E. Kara; L. Akarun,"Bilgisayar MÃ¼hendisliÄi BÃ¶lÃ¼mÃ¼, BoÄaziÃ§i Ãniversitesi, Istanbul, Turkey; Bilgisayar MÃ¼hendisliÄi BÃ¶lÃ¼mÃ¼, BoÄaziÃ§i Ãniversitesi, Istanbul, Turkey; Bilgisayar MÃ¼hendisliÄi BÃ¶lÃ¼mÃ¼, BoÄaziÃ§i Ãniversitesi, Istanbul, Turkey; Bilgisayar MÃ¼hendisliÄi BÃ¶lÃ¼mÃ¼, BoÄaziÃ§i Ãniversitesi, Istanbul, Turkey",2012 20th Signal Processing and Communications Applications Conference (SIU),28-May-12,2012,,,1,4,"This paper describes our method to fit a 3D skeleton to the human hand using depth images. The human hand is represented by a 3D skeleton with 21 parts. This model is used to generate synthetic depth images, that are used to train Random Decision Forests (RDF), which are used to assign each pixel to a hand part. Mean-shift method is used on the classification results and joint locations are estimated. The system can run in real time at 30 fps on Kinect depth images. We use this method and Support Vector Machines for classification and obtain 99.9% recognition rate on the American Sign Language (ASL) digit recognition problem.",2165-0608,978-1-4673-0056-8,10.1109/SIU.2012.6204611,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6204611,,Three dimensional displays;Estimation;Real time systems;Pattern recognition;Computer vision;Conferences;Resource description framework,pose estimation;spatial variables measurement;support vector machines,3D hand pose estimation;3D hand pose classification;depth sensors;3D skeleton;human hand;synthetic depth images;random decision forests;mean-shift method;support vector machines;American Sign Language;ASL digit recognition problem,,2,,11,IEEE,28-May-12,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Unity based Urban Environment Simulation for Autonomous Vehicle Stereo Vision Evaluation,M. VukiÄ; B. GrgiÄ; D. DinÄir; L. Kostelac; I. MarkoviÄ,"Faculty of Electrical Engineering and Computin Unska 3, University of Zagreb, Zagreb, Croatia; Faculty of Electrical Engineering and Computin Unska 3, University of Zagreb, Zagreb, Croatia; Faculty of Electrical Engineering and Computin Unska 3, University of Zagreb, Zagreb, Croatia; Faculty of Electrical Engineering and Computin Unska 3, University of Zagreb, Zagreb, Croatia; Faculty of Electrical Engineering and Computin Unska 3, University of Zagreb, Zagreb, Croatia","2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",11-Jul-19,2019,,,949,954,"Testing and evaluation of sensor processing algorithms for autonomous vehicles is challenging due to the problem of collecting reference data. To ensure safety and robustness, many man-hours need to be spent in collecting and preparing such data. One solution to alleviate this problem is to use computer simulations. Computer simulations can model a real system with all its static and dynamic characteristics. This approach provides efficiency and precision when collecting data and reduces testing time. The aim of this paper is development of a simulation environment based on Unity where it would be possible to test sensors and algorithms for autonomous vehicles and show deviations from reference data. The proposed simulation model contains typical city objects and participants: roads, sidewalks, buildings, pedestrians, traffic signs and vehicles. In this paper we simulate motion and sensors from a single vehicle equipped with a stereo camera setup. The program environment Unity is used for designing the simulation, and behavioral scripts are executed with C# programming language. To showcase the testing of applicable algorithms, OpenCV class for computing stereo correspondence, using the semi-global block matching algorithm, is used on simulated stereo images and we discuss future development of the simulation.",2623-8764,978-953-233-098-4,10.23919/MIPRO.2019.8756805,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8756805,Autonomous Vehicles;Urban Scenario Simulation;Stereo Vision,Cameras;Autonomous vehicles;Lighting;Computational modeling;Testing;Urban areas;Robot sensing systems,C# language;cameras;computer simulation;image matching;image sensors;mobile robots;road vehicles;robot vision;stereo image processing;traffic engineering computing,urban environment simulation;autonomous vehicle stereo vision evaluation;sensor processing algorithms;computer simulations;static characteristics;dynamic characteristics;stereo camera setup;stereo correspondence;semiglobal block matching algorithm;stereo images;Unity;C# programming language;OpenCV class,,3,,22,,11-Jul-19,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
A visual computing environment for very large scale biomolecular modeling,M. Zeller; J. C. Phillips; A. Dalke; W. Humphrey; K. Schulten; R. Sharma; T. S. Huang; V. I. Pavlovic; Y. Zhao; Z. Lo; S. Chu,"Backman Institute for Advanced Science and Technology, University of Illinois, Urbana-Champaign, Urbana, IL, USA; Backman Institute for Advanced Science and Technology, University of Illinois, Urbana-Champaign, Urbana, IL, USA; Backman Institute for Advanced Science and Technology, University of Illinois, Urbana-Champaign, Urbana, IL, USA; Backman Institute for Advanced Science and Technology, University of Illinois, Urbana-Champaign, Urbana, IL, USA; Backman Institute for Advanced Science and Technology, University of Illinois, Urbana-Champaign, Urbana, IL, USA; Backman Institute for Advanced Science and Technology, University of Illinois, Urbana-Champaign, Urbana, IL, USA; Backman Institute for Advanced Science and Technology, University of Illinois, Urbana-Champaign, Urbana, IL, USA; Backman Institute for Advanced Science and Technology, University of Illinois, Urbana-Champaign, Urbana, IL, USA; Backman Institute for Advanced Science and Technology, University of Illinois, Urbana-Champaign, Urbana, IL, USA; Backman Institute for Advanced Science and Technology, University of Illinois, Urbana-Champaign, Urbana, IL, USA; Backman Institute for Advanced Science and Technology, University of Illinois, Urbana-Champaign, Urbana, IL, USA","Proceedings IEEE International Conference on Application-Specific Systems, Architectures and Processors",06-Aug-02,1997,,,3,12,"Knowledge of the complex molecular structures of living cells is being accumulated at a tremendous rate. Key technologies enabling this success have been, high performance computing and powerful molecular graphics applications, but the technology is beginning to seriously lag behind challenges posed by the size and number of new structures and by the emerging opportunities in drug design and genetic engineering. A visual computing environment is being developed which permits interactive modeling of biopolymers by linking a 3D molecular graphics program with an efficient molecular dynamics simulation program executed on remote high-performance parallel computers. The system will be ideally suited for distributed computing environments, by utilizing both local 3D graphics facilities and the peak capacity of high-performance computers for the purpose of interactive biomolecular modeling. To create an interactive 3D environment three input methods will be explored: (1) a six degree of freedom ""mouse"" for controlling the space shared by the model and the user; (2) voice commands monitored through a microphone and recognized by a speech recognition interface; (3) hand gestures, detected through cameras and interpreted using computer vision techniques. Controlling 3D graphics connected to real time simulations and the use of voice with suitable language semantics, as well as hand gestures, promise great benefits for many types of problem solving environments. Our focus on structural biology takes advantage of existing sophisticated software, provides concrete objectives, defines a well-posed domain of tasks and offers a well-developed vocabulary for spoken communication.",2160-0511,0-8186-7959-X,10.1109/ASAP.1997.606807,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=606807,,Large-scale systems;Computer graphics;Pharmaceutical technology;Concurrent computing;Computational modeling;Distributed computing;Speech recognition;High performance computing;Drugs;Genetic engineering,computer vision;speech recognition;problem solving;data visualisation;programming environments;interactive programming;medical computing,visual computing environment;very large scale biomolecular modeling;living cells;complex molecular structures;high performance computing;molecular graphics;drug design;genetic engineering;interactive modeling;biopolymers;3D molecular graphics program;molecular dynamics simulation program;remote high-performance parallel computers;distributed computing environments;3D graphics;six degree of freedom;voice commands;microphone;speech recognition interface;hand gestures;computer vision;real time simulations,,11,1,12,IEEE,06-Aug-02,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Hand posture recognition using K-NN and Support Vector Machine classifiers evaluated on our proposed HandReader dataset,G. Tofighi; A. N. Venetsanopoulos; K. Raahemifar; S. Beheshti; H. Mohammadi,"Department of Electrical and Computer Engineering, Ryerson University, Toronto, Canada; Department of Electrical and Computer Engineering, Ryerson University, Toronto, Canada; Department of Electrical and Computer Engineering, Ryerson University, Toronto, Canada; Department of Electrical and Computer Engineering, Ryerson University, Toronto, Canada; Department of Electrical and Computer Engineering, Ryerson University, Toronto, Canada",2013 18th International Conference on Digital Signal Processing (DSP),10-Oct-13,2013,,,1,5,"In this paper, we propose a real-time vision-based hand posture recognition approach, based on appearance-based features of the hand poses. Our approach has three main steps: Preprocessing, Feature Extraction and Posture Recognition. Additionally, a new hand posture dataset called HandReader is created and introduced. HandReader is a dataset of 500 images of 10 different hand postures which are 10 non-motion-based American Sign Language alphabets with dark backgrounds. The dataset is gathered by capturing images of 50 male and female individuals performing these 10 hand postures in front of a common camera. 20% of the HandReader images are used for the training purpose and the remaining 80% are used to test the proposed methodology. All the images are normalized after applying the preprocessing step. The normalized images are then converted to feature vectors in the Feature Extraction step. In order to train the system, k-NN classifier and SVM classifiers with linear and RBF kernel have been employed and results were compared. These approaches were used to classify hand posture images into 10 different posture classes. The SVM classifier with linear kernel performed better with the highest true detection rate (96%) among other proposed techniques.",2165-3577,978-1-4673-5807-1,10.1109/ICDSP.2013.6622679,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622679,Hand posture recognition;Human computer interaction;Virtual environment;Shape recognition;HandReader dataset;k-NN;SVM,Computers;Support vector machine classification;Robots,feature extraction;image classification;pose estimation;support vector machines,support vector machine classifiers;HandReader dataset;real time vision based hand posture recognition;appearance based features;hand poses;feature extraction;posture dataset;HandReader images;k-NN classifier;SVM classifiers;RBF kernel;hand posture image classification;posture classes;true detection rate,,6,1,13,IEEE,10-Oct-13,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
An Edge Computing Platform of Guide-dog Robot for Visually Impaired,J. Zhu; Y. Chen; M. Zhang; Q. Chen; Y. Guo; H. Min; Z. Chen,"School of Software, Engineering South China, University of Technology, Guangzhou, China; School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, USA; School of Automation, Science and Control Engineering South China, University of Technology, Guangzhou, China; College of computer & information science, Southwest University, Chongqing, China; School of Information Science and Technology, Qingdao University of Science and Technology, Qingdao, China; School of Software, Engineering South China, University of Technology, Guangzhou, China; EVOC Intelligent Technology Company Limited, Shen zhen, China",2019 IEEE 14th International Symposium on Autonomous Decentralized System (ISADS),04-Aug-20,2019,,,1,7,"Information processing of guide dog robot requires expensive computing resources to meet real-time performance. We propose an edge computing framework based on Intel Up-Squared board and neural compute stick. Image processing and real-time control are performed on the framework. In addition, the voice recognition and commanding are also implemented, which are processed on Amazon cloud service. Vision, speech, and control services are integrated by ASU VIPLE (Visual IoT/Robotics Programming Language Environment). A prototype is developed, which implements the guide dog's obstacle avoidance, traffic sign recognition and following, and voice interaction with human.",2640-7485,978-1-7281-1672-3,10.1109/ISADS45777.2019.9155620,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9155620,Guide dog robot;Visually Impaired;Edge Computing;Neural Compute Stick;Internet of thing;ASU VIPLE;Embedded system,Dogs;Mobile robots;Robot sensing systems;Wheelchairs;Edge computing;Cameras,cloud computing;collision avoidance;handicapped aids;human-robot interaction;mobile robots;robot vision;speech recognition,information processing;real-time performance;Intel Up-Squared board;neural compute stick;real-time control;Amazon cloud service;control services;edge computing platform;guide-dog robot;visually impaired;ASU VIPLE;obstacle avoidance;traffic sign recognition and following;voice interaction with human,,2,,16,IEEE,04-Aug-20,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Precise detailed detection of faces and facial features,Liya Ding; A. M. Martinez,"Department Electrical and Computer Engineering, Ohio State Uinversity, Columbus, OH, USA; Department Electrical and Computer Engineering, Ohio State Uinversity, Columbus, OH, USA",2008 IEEE Conference on Computer Vision and Pattern Recognition,05-Aug-08,2008,,,1,7,"Face detection has advanced dramatically over the past three decades. Algorithms can now quite reliably detect faces in clutter in or near real time. However, much still needs to be done to provide an accurate and detailed description of external and internal features. This paper presents an approach to achieve this goal. Previous learning algorithms have had limited success on this task because the shape and texture of facial features varies widely under changing expression, pose and illumination. We address this problem with the use of subclass divisions. In this approach, we use an algorithm to automatically divide the training samples of each facial feature into a set of subclasses, each representing a distinct construction of the same facial component (e.g., close versus open eye lids). The key idea used to achieve accurate detections is to not only learn the textural information of the facial feature to be detected but that of its context (i.e., surroundings). This process permits a precise detection of key facial features. We then combine this approach with edge and color segmentation to provide an accurate and detailed detection of the shape of the major facial features (brows, eyes, nose, mouth and chin). We use this face detection algorithm to obtain precise descriptions of the facial features in video sequences of American Sign Language (ASL) sentences, where the variability in expressions can be extreme. Extensive experimental validation demonstrates our method is almost as precise as manual detection, ~ 2% error.",1063-6919,978-1-4244-2242-5,10.1109/CVPR.2008.4587812,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4587812,,Face detection;Facial features;Mouth;Lighting;Eyes;Handicapped aids;Active shape model;Reliability engineering;Image edge detection;Nose,face recognition;image sequences;image texture,faces precise detailed detection;facial features;facial feature;textural information;color segmentation;face detection algorithm;video sequences;American Sign Language sentences,,10,1,17,IEEE,05-Aug-08,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Performance evaluation of Decision Tree and neural network techniques for road scene image classification task,H. Rouabeh; C. Abdelmoula; M. Masmoudi,"EMC Research Group, National Engineering School of Sfax, Tunisia; EMC Research Group, National Engineering School of Sfax, Tunisia; EMC Research Group, National Engineering School of Sfax, Tunisia","International Image Processing, Applications and Systems Conference",19-Feb-15,2014,,,1,6,"This paper discusses the evaluation of two supervised learning based image classification algorithms. The classification subject of this work is part of a complete vision based road sign recognition system to be implanted using the VHDL language on an FPGA card for driver assistance applications. The classification is used in order to classify road scene images into different day times according to scene illumination and weather conditions. Due to the sensitivity of colors to illumination variation, the classification task is developed to improve the red color segmentation task which presents an important level in the road sign recognition system. In order to achieve real-time processing tasks and to reduce computing time and hardware resources occupation, the performance of the two predictive modeling techniques which are Neural Networks and Decision Trees is evaluated in this work. The VHDL circuit of the Decision Tree classifier is presented as well.",,978-1-4799-7069-8,10.1109/IPAS.2014.7043274,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7043274,Image classification;Decision Tree;Neural Networks;VHDL;ModelSim,Decision trees;Classification algorithms;Roads;Image classification;Training;Neural networks;Computer architecture,decision trees;driver information systems;field programmable gate arrays;image classification;image colour analysis;image recognition;neural nets,neural network techniques;road scene image classification task;performance evaluation;supervised learning based image classification algorithms;vision based road sign recognition system;FPGA card;driver assistance applications;scene illumination;weather conditions;red color segmentation task;VHDL circuit;decision tree classifier,,1,,15,IEEE,19-Feb-15,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Motion Trajectory Learning in the DFT-Coefficient Feature Space,A. Naftel; S. Khalid,"School of Informatics, University of Manchester, Institute of Science and Technology, Manchester, UK; School of Informatics, University of Manchester, Institute of Science and Technology, Manchester, UK",Fourth IEEE International Conference on Computer Vision Systems (ICVS'06),06-Mar-06,2006,,,47,47,"Techniques for understanding video object motion activity are becoming increasingly important with the widespread adoption of CCTV surveillance systems. In this paper we propose a novel vision system for clustering and classification of object-based video motion clips using spatiotemporal models. Object trajectories are modeled as motion time series using the lowest order Fourier coefficients obtained by Discrete Fourier Transform. Trajectory clustering is then carried out in the DFT-coefficient feature space to discover patterns of similar object motion activity. The DFT coefficients are used as input feature vectors to a Self-Organising Map which can learn similarities between object trajectories in an unsupervised manner. Encoding trajectories in this way leads to efficiency gains over existing approaches that use discrete point-based flow vectors to represent the whole trajectory. Assuming the clusters of trajectory points are distributed normally in the coefficient feature space, we propose a simple Mahalanobis classifier for the detection of anomalous trajectories. Our proposed techniques are validated on three different datasets - Australian sign language, handlabelled object trajectories from video surveillance footage and real-time tracking data obtained in the laboratory. Applications to event detection and motion data mining for visual surveillance systems are envisaged.",,0-7695-2506-7,10.1109/ICVS.2006.41,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1578735,,Discrete Fourier transforms;Trajectory;Machine vision;Spatiotemporal phenomena;Encoding;Australia;Handicapped aids;Video surveillance;Laboratories;Event detection,,,,32,2,34,IEEE,06-Mar-06,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
"Proceedings IEEE ICCV Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems",,,"Proceedings IEEE ICCV Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems",07-Aug-02,2001,,,i,,Presents the front cover of the proceedings record.,1530-1044,0-7695-1074-4,10.1109/RATFG.2001.938902,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=938902,,,real-time systems;face recognition;gesture recognition;image motion analysis,sample-based synthesis;talking heads;movies reconstruction;facial expressions;3D face model reconstruction;real-time 3D hand posture estimation;2D appearance retrieval;monocular camera;probabilistic techniques;automatic learning;appearance face models;video-based online face recognition;identity surfaces;nonlinear mapping;multi-view face patterns;Gaussian distribution;low dimensional space;real-time stereo tracking;multiple moving heads;head gestures;computer control;active appearance algorithm;facial feature tracking;3D tracking;dynamic time warping;off-line recognition;signer-independent sign language recognition;boosting;vision-based microphone switch;speech intent detection;face detection;video cues;audio cues;wavelet subspace;hand-color classification;mean shift tracking;view-subspace analysis;fast hand gesture recognition;real-time teleconferencing;visual models;social engagement;hybrid face recognition systems;profile views;MUGSHOT database;auto clustering;unsupervised learning;atomic gesture components;minimum description length;facial expression recognition;continuous dynamic programming;robust facial feature point detection;nonlinear illuminations;stabilized adaptive appearance changes model,,,,,IEEE,07-Aug-02,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
[Front matter],,,2009 24th International Conference Image and Vision Computing New Zealand,12-Jan-10,2009,,,1,8,This article deals with the following topics: Spatial Reasoning; Robot Navigation; Helmholtz-Hodge Decomposition;Omnidirectional Optical Flow; Smart Wheelchair Guidance; Optical Flow; Linear Systems; Motion Analysis; Real-Time Correlogram Tracking; Airborne Traffic Surveillance;Tabu Search Optimization; Real-Time Video Tracking ; Digital Elevation Model Accuracy; Alos-Prism Stereo Imagery; Strip-Mining Approach; VQ Image Coding; GPGPU Implementation; Robust Rigid Registration; Image Matching; Arbitrary Extrinsic Photometric Noise;Red Blood Cell Segmentation; SEM Images; Facial Expressions; Quadratic Deformation Model Analysis; Quadratic Deformation Model Synthesis; Face Recognition;Human Face Perception; Generalized Colour Image Segmentation; Kiwifruit Detection; Stereo Accuracy; Collision Avoidance; Multiclass Object Classification; Computer Vision; Linear Genetic Programming; Range Imaging System; Appearance Space Attributes; Texture Synthesis; Bag-Of-Words Speedometer; Single Camera SLAM; Ground-Plane Detection; Stereo Depth Values; Vision Science Experiments; Robot Navigation; Sign Language Analysis; Sign Language Recognition; Model-Based Least-Squares Optimal Interpolation; Sift-Based Object Recognition; Fast Alphabet Creation; Reduced Curse Of Dimensionality; Statistical Method; 3D Image Data Segmentation; Vehicle Pose Identification; Automatic Joint Computation; Automatic Skeleton Computation; 3D Face Feature Segmentation; Medical Imaging; Generic Cameras; Camera Shading Calibration; Spatially Modulated Field;Image Compression Scheme; Huffman Interpolation; Adaptive Interpolation; Improving Image Quality ; Spectral CT Volume Rendering; Human Motion Capture; Boosted Dynamic Active Shape Model; Pseudomonas Aeruginosa Bacteria Segmentation; Motile Cells; Edge Detection; PSO; Matching Road Networks; Mobile Inverted Pendulum Robot; Skin Color Model; Reliable Face Localization; Automatic Lung Segmentation; HRCT Images; Diffuse Parenchymal Lung Disease; Graph-Cut Techniques; Sub-Pel Motion Estimation; Contextual Image Filtering; Fuzzy Logic; Multi-Sensor Image Fusion; Knowledge Acquisition; Computer Vision; Smoothed Particle Hydrodynamics; Hough Transforms; Automatic Medical Image Categorisation; Support Vector Machine Classifier; Mixed Pixel Restoration; Surface Projection; Edge Strength Evaluation; and Reaction-Diffusion Systems.,2151-2205,978-1-4244-4697-1,10.1109/IVCNZ.2009.5378350,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5378350,,,collision avoidance;computer vision;data compression;face recognition;image coding;image motion analysis;image segmentation;knowledge acquisition;linear systems;medical image processing;mobile robots;particle swarm optimisation;search problems;spatial reasoning;stereo image processing;support vector machines;surveillance;tracking;wheelchairs,spatial reasoning;robot navigation;Helmholtz-Hodge decomposition;omnidirectional optical flow;smart wheelchair guidance;optical flow;linear systems;motion analysis;real time correlogram tracking;airborne traffic surveillance;tabu search;optimization;real-time video tracking;digital elevation model;alos-prism stereo imagery;strip mining;VQ image coding;image matching;quadratic deformation model;face recognition;human face perception;image segmentation;single camera SLAM;sign language recognition;object recognition;statistical method;3D face feature segmentation;medical imaging;image compression;edge detection;particle swarm optimization;mobile inverted pendulum robot;automatic lung segmentation;contextual image filtering;fuzzy logic;sensor image fusion;knowledge acquisition;computer vision;support vector machine,,,,,IEEE,12-Jan-10,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
Table of contents,,,2014 International Conference on Applied Electronics,19-Jan-15,2014,,,vii,xii,The following topics are dealt with: high-speed analog-to-digital converters; nanometer CMOS technology; pseudo random code regeneration; oscillation frequency value; oscillation-based tests; reconfigurable filter structure; universal embedded controller; matrix converter; continuous-time sigma-delta modulator; integrated LC filter; human-robot cooperation; seismic sensor system; MEMS accelerometer; memristor pinched hysteresis loops; PV panels; MPPT controller; PV power system modelling; PLP feature extraction optimization; LVCSR recognition; MP3 data; dead-time compensation strategy; adaptive harmonic compensator; Cartesian genetic programming; desktop based real time oxygen auto-ventilation; gas monitoring system; homecare respiratory application; subband optimization; EEG-based classification; HART compatible HART line powered communication module; interactive hybrid control-flow checking method; polyphase FIR filter structure; VHDL language; FPGA; B3C converter; heat transfer; electronic system; moving object searching; virtual instrumentation; ISO26262 motorbikes; nonlinear stabilization; state space energy error feedback; quadrature oscillator; operational transresistance amplifier; quantum computation system emulator; low noise amplifier; SVPWM algorithm; H-bridge power inverter; LCL filter; 3D machine vision system; contact strips inspection; railway vehicle current collectors; wrist cuff method; mean arterial pressure; dual-cuff blood pressure system; contactless electrical energy transfer; varying air gap; resonant converter; nonlinear inductance; programmable PWM modulator; OPWM test platform; 3D FEM simulation; EMC testing; input current harmonics; voltage-source active rectifier; motion detection system; NiTi pressure sensors; phase coupled oscillator; dissipation normal form; space vector pulse width modulation; RF circuits response; software defined radio system; adaptive mechanical model; cardiovascular system; RF network combiner; wireless multicommunication system; smart households; graph method; SI circuit solution; M-FSK intrapulse modulation analysis; subspectral decomposition method; small loop antennas parameter measurement; GTEM cell; radio frequency energy harvesting system; boost power factor correction topology; average current control; critical conduction mode; EKF based position estimators; speed estimators; sensorless DTC; permanent magnet synchronous machines; E-D-Mode InAlN-GaN HFET inverter; low frequency amplifier; real-time multicore systems; genetic algorithm; runnable sequencing; bandgap voltage reference; high-order temperature compensation; two stepped system; train dynamic motion; wheel slip; CAN nodes health monitoring; ADS-B channel; electrical resistance measurement; contact resistance; sensor fusion; IR sensors; Kalman filter;CMOS class-AB amplifier; rendering moving sound source; headphone-based virtual acoustic reality; high spectral quality sinusoidal oscillator; energy based amplitude control; wireless electronic systems; physiological parameters measuring; hidden pacemaker pulses detection; wavelet transform; Hilbert transform; feedback control; electrospinning process; iterated maps; nonlinear dynamics generation; genetic reasoning; finger sign identification; forearm electromyogram; statistical multirate high-resolution signal reconstruction; empirical mode decomposition based denoising approach; RF signal denoising; OFDM system; AWGN channels; feedforward approach; DC cancellation; fully-differential instrumentation amplifiers; fuzzy expert systems; and human gait phase.,1803-7232,978-8-0261-0277-9,10.1109/AE.2014.7011653,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7011653,,,AWGN channels;carrier transmission on power lines;CMOS integrated circuits;compensation;computer vision;contact resistance;electric current control;electric resistance measurement;electroencephalography;electromagnetic compatibility;electromyography;electrospinning;energy harvesting;expert systems;feature extraction;field programmable gate arrays;fingerprint identification;finite element analysis;FIR filters;frequency shift keying;gait analysis;genetic algorithms;graph theory;high electron mobility transistors;Hilbert transforms;human-robot interaction;infrared detectors;instrumentation amplifiers;Kalman filters;loop antennas;low noise amplifiers;matrix convertors;maximum power point trackers;memristors;microsensors;motorcycles;multiprocessing systems;OFDM modulation;operational amplifiers;oscillators;permanent magnet machines;photovoltaic power systems;power factor correction;pressure sensors;PWM invertors;random number generation;rectifiers;resonant power convertors;sensor fusion;sensorless machine control;sigma-delta modulation;signal denoising;signal reconstruction;software radio;synchronous machines;TEM cells;traction current collection;ventilation;virtual instrumentation;virtual reality;wavelet transforms,high-speed analog-to-digital converters;nanometer CMOS technology;pseudo random code regeneration;oscillation frequency value;oscillation-based tests;reconfigurable filter structure;universal embedded controller;matrix converter;continuous-time sigma-delta modulator;integrated LC filter;human-robot cooperation;seismic sensor system;MEMS accelerometer;memristor pinched hysteresis loops;PV panels;MPPT controller;PV power system modelling;PLP feature extraction optimization;LVCSR recognition;MP3 data;dead-time compensation strategy;adaptive harmonic compensator;Cartesian genetic programming;desktop based real time oxygen auto-ventilation;gas monitoring system;homecare respiratory application;subband optimization;EEG-based classification;HART compatible HART line powered communication module;interactive hybrid control-flow checking method;polyphase FIR filter structure;VHDL language;FPGA;B3C converter;heat transfer;moving object searching;virtual instrumentation;ISO26262 motorbikes;nonlinear stabilization;state space energy error feedback;quadrature oscillator;operational transresistance amplifier;quantum computation system emulator;low noise amplifier;SVPWM algorithm;H-bridge power inverter;LCL filter;3D machine vision system;contact strips inspection;railway vehicle current collectors;wrist cuff method;mean arterial pressure;dual-cuff blood pressure system;contactless electrical energy transfer;varying air gap;resonant converter;nonlinear inductance;programmable PWM modulator;OPWM test platform;3D FEM simulation;EMC testing;input current harmonics;voltage-source active rectifier;motion detection system;NiTi pressure sensors;phase coupled oscillator;dissipation normal form;space vector pulse width modulation;RF circuits response;software defined radio system;adaptive mechanical model;cardiovascular system;RF network combiner;wireless multicommunication system;smart households;graph method;SI circuit solution;M-FSK intrapulse modulation analysis;subspectral decomposition method;small loop antennas parameter measurement;GTEM cell;radio frequency energy harvesting system;boost power factor correction topology;average current control;critical conduction mode;EKF based position estimators;speed estimators;sensorless DTC;permanent magnet synchronous machines;E-D-Mode InAlN-GaN HFET inverter;low frequency amplifier;real-time multicore systems;genetic algorithm;runnable sequencing;bandgap voltage reference;high-order temperature compensation;two stepped system;train dynamic motion;wheel slip;CAN nodes health monitoring;ADS-B channel;electrical resistance measurement;contact resistance;sensor fusion;IR sensors;Kalman filter;CMOS class-AB amplifier;rendering moving sound source;headphone-based virtual acoustic reality;high spectral quality sinusoidal oscillator;energy based amplitude control;wireless electronic systems;physiological parameters measuring;hidden pacemaker pulses detection;wavelet transform;Hilbert transform;feedback control;electrospinning process;iterated maps;nonlinear dynamics generation;genetic reasoning;finger sign identification;forearm electromyogram;statistical multirate high-resolution signal reconstruction;empirical mode decomposition based denoising approach;RF signal denoising;OFDM system;AWGN channels;feedforward approach;DC cancellation;fully-differential instrumentation amplifiers;fuzzy expert systems;human gait phase,,,,,,19-Jan-15,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
[Title page],,,2011 IEEE Student Conference on Research and Development,09-Feb-12,2011,,,1,1,The following topics are dealt with: static single cell fault testing; hybrid solar cell; nanocomposite optical properties; nanocomposite electrical properties; thin films heat treatment; thermal CVD; camphor oil; variable speed wind turbine; Z-source inverter; induction motor drive; photovoltaic array; brushless permanent magnet motors; multiuser McCDMA; Rayleigh channel; Gaussian channel; multilayer network; OFDM system; certificateless public key infrastructure; one legged hopping robot; moving vehicle noise classification; Phoneme based sign language recognition system; neural nets; intelligent vision system; real time clinical diagnostic system; power split hybrid electric vehicles; discrete sliding mode control; hydraulic actuator system; dynamic traffic-light phase plan protocol; DNA sequence design; circular antenna array; cognitive radio network; magnetic circuit analysis; bandwidth allocation; Web page quality assessment; Kanban controlled manufacturing system; Myanmar WordNet lexical database; nanostructured copper iodide thin films; low voltage energy harvesting circuits; on-chip high speed VLSI RLCG global interconnect; particle swarm optimisation; optimal linear FIR high pass filter design; high performance modular hardware accelerator architecture; Domino logic; location based information system; water bath control system; patient management system; introductory programming teaching; project scheduling; MAC protocol; wireless ad hoc networks; VoIP services PSTN services; and XML scheme documents.,,978-1-4673-0102-2,10.1109/SCOReD.2011.6148697,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6148697,,,access protocols;antenna arrays;bandwidth allocation;brushless DC motors;chemical vapour deposition;cognitive radio;computer science education;FIR filters;Gaussian channels;gesture recognition;heat treatment;Internet telephony;invertors;kanban;logic design;low-power electronics;magnetic circuits;medical information systems;mobile ad hoc networks;mobile robots;multiprocessor interconnection networks;nanocomposites;neural nets;OFDM modulation;photovoltaic cells;public key cryptography;Rayleigh channels;solar cells;thin films;variable structure systems;very high speed integrated circuits;Web sites;wind turbines;word processing;XML,static single cell fault testing;hybrid solar cell;nanocomposite optical properties;nanocomposite electrical properties;thin films heat treatment;thermal CVD;camphor oil;variable speed wind turbine;Z-source inverter;induction motor drive;photovoltaic array;brushless permanent magnet motors;multiuser McCDMA;Rayleigh channel;Gaussian channel;multilayer network;OFDM system;certificateless public key infrastructure;one legged hopping robot;moving vehicle noise classification;Phoneme based sign language recognition system;neural nets;intelligent vision system;real time clinical diagnostic system;power split hybrid electric vehicles;discrete sliding mode control;hydraulic actuator system;dynamic traffic-light phase plan protocol;DNA sequence design;circular antenna array;cognitive radio network;magnetic circuit analysis;bandwidth allocation;Web page quality assessment;Kanban controlled manufacturing system;Myanmar WordNet lexical database;nanostructured copper iodide thin films;low voltage energy harvesting circuits;on-chip high speed VLSI RLCG global interconnect;particle swarm optimisation;optimal linear FIR high pass filter design;high performance modular hardware accelerator architecture;Domino logic;location based information system;water bath control system;patient management system;introductory programming teaching;project scheduling;MAC protocol;wireless ad hoc networks;VoIP services;PSTN services;XML scheme documents,,,,,IEEE,09-Feb-12,,,IEEE,IEEE Conferences,No,No,Yes,No,,,
